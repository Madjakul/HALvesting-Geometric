<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Experiments on Building Language Resources for Multi-Modal Dialogue Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@loria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Lorraine</orgName>
								<address>
									<addrLine>Campus scientifique</addrLine>
									<postCode>54506</postCode>
									<settlement>Batiment LORIA, Vandoeuvre-les-Nancy cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amalia</forename><surname>Todirascu</surname></persName>
							<email>amalia.todirascu@utt.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Universit√© de Technologie de Troyes</orgName>
								<address>
									<addrLine>12, rue Marie Curie</addrLine>
									<postCode>10010</postCode>
									<settlement>Troyes cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Langlois</surname></persName>
							<email>david.langlois@loria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA Lorraine</orgName>
								<address>
									<addrLine>Campus scientifique</addrLine>
									<postCode>54506</postCode>
									<settlement>Batiment LORIA, Vandoeuvre-les-Nancy cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Experiments on Building Language Resources for Multi-Modal Dialogue Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0FF25751C54FADF1DB1E600475473897</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper presents the experiments made to adapt and to synchronise the linguistic resources of the French language processing modules integrated in the MIAMM prototype, designed to handle multi-modal human-machine interactions. These experiments allowed us to identify a methodology for adapting multilingual resources for a dialogue system. In the paper, we describe the iterative joint process used to build linguistic resources for the two cooperative modules: speech recognition for speech modality and syntactic/semantic parsing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper focuses on the identification of a methodology for adapting linguistic resources for a human-machine dialogue system. The prototype resulting from the European Multimedia Information Access using Multiple Modalities (MIAMM) project ( <ref type="bibr">(Reithinger and all, 2003)</ref>, <ref type="bibr" target="#b6">(Kumar and Romary, 2002)</ref>) proposes to the human user several modalities to explore a music database: speech, haptic interfaces, visualisation, combined into a humanmachine dialogue system.</p><p>Such human-dialogue system requires a language model designed for the application. While the MIAMM project integrates innovative haptic modules, we have been confronted to the lack of real user-system interactions. It is difficult to find annotated dialogue corpora for a specific domain (containing only speech and text), multimodal corpora including haptic interaction are not yet available. Building annotated dialogue corpora is very expensive <ref type="bibr" target="#b10">(Rapp &amp; Strube, 2002)</ref>. Due to the fact that the haptic interfaces were not available at the beginning of the project, we had to develop suitable linguistic resources.</p><p>We present here the adaptation process of our tools and pre-existing linguistic resources for this project to provide language models for the speech modality (speech recognition) and for the parser (used to build a semantic representation of the speech input).</p><p>Across the various languages (French, English and German) used in the MIAMM project, we tried to maintain the same linguistic coverage, even if the actual implementations of various parsers and speech recognisers were different. For this purpose, the speech recognisers and the parsers use a shared language model (a shared vocabulary and grammar), established on the basis of user scenarios.</p><p>We tested several methods for speech and text processing. We use two robust parsing methods for information extraction: template-based parsing and TAGbased grammars. For English and German, we use the same speech recogniser, together with the SPIN templatebased parser. For French, we use the ESPERE speech recogniser <ref type="bibr">(Fohr and all, 2000)</ref> and a LTAG parser <ref type="bibr">(Lopez's parser (Lopez, 1999)</ref>) using local grammars in order to extract the semantic interpretation. The speech recognisers output wordgraphs containing most probable sentences (in MPEG-7 format), the SPIN parsers process them and provide semantic interpretations to the Dialogue Processing Manager. All these modules use a shared language model and a similar linguistic coverage.</p><p>This paper illustrates the work done for the French modules, even if the actual prototype includes English, German and French languages. We chose the French modules in order to illustrate the adaptation process of modules implementing different approaches: statistical methods for speech recogniser and classical linguistic processing approach based on TAG grammars <ref type="bibr" target="#b4">(Joshi, 1987)</ref> for parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The French modules</head><p>We present the main features of the French modules interpreting speech input and providing a semantic interpretation according to the domain model.</p><p>The ESPERE speech recognition system is used for acquiring/recognising vocal commands from user. Its output is a word lattice (in MPEG7 format) containing the n-best possible sentences matching the acoustic input. ESPERE relies on the HMM technology <ref type="bibr" target="#b5">(Kai-Fu &amp; Fileno, 1992)</ref> and is dedicated to small vocabulary applications. Basically, the system is made up of two modules: (1) the acoustic module is composed of 40 monophones trained on the BREF80 database <ref type="bibr">(Lamel and all, 1991)</ref>; (2) the language model is a statistical bigram model <ref type="bibr" target="#b3">(Jelinek, 1990)</ref>, but more performant language models can be used for parsing the word lattice (as it is done in the MIAMM project).</p><p>The Lopez parser <ref type="bibr" target="#b8">(Lopez, 1999)</ref>, used for interpreting the output of the speech recogniser, is based on the Lexicalised Tree Adjoining Grammar <ref type="bibr" target="#b4">(Joshi, 1987)</ref> formalism. We chose this parser because it provides partial parsing results (in order to handle noisy or erroneous input) and because LTAG represent words in their syntactic context (helping us to build a semantic interpretation). The parser use general French grammar validated by linguists, described in Tree Adjoining Grammar Markup Language (TAGML) format <ref type="bibr">(Pardo and all, 2000)</ref>. Using the information provided by syntax, we added links to the MIAMM's domain-specific ontology, for obtaining a relevant semantic interpretation, in MMIL format <ref type="bibr" target="#b6">(Kumar and Romary, 2002)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Creating/adapting linguistic resources</head><p>The methodology used for adapting/creating the language models for our project follows the steps presented in Figure <ref type="figure" target="#fig_0">1</ref>. To build the language resources, we stemmed on basic interaction scenarios, while lacking real interaction corpora. We concertated the efforts of building the grammar and the vocabulary to have similar coverage across languages. For each language, one group designed a context-free grammar to cover these scenarios. The technical vocabularies were extracted from scenarios and grammars and were translated for each language to maintain the same semantic coverage. The statistical language model used for speech recognition was developed directly from these resources. The LTAG parser's resources (used to build semantic representations of speech input) were developed from general LTAG resources and adapted to the application by comparing the linguistic coverage with the French context-free grammar.</p><p>While scenarios changed several times during the project, we used an iterative joint process to update resources and language models in order to match the application requirements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">User scenarios</head><p>Due to the various languages and NLP techniques integrated in the system, we wanted to build language resources covering the same linguistic phenomena in all the languages. We preferred to have an uniform linguistic coverage instead of having only a semantic coverage, as most of the dialogue systems propose <ref type="bibr" target="#b10">(Rapp &amp; Strube, 2002)</ref>. A homogeneous linguistic coverage consists of several styles (or registers -familiar, elaborated), specific phrases (politeness phrases, time intervals -"from the sixties"), various syntactic components (passive constructions, relative clauses, questions and ellipses) as well as dates or names. We treat identically similar linguistic phenomena in every language. This method assures that the semantic coverage is also similar. The most difficult task was to identify the most significant linguistic phenomena to be handled by the language modules <ref type="bibr">(Wilks and all, 2000)</ref>.</p><p>Due to the lack of some functionalities (haptic and visual interaction), the MIAMM's human factor team proposed possible user scenarios. The scenarios contain possible user interactions involving one or several modalities: haptic, graphic or speech. We do not have real data for training the system, so we replace it with made-up training data.</p><p>From the initial scenarios, we identified the syntactic elements and the required vocabulary: some basic predicates, domain-specific objects (database's specific categories: songs, titles, styles, albums etc.), auxiliary phrases (opening session items, closing phrases, referential mechanisms -alterity, similarity, politeness expressions), modality specific vocabulary (visualisation styles, visualisation predicates etc.).</p><p>The advantage of this user scenario-based approach is that each developer adapts the resources independently and he decides himself which new entries to be added to the existing lexicons and grammars. The parsers and the speech recognisers could be tested independently for each language, without waiting the other teams. The drawbacks of this approach are the requirements of building exhaustive user scenarios (impossible while some functionalities are not available yet), as well as the different stages of development of the various modules.</p><p>3.2. Designing the language models 3.2.1. Creating a training corpus for the speech modality The bootstrap of a bigram model, used by the speech recogniser, is a training corpus relevant to the task. Unfortunately, as explained in the introduction, such a corpus was not available and we had to remedy this lack.</p><p>In order to generate a training corpus, we designed a context-free grammar. By developing this grammar, our objective was to benefit from the compactness, the flexibility of this formalism to model a language allowing a wide range of possibilities for user to utter commands and requests. This grammar contains almost 200 rules and is based on a 400 word vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Training the bigram model</head><p>For training the language model, it was not possible to collect the bigram frequencies directly from the corpus generated with the grammar, because this corpus was too huge. Rather, we partially generated the training corpus at a class level. These classes were chosen among nonterminals. For example, one sentence of this training corpus is:</p><p>"donne-moi le GENRE des ann√©es DECADES" (give me the GENRE of the DECADES's)</p><p>With this corpus, we assumed a uniform distribution of the words into each class. For example: In the following sections, we describe several methods to estimate the bigram probabilities and give the performance of the speech recogniser for each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Adapting the TAG parser</head><p>Lopez's parser has an initial domain-independent lexicon and grammar, not very useful in the context of multimodal musical search. We add domain-specific words or words designing several types of searches in the musical database (by similarity, by musical dimensions: mood, style, genre) to the lexicon, and new domain-specific lexical categories (used to build specific syntactic components: a style followed by a mood and by a time interval, a request verb followed by a similarity search). We added new lexical entries specific to various humanmachine interactions (haptic, visual).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User scenario</head><p>Context-free grammar (En, Fr, Ge) LTAG resources for free texts Parser's resources Technical vocabulary (En, Fr, Ge) SR's language model modellanguag The parser's output (derivation trees and derived trees) are used to build a semantic representation in MMIL format <ref type="bibr" target="#b6">(Kumar and Romary, 2002)</ref>. MMIL elements contain several events and participants and relations between these elements. The relations correspond to the syntactic structure represented by each elementary tree. A mapping between the various lexical entries and the domain-specific ontology was required to build the appropriate semantic representation. We inspected the context-free grammar's rules and we generated specific local grammars (elementary trees tagged with semantic relations, by using a meta-grammar <ref type="bibr">(Gaiffe and all, 2003)</ref>), for modelling each specific phenomena. MMIL specifications changed also during the project, so several elementary trees have been added (alternatives, time periods); some morphological features (mode, tense) have been modified in order to handle the changes.</p><p>The main changes of the grammar concern the preference for using substitutions instead of adjunctions in order to reduce the number of parsing results. The use of substitutions reduces the number of possible parsing results, in order to increase parser's efficiency. If a substituted syntactic component missed, it is interpreted as an empty MMIL participant or event.</p><p>The linguistic coverage concerns several possible combinations of the following syntactic components: elliptic phrases (celui-ci, celui-l√†), domain-specific noun groups (du GENRE, du GENRE MOOD, une liste de chansons/albums, TITRE, ARTIST), opening and closing events (commence, annule, oui, non), demand verbs (demander, vouloir), navigation verbs (avancer, afficher, d√©placer, montrer), very simple negation (only to cancel the previous orders).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recognition experiments</head><p>In this section, we describe several ways to estimate the bigram probabilities and give the performance of the speech recogniser for each of these ways.</p><p>As the speech recogniser is integrated into the general architecture of the MIAMM project, the evaluation should be an user-centered evaluation. But such an experimental protocol is not ready for the moment. So we decided to evaluate the system in terms of Word Error Rate. This evaluation is required because speech recognition accuracy must be high to build an effective dialogue with the user. Too many errors at the recognition step are not acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental protocol</head><p>We recorded 88 sentences that can be parsed by the TAG parser, e. g. that can be generated by the grammar. These sentences were selected to cover the most possible linguistic phenomena. We remark that, even if we decided to give enough liberty to the user for the speech modality, each acceptable phrase will be parsed. Therefore, we decided to not use out-of-application sentences, and outof-vocabulary words.</p><p>The sentences were recorded by 4 speakers, 2 females (OM and AB) and 2 males (KS and DL). Each of them recorded 22 sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimating bigram probabilities and evaluation</head><p>In this section, we describe several methods to estimate the bigram probabilities. For each method, we evaluate the corresponding speech recognition system by the Word Error Rate on the 88 sentences. Two parameters are used to integrate the language model into the system: the language model's weight in comparison with the acoustic models; an additional cost added to each bigram in order to prevent from too many insertions. In the following experiments, the results are given for the best values for these parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Estimation 1</head><p>The first idea consists in estimating the bigram probabilities by using directly the bigram frequencies from the training corpus. The performances are given in Table <ref type="table">1</ref>. We can first remark that the WER is low. A study of the errors shows some confusions between very acoustically closed words <ref type="bibr">(1980 and 1981, veux-tu and peux-tu)</ref>. Globally, these errors do not modify the overall semantics of the sentences. WER (speaker, error rare) OM, 2.7 KS, 4.8 AB, 3.7 DL, 4.3 Overall error rate : 3.8 Standard deviation: 0.9 Table 1 : performance for Estimation 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Estimation 2</head><p>The second method makes the hypothesis that the probabilities may be not representative of a real life use because the training corpus has been generated from the grammar. In order to check this hypothesis, we evaluated a system where all bigrams have the same probabilities. But, in this model, bigrams which do not occur in the training corpus are given a null probability. So, this model gives only a binary information: a given bigram is part or not of the application's language. The performances are given in Table <ref type="table">2</ref>. We remark that WER increases a bit, but the increasing is not significant. This evaluation tends to confirm that real life probabilities may be not important (for this experiment). WER (speaker, error rare) OM, 3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Estimation 3</head><p>For the following experiment we abandon the bigram constraints given by the grammar. For that, we used the Good Turing discounting so that all bigrams get a not null probability. The discount is applied to the bigram frequency from the training corpus generated with the grammar. This method is the first step towards a model less dedicated to the application, even if the vocabulary remains the same. The results of the system using this language model are very bad compared to the ones described in this paper. We can conclude that the constraints given by the context-free grammar are necessary, even at bigram level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Estimation 4</head><p>Last, we tried to extract the bigrams probabilities from a general, free language corpus. We extracted the bigrams probabilities for bigrams present both in the general corpus and the application's corpus. As general corpus, we chose 2 years of the French newspaper ¬´ Le Monde ¬ª. We used a linear combination between the two models (GM for the not specific (General) Model, and AM for the Application's Model). This way is a kind of language model adaptation <ref type="bibr" target="#b0">(Bellagarda 2004</ref>). The performances of the linear combination for several values of the AM's weight are given in Figure <ref type="figure" target="#fig_2">2</ref>. This figure shows that the bigram probabilities estimated from "Le Monde" lead to worse results when the weight of this corpus increases. This indicates that the general model generalizes too much the syntactic features of the application. The dedicated context-free grammar must be the central bedrock of the language model. Using more general language need specific adaptation processes. One important point is that such process should take into account the necessary homogeneity with the LTAG grammar's language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Parsing Experiments</head><p>During the iteration phase, we refined the parser's linguistic resources by interacting with other modules. Visualisation and haptics provided new functionalities, so we added new lexical entries, specific lexical categories (VISUALISATION_MODE, DIRECTION) and specific elementary trees (for specific navigation commands, for time intervals) After testing the parser and the SR, we need to synchronise the language model and the parser's language resources in order to cover the same training corpus. The vocabularies of the two modules are now very similar, after completing them with missing flexed forms or syntagms.</p><p>Dialogue Manager module uses a domain ontology to decide which action to do as the answer to the user's requests. Domain ontology changed several times during the project; we had to re-generate the mapping between lexical entries and the domain concepts.</p><p>French parser is quite slow compared to the other parsers (for German and for English), due to the fact that the TAG grammar is large (contains a lot of elementary trees for specific phenomena). But, even if partial parsing is provided, the parser builds some MMIL components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and further work</head><p>The ESPERE speech recogniser and the TAG parser cover the same linguistic phenomena and share the same lexicon, due to the use of shared user scenarios. The relevance of the test corpus will be evaluated by comparing with real user input from the MIAMM prototype, but it helped us to adapt the language modules in the absence of well-defined system's specifications. Further work will focus on the evaluation of methodologies for building test suites, in the context of a multi-modal dialogue system.</p><p>The MIAMM project involves our two teams: the "Langue et Dialogue" group which aims at building human-machine dialogue systems, and the Speech Group which aims at speech recognition. This project is the first step towards a collaboration based on the use of formal language/dialogue models during the speech recognition process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Adapting MIAMM linguistic resources</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: WER for several linear combinations between GM and AM</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Statistical language model adaptation : review and perspectives</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bellagarda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The automatic speech recognition engine ESPERE : experiments on telephone speech</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Antoine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSLP 2000</title>
		<meeting>ICSLP 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta-grammar Compiler</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gaiffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crabbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roussanaly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TAG+6</title>
		<meeting>TAG+6<address><addrLine>Venice</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-organized language modeling for speech recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Speech Recognition</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K.-F</forename><surname>Lee</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An Introduction to Tree Adjoining Grammars</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Mathematics of Language</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Continuous speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kai-Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fileno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Speech Signal Processing</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">Mohan</forename><surname>Sondhi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Comprehensive Framework for Multimodal Meaning Representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Computational Semantics (IWCS-5)</title>
		<meeting><address><addrLine>Tilburg, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BREF, a large vocabulary spoken corpus for french</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Gauvain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eskenazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Speech Communication and Technology</title>
		<meeting><address><addrLine>Genova, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="505" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Robust Parsing with Lexicalized Tree Adjoining Grammars</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Nancy, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>INRIA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D.Thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical aspects in compiling tabular TAG parsers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De La Clergerie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TAG+5 Workshop</title>
		<meeting>the TAG+5 Workshop<address><addrLine>Paris 7, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Universit√©</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Iterative Data Collection Approach for Multimodal Dialogue Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of LREC</title>
		<meeting>eeding of LREC</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="661" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">MIAMM-A Multi-Modal Dialogue System Using Haptics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reithinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fedeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pecourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural, Intelligent and Effective Interaction in Multimodal Dialogue Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Van Kuppevelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Dybkjaer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">O</forename><surname>Bernsen</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publisher</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human-Computer Conversation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Catizone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Microcomputers</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Dekker</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
