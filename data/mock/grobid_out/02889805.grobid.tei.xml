<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CamemBERT: a Tasty French Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
							<email>louismartin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
							<email>benjamin.muller@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">Javier</forename><surname>Ortiz Suárez</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
							<email>yoa.dupont@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Villemonte</forename><surname>Éric</surname></persName>
						</author>
						<author>
							<persName><forename type="first">La</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Djamé</forename><surname>Clergerie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Seddah</surname></persName>
						</author>
						<author>
							<persName><surname>Sagot</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Éric</forename><surname>Villemonte De La Clergerie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
							<email>djame.seddah@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
							<email>benoit.sagot@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CamemBERT: a Tasty French Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6EA53A9AC0B8D5AD8F8DF49A8BD19A88</idno>
					<idno type="DOI">10.18653/v1/2020.acl-</idno>
					<note type="submission">Submitted on 5 Jul 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained word representations have a long history in Natural Language Processing (NLP), from noncontextual <ref type="bibr" target="#b5">(Brown et al., 1992;</ref><ref type="bibr" target="#b2">Ando and Zhang, 2005;</ref><ref type="bibr" target="#b39">Mikolov et al., 2013;</ref><ref type="bibr" target="#b44">Pennington et al., 2014)</ref> to contextual word embeddings <ref type="bibr" target="#b45">(Peters et al., 2018;</ref><ref type="bibr" target="#b1">Akbik et al., 2018)</ref>. Word representations are usually obtained by training language model architectures on large amounts of textual data and then fed as an input to more complex task-specific architectures. More recently, these specialized architectures have been replaced altogether by large-scale pretrained language models which are fine-tuned for each application considered. This shift has resulted in large improvements in performance over a wide * Equal contribution. Order determined alphabetically. range of tasks <ref type="bibr" target="#b15">(Devlin et al., 2019;</ref><ref type="bibr" target="#b48">Radford et al., 2019;</ref><ref type="bibr" target="#b35">Liu et al., 2019;</ref><ref type="bibr" target="#b49">Raffel et al., 2019)</ref>.</p><p>These transfer learning methods exhibit clear advantages over more traditional task-specific approaches. In particular, they can be trained in an unsupervized manner, thereby taking advantage of the information contained in large amounts of raw text. Yet they come with implementation challenges, namely the amount of data and computational resources needed for pretraining, which can reach hundreds of gigabytes of text and require hundreds of GPUs <ref type="bibr" target="#b66">(Yang et al., 2019;</ref><ref type="bibr" target="#b35">Liu et al., 2019)</ref>. This has limited the availability of these state-of-the-art models to the English language, at least in the monolingual setting. This is particularly inconvenient as it hinders their practical use in NLP systems. It also prevents us from investigating their language modelling capacity, for instance in the case of morphologically rich languages.</p><p>Although multilingual models give remarkable results, they are often larger, and their results, as we will observe for French, can lag behind their monolingual counterparts for high-resource languages.</p><p>In order to reproduce and validate results that have so far only been obtained for English, we take advantage of the newly available multilingual corpora OSCAR <ref type="bibr">(Ortiz Suárez et al., 2019)</ref> to train a monolingual language model for French, dubbed CamemBERT. We also train alternative versions of CamemBERT on different smaller corpora with different levels of homogeneity in genre and style in order to assess the impact of these parameters on downstream task performance. CamemBERT uses the RoBERTa architecture <ref type="bibr" target="#b35">(Liu et al., 2019)</ref>, an improved variant of the high-performing and widely used BERT architecture <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>.</p><p>We evaluate our model on four different downstream tasks for French: part-of-speech (POS) tagging, dependency parsing, named entity recognition (NER) and natural language inference (NLI).</p><p>CamemBERT improves on the state of the art in all four tasks compared to previous monolingual and multilingual approaches including mBERT, XLM and XLM-R, which confirms the effectiveness of large pretrained language models for French.</p><p>We make the following contributions:</p><p>• First release of a monolingual RoBERTa model for the French language using recently introduced large-scale open source corpora from the Oscar collection and first outside the original BERT authors to release such a large model for an other language than English. 1</p><p>• We achieve state-of-the-art results on four downstream tasks: POS tagging, dependency parsing, NER and NLI, confirming the effectiveness of BERT-based language models for French.</p><p>• We demonstrate that small and diverse training sets can achieve similar performance to large-scale corpora, by analysing the importance of the pretraining corpus in terms of size and domain.</p><p>2 Previous work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contextual Language Models</head><p>From non-contextual to contextual word embeddings The first neural word vector representations were non-contextualized word embeddings, most notably word2vec <ref type="bibr" target="#b39">(Mikolov et al., 2013)</ref>, GloVe <ref type="bibr" target="#b44">(Pennington et al., 2014)</ref> and fastText <ref type="bibr" target="#b38">(Mikolov et al., 2018)</ref>, which were designed to be used as input to task-specific neural architectures.</p><p>Contextualized word representations such as ELMo <ref type="bibr" target="#b45">(Peters et al., 2018)</ref> and flair <ref type="bibr" target="#b1">(Akbik et al., 2018)</ref>, improved the representational power of word embeddings by taking context into account. Among other reasons, they improved the performance of models on many tasks by handling words polysemy. This paved the way for larger contextualized models that replaced downstream architectures altogether in most tasks. Trained with language modeling objectives, these approaches range from LSTMbased architectures such as <ref type="bibr" target="#b12">(Dai and Le, 2015)</ref>, to the successful transformer-based architectures such as GPT2 <ref type="bibr" target="#b48">(Radford et al., 2019)</ref>, BERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b35">(Liu et al., 2019)</ref> and more recently ALBERT <ref type="bibr" target="#b33">(Lan et al., 2019)</ref> and T5 <ref type="bibr" target="#b49">(Raffel et al., 2019)</ref>.</p><p>Non-English contextualized models Following the success of large pretrained language models, they were extended to the multilingual setting with multilingual BERT (hereafter mBERT) <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref>, a single multilingual model for 104 different languages trained on Wikipedia data, and later XLM <ref type="bibr" target="#b32">(Lample and Conneau, 2019)</ref>, which significantly improved unsupervized machine translation. More recently XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2019)</ref>, extended XLM by training on 2.5TB of data and outperformed previous scores on multilingual benchmarks. They show that multilingual models can obtain results competitive with monolingual models by leveraging higher quality data from other languages on specific downstream tasks.</p><p>A few non-English monolingual models have been released: ELMo models for Japanese, Portuguese, German and Basque<ref type="foot" target="#foot_0">2</ref> and BERT for Simplified and Traditional Chinese <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref> and German <ref type="bibr" target="#b9">(Chan et al., 2019)</ref>.</p><p>However, to the best of our knowledge, no particular effort has been made toward training models for languages other than English at a scale similar to the latest English models (e.g. RoBERTa trained on more than 100GB of data).</p><p>BERT and RoBERTa Our approach is based on RoBERTa <ref type="bibr" target="#b35">(Liu et al., 2019)</ref> which itself is based on BERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>. BERT is a multi-layer bidirectional Transformer encoder trained with a masked language modeling (MLM) objective, inspired by the Cloze task <ref type="bibr" target="#b58">(Taylor, 1953)</ref>. It comes in two sizes: the BERT BASE architecture and the BERT LARGE architecture. The BERT BASE architecture is 3 times smaller and therefore faster and easier to use while BERT LARGE achieves increased performance on downstream tasks. RoBERTa improves the original implementation of BERT by identifying key design choices for better performance, using dynamic masking, removing the next sentence prediction task, training with larger batches, on more data, and for longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Downstream evaluation tasks</head><p>In this section, we present the four downstream tasks that we use to evaluate CamemBERT, namely: Part-Of-Speech (POS) tagging, dependency parsing, Named Entity Recognition (NER) and Natural Language Inference (NLI). We also present the baselines that we will use for comparison.</p><p>Tasks POS tagging is a low-level syntactic task, which consists in assigning to each word its corresponding grammatical category. Dependency parsing consists in predicting the labeled syntactic tree in order to capture the syntactic relations between words.</p><p>For both of these tasks we run our experiments using the Universal Dependencies (UD)<ref type="foot" target="#foot_1">3</ref> framework and its corresponding UD POS tag set <ref type="bibr" target="#b46">(Petrov et al., 2012)</ref> and UD treebank collection <ref type="bibr">(Nivre et al., 2018)</ref>, which was used for the CoNLL 2018 shared task <ref type="bibr" target="#b53">(Seker et al., 2018)</ref>. We perform our evaluations on the four freely available French UD treebanks in UD v2.2: GSD <ref type="bibr" target="#b37">(McDonald et al., 2013)</ref>, Sequoia<ref type="foot" target="#foot_2">4</ref>  <ref type="bibr" target="#b8">(Candito and Seddah, 2012;</ref><ref type="bibr" target="#b7">Candito et al., 2014)</ref>, Spoken <ref type="bibr" target="#b28">(Lacheret et al., 2014;</ref><ref type="bibr" target="#b3">Bawden et al., 2014)</ref> <ref type="foot" target="#foot_3">5</ref> , and ParTUT <ref type="bibr" target="#b51">(Sanguinetti and Bosco, 2015)</ref>. A brief overview of the size and content of each treebank can be found in Table <ref type="table">1</ref>.   We also evaluate our model in NER, which is a sequence labeling task predicting which words refer to real-world objects, such as people, locations, artifacts and organisations. We use the French Treebank<ref type="foot" target="#foot_4">6</ref> (FTB) <ref type="bibr" target="#b0">(Abeillé et al., 2003)</ref> in its 2008 version introduced by <ref type="bibr" target="#b6">Candito and Crabbé (2009)</ref> and with NER annotations by <ref type="bibr" target="#b50">Sagot et al. (2012)</ref>. The FTB contains more than 11 thousand entity mentions distributed among 7 different entity types. A brief overview of the FTB can also be found in Table <ref type="table">1</ref>.</p><formula xml:id="formula_0">••••••••••••••••••• Medical, News Sequoia 68,615 3,099 Non-fiction, Wiki •••••••••••••••••••• Spoken 34,972 2,786 Spoken •••••••••••••••••••• ParTUT 27,658 1,020 Legal, News, Wikis •••••••••••••••••••• FTB 350,</formula><p>Finally, we evaluate our model on NLI, using the French part of the XNLI dataset <ref type="bibr" target="#b11">(Conneau et al., 2018)</ref>. NLI consists in predicting whether a hypothesis sentence is entailed, neutral or contradicts a premise sentence. The XNLI dataset is the exten-sion of the Multi-Genre NLI (MultiNLI) corpus <ref type="bibr" target="#b63">(Williams et al., 2018)</ref> to 15 languages by translating the validation and test sets manually into each of those languages. The English training set is machine translated for all languages other than English. The dataset is composed of 122k train, 2490 development and 5010 test examples for each language. As usual, NLI performance is evaluated using accuracy.</p><p>Baselines In dependency parsing and POStagging we compare our model with:</p><p>• mBERT: The multilingual cased version of BERT (see Section 2.1). We fine-tune mBERT on each of the treebanks with an additional layer for POS-tagging and dependency parsing, in the same conditions as our Camem-BERT model.</p><p>• XLM MLM-TLM : A multilingual pretrained language model from <ref type="bibr" target="#b32">Lample and Conneau (2019)</ref>, which showed better performance than mBERT on NLI. We use the version available in the Hugging's Face transformer library <ref type="bibr" target="#b64">(Wolf et al., 2019)</ref>; like mBERT, we fine-tune it in the same conditions as our model.</p><p>• UDify <ref type="bibr" target="#b24">(Kondratyuk, 2019)</ref>: A multitask and multilingual model based on mBERT, UDify is trained simultaneously on 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages. We report the scores from <ref type="bibr" target="#b24">Kondratyuk (2019)</ref> paper.</p><p>• UDPipe Future <ref type="bibr" target="#b55">(Straka, 2018)</ref>: An LSTMbased model ranked 3 rd in dependency parsing and 6 th in POS tagging at the CoNLL 2018 shared task <ref type="bibr" target="#b53">(Seker et al., 2018)</ref>. We report the scores from Kondratyuk (2019) paper.</p><p>• UDPipe Future + mBERT + Flair <ref type="bibr" target="#b56">(Straka et al., 2019)</ref>: The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings. We report the scores from <ref type="bibr" target="#b56">Straka et al. (2019)</ref> paper.</p><p>In French, no extensive work has been done on NER due to the limited availability of annotated corpora. Thus we compare our model with the only recent available baselines set by <ref type="bibr" target="#b17">Dupont (2017)</ref>, who trained both CRF <ref type="bibr" target="#b29">(Lafferty et al., 2001)</ref> and BiLSTM-CRF <ref type="bibr" target="#b31">(Lample et al., 2016)</ref> architectures on the FTB and enhanced them using heuristics and pretrained word embeddings. Additionally, as for POS and dependency parsing, we compare our model to a fine-tuned version of mBERT for the NER task.</p><p>For XNLI, we provide the scores of mBERT which has been reported for French by <ref type="bibr" target="#b65">Wu and Dredze (2019)</ref>. We report scores from XLM MLM-TLM (described above), the best model from <ref type="bibr" target="#b32">Lample and Conneau (2019)</ref>. We also report the results of XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CamemBERT: a French Language Model</head><p>In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for CamemBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training data</head><p>Pretrained language models benefits from being trained on large datasets <ref type="bibr" target="#b14">(Devlin et al., 2018;</ref><ref type="bibr" target="#b35">Liu et al., 2019;</ref><ref type="bibr" target="#b49">Raffel et al., 2019)</ref>. We therefore use the French part of the OSCAR corpus <ref type="bibr">(Ortiz Suárez et al., 2019)</ref>, a pre-filtered and pre-classified version of Common Crawl. 7</p><p>OSCAR is a set of monolingual corpora extracted from Common Crawl snapshots. It follows the same approach as <ref type="bibr" target="#b18">(Grave et al., 2018)</ref> by using a language classification model based on the fastText linear classifier <ref type="bibr" target="#b19">(Grave et al., 2017;</ref><ref type="bibr" target="#b22">Joulin et al., 2016)</ref> pretrained on Wikipedia, Tatoeba and SETimes, which supports 176 languages. No other filtering is done. We use a non-shuffled version of the French data, which amounts to 138GB of raw text and 32.7B tokens after subword tokenization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-processing</head><p>We segment the input text data into subword units using SentencePiece <ref type="bibr" target="#b27">(Kudo and Richardson, 2018)</ref>.</p><p>SentencePiece is an extension of Byte-Pair encoding (BPE) <ref type="bibr" target="#b54">(Sennrich et al., 2016)</ref> and WordPiece <ref type="bibr" target="#b26">(Kudo, 2018)</ref> that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. We use a vocabulary size of 32k subword tokens. These subwords are learned on 10 7 sentences sampled randomly from the pretraining dataset. We do not use subword regularisation (i.e. sampling from multiple possible segmentations) for the sake of simplicity.</p><p>7 https://commoncrawl.org/about/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language Modeling</head><p>Transformer Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref>. Given the widespread usage of Transformers, we do not describe them here and refer the reader to <ref type="bibr" target="#b60">(Vaswani et al., 2017)</ref>. CamemBERT uses the original architectures of BERT BASE (12 layers, 768 hidden dimensions, 12 attention heads, 110M parameters) and BERT LARGE (24 layers, 1024 hidden dimensions, 16 attention heads, 335M parameters).</p><p>CamemBERT is very similar to RoBERTa, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization <ref type="bibr" target="#b27">(Kudo and Richardson, 2018)</ref> instead of WordPiece <ref type="bibr" target="#b52">(Schuster and Nakajima, 2012)</ref>.</p><p>Pretraining Objective We train our model on the Masked Language Modeling (MLM) task.</p><p>Given an input text sequence composed of N tokens x 1 , ..., x N , we select 15% of tokens for possible replacement. Among those selected tokens, 80% are replaced with the special &lt;MASK&gt; token, 10% are left unchanged and 10% are replaced by a random token. The model is then trained to predict the initial masked tokens using cross-entropy loss. Following the RoBERTa approach, we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing. This improves variability and makes the model more robust when training for multiple epochs.</p><p>Since we use SentencePiece to tokenize our corpus, the input tokens to the model are a mix of whole words and subwords. An upgraded version of BERT<ref type="foot" target="#foot_5">8</ref> and <ref type="bibr" target="#b21">Joshi et al. (2019)</ref> have shown that masking whole words instead of individual subwords leads to improved performance. Whole-word Masking (WWM) makes the training task more difficult because the model has to predict a whole word rather than predicting only part of the word given the rest. We train our models using WWM by using whitespaces in the initial untokenized text as word delimiters.</p><p>WWM is implemented by first randomly sampling 15% of the words in the sequence and then considering all subword tokens in each of this 15% for candidate replacement. This amounts to a proportion of selected tokens that is close to the original 15%. These tokens are then either replaced by &lt;MASK&gt; tokens (80%), left unchanged (10%) or replaced by a random token.</p><p>Subsequent work has shown that the next sentence prediction (NSP) task originally used in BERT does not improve downstream task performance <ref type="bibr" target="#b32">(Lample and Conneau, 2019;</ref><ref type="bibr" target="#b35">Liu et al., 2019)</ref>, thus we also remove it.</p><p>Optimisation Following <ref type="bibr" target="#b35">(Liu et al., 2019)</ref>, we optimize the model using Adam (Kingma and Ba, 2014) (β 1 = 0.9, β 2 = 0.98) for 100k steps with large batch sizes of 8192 sequences, each sequence containing at most 512 tokens. We enforce each sequence to only contain complete paragraphs (which correspond to lines in the our pretraining dataset).</p><p>Pretraining We use the RoBERTa implementation in the fairseq library <ref type="bibr" target="#b43">(Ott et al., 2019)</ref>. Our learning rate is warmed up for 10k steps up to a peak value of 0.0007 instead of the original 0.0001 given our large batch size, and then fades to zero with polynomial decay. Unless otherwise specified, our models use the BASE architecture, and are pretrained for 100k backpropagation steps on 256 Nvidia V100 GPUs (32GB each) for a day. We do not train our models for longer due to practical considerations, even though the performance still seemed to be increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Using CamemBERT for downstream tasks</head><p>We use the pretrained CamemBERT in two ways. In the first one, which we refer to as fine-tuning, we fine-tune the model on a specific task in an endto-end manner. In the second one, referred to as feature-based embeddings or simply embeddings, we extract frozen contextual embedding vectors from CamemBERT. These two complementary approaches shed light on the quality of the pretrained hidden representations captured by CamemBERT.</p><p>Fine-tuning For each task, we append the relevant predictive layer on top of CamemBERT's architecture. Following the work done on BERT <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>, for sequence tagging and sequence labeling we append a linear layer that respectively takes as input the last hidden representation of the &lt;s&gt; special token and the last hidden representation of the first subword token of each word. For dependency parsing, we plug a bi-affine graph predictor head as inspired by <ref type="bibr" target="#b16">Dozat and Manning (2017)</ref>. We refer the reader to this article for more details on this module. We fine-tune on XNLI by adding a classification head composed of one hidden layer with a non-linearity and one linear projection layer, with input dropout for both. We fine-tune CamemBERT independently for each task and each dataset. We optimize the model using the Adam optimiser (Kingma and Ba, 2014) with a fixed learning rate. We run a grid search on a combination of learning rates and batch sizes. We select the best model on the validation set out of the 30 first epochs. For NLI we use the default hyperparameters provided by the authors of RoBERTa on the MNLI task. 9 Although this might have pushed the performances even further, we do not apply any regularisation techniques such as weight decay, learning rate warm-up or discriminative finetuning, except for NLI. We show that fine-tuning CamemBERT in a straightforward manner leads to state-of-the-art results on all tasks and outperforms the existing BERT-based models in all cases. The POS tagging, dependency parsing, and NER experiments are run using Hugging Face's Transformer library extended to support CamemBERT and dependency parsing <ref type="bibr" target="#b64">(Wolf et al., 2019)</ref>. The NLI experiments use the fairseq library following the RoBERTa implementation.</p><p>Embeddings Following <ref type="bibr" target="#b57">Straková et al. (2019)</ref> and <ref type="bibr" target="#b56">Straka et al. (2019)</ref> for mBERT and the English BERT, we make use of CamemBERT in a feature-based embeddings setting. In order to obtain a representation for a given token, we first compute the average of each sub-word's representations in the last four layers of the Transformer, and then average the resulting sub-word vectors.</p><p>We evaluate CamemBERT in the embeddings setting for POS tagging, dependency parsing and NER; using the open-source implementations of <ref type="bibr" target="#b56">Straka et al. (2019)</ref>    <ref type="bibr" target="#b17">(Dupont, 2017)</ref> 85.57 mBERT (fine-tuned) 87.35</p><formula xml:id="formula_1">•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• CamemBERT (fine-</formula><formula xml:id="formula_2">••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••</formula><p>CamemBERT (fine-tuned) 89.08 LSTM+CRF+CamemBERT (embeddings) 89.55  <ref type="bibr" target="#b10">(Conneau et al., 2019)</ref> 80.1 270M</p><formula xml:id="formula_3">••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• CamemBERT (fine-tuned) 82.5<label>110M</label></formula><p>Supplement: LARGE models XLM-RLARGE <ref type="bibr" target="#b10">(Conneau et al., 2019)</ref> 85.2 550M POS tagging and dependency parsing For POS tagging and dependency parsing, we compare CamemBERT with other models in the two settings: fine-tuning and as feature-based embeddings.</p><formula xml:id="formula_4">••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• CamemBERTLARGE (fine-tuned) 85.7 335M</formula><p>We report the results in Table <ref type="table" target="#tab_4">2</ref>.</p><p>CamemBERT reaches state-of-the-art scores on all treebanks and metrics in both scenarios. The two approaches achieve similar scores, with a slight advantage for the fine-tuned version of CamemBERT, thus questioning the need for complex task-specific architectures such as UDPipe Future.</p><p>Despite a much simpler optimisation process and no task specific architecture, fine-tuning Camem-BERT outperforms UDify on all treebanks and sometimes by a large margin (e.g. +4.15% LAS on Sequoia and +5.37 LAS on ParTUT). Camem-BERT also reaches better performance than other multilingual pretrained models such as mBERT and XLM MLM-TLM on all treebanks.</p><p>CamemBERT achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+mBERT +Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where CamemBERT lags by 0.03% and 0.14% UPOS respectively. UDPipe Fu-ture+mBERT +Flair uses the contextualized string embeddings Flair <ref type="bibr" target="#b1">(Akbik et al., 2018)</ref>, which are in fact pretrained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes. This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT +Flair model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Named-Entity Recognition</head><p>For NER, we similarly evaluate CamemBERT in the fine-tuning setting and as input embeddings to the task specific architecture LSTM+CRF. We report these scores in Table <ref type="table" target="#tab_5">3</ref>.</p><p>In both scenarios, CamemBERT achieves higher F1 scores than the traditional CRF-based architectures, both non-neural and neural, and than finetuned multilingual BERT models. <ref type="foot" target="#foot_7">11</ref>Using CamemBERT as embeddings to the traditional LSTM+CRF architecture gives slightly higher scores than by fine-tuning the model (89.08 vs. 89.55). This demonstrates that although CamemBERT can be used successfully without any task-specific architecture, it can still produce high quality contextualized embeddings that might be useful in scenarios where powerful downstream architectures exist.</p><p>Natural Language Inference On the XNLI benchmark, we compare CamemBERT to previous state-of-the-art multilingual models in the finetuning setting. In addition to the standard Camem-BERT model with a BASE architecture, we train another model with the LARGE architecture, referred to as CamemBERT LARGE , for a fair comparison with XLM-R LARGE . This model is trained with the CCNet corpus, described in Sec. 6, for 100k steps. <ref type="foot" target="#foot_8">12</ref> We expect that training the model for longer would yield even better performance.</p><p>CamemBERT reaches higher accuracy than its BASE counterparts reaching +5.6% over mBERT, +2.3 over XLM MLM-TLM , and +2.4 over XLM-R BASE . CamemBERT also uses as few as half as many parameters (110M vs. 270M for XLM-R BASE ).</p><p>CamemBERT LARGE achieves a state-of-the-art accuracy of 85.7% on the XNLI benchmark, as opposed to 85.2, for the recent XLM-R LARGE .</p><p>CamemBERT uses fewer parameters than multilingual models, mostly because of its smaller vocabulary size (e.g. 32k vs. 250k for XLM-R). Two elements might explain the better performance of CamemBERT over XLM-R. Even though XLM-R was trained on an impressive amount of data (2.5TB), only 57GB of this data is in French, whereas we used 138GB of French data. Additionally XLM-R also handles 100 languages, and the authors show that when reducing the number of languages to 7, they can reach 82.5% accuracy for French XNLI with their BASE architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of CamemBERT's results</head><p>Camem-BERT improves the state of the art for the 4 downstream tasks considered, thereby confirming on French the usefulness of Transformer-based models. We obtain these results when using Camem-BERT as a fine-tuned model or when used as contextual embeddings with task-specific architectures. This questions the need for more complex downstream architectures, similar to what was shown for English <ref type="bibr" target="#b15">(Devlin et al., 2019)</ref>. Additionally, this suggests that CamemBERT is also able to produce high-quality representations out-of-the-box without further tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Impact of corpus origin and size</head><p>In this section we investigate the influence of the homogeneity and size of the pretraining corpus on downstream task performance. With this aim, we train alternative version of CamemBERT by varying the pretraining datasets. For this experiment, we fix the number of pretraining steps to 100k, and allow the number of epochs to vary accordingly (more epochs for smaller dataset sizes). All models use the BASE architecture.</p><p>In order to investigate the need for homogeneous clean data versus more diverse and possibly noisier data, we use alternative sources of pretraining data in addition to OSCAR:</p><p>• Wikipedia, which is homogeneous in terms of genre and style. We use the official 2019 French Wikipedia dumps<ref type="foot" target="#foot_9">13</ref> . We remove HTML tags and tables using Giuseppe Attardi's WikiExtractor.<ref type="foot" target="#foot_10">14</ref> </p><p>• CCNet <ref type="bibr" target="#b62">(Wenzek et al., 2019)</ref>, a dataset extracted from Common Crawl with a different filtering process than for OSCAR. It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables. <ref type="foot" target="#foot_11">15</ref> As this filtering step biases the noisy data from Common Crawl to more Wikipedia-like text, we expect CCNet to act as a middle ground between the unfiltered "noisy" OSCAR dataset, and the "clean" Wikipedia dataset. As a result of the different filtering processes, CCNet contains longer documents on average compared to OSCAR with smaller-and often noisier-documents weeded out.</p><p>Table <ref type="table" target="#tab_10">6</ref> summarizes statistics of these different corpora.</p><p>In order to make the comparison between these three sources of pretraining data, we randomly sample 4GB of text (at the document level) from OS-CAR and CCNet, thereby creating samples of both Common-Crawl-based corpora of the same size as the French Wikipedia. These smaller 4GB samples also provides us a way to investigate the impact   of pretraining data size. Downstream task performance for our alternative versions of CamemBERT are provided in Table <ref type="table" target="#tab_9">5</ref>. The upper section reports scores in the fine-tuning setting while the lower section reports scores for the embeddings.</p><formula xml:id="formula_5">••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• OSCAR 138GB<label>98</label></formula><formula xml:id="formula_6">••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• OSCAR 138GB</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Common Crawl vs. Wikipedia?</head><p>Table <ref type="table" target="#tab_9">5</ref> clearly shows that models trained on the 4GB versions of OSCAR and CCNet (Common Crawl) perform consistently better than the the one trained on the French Wikipedia. This is true both in the fine-tuning and embeddings setting. Unsurprisingly, the gap is larger on tasks involving texts whose genre and style are more divergent from those of Wikipedia, such as tagging and parsing on the Spoken treebank. The performance gap is also very large on the XNLI task, probably as a consequence of the larger diversity of Common-Crawl-based corpora in terms of genres and topics. XNLI is indeed based on multiNLI which covers a range of genres of spoken and written text.</p><p>The downstream task performances of the models trained on the 4GB version of CCNet and OS-CAR are much more similar. 16 16 We provide the results of a model trained on the whole CCNet corpus in the Appendix. The conclusions are similar when comparing models trained on the full corpora: downstream results are similar when using OSCAR or CCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">How much data do you need?</head><p>An unexpected outcome of our experiments is that the model trained "only" on the 4GB sample of OS-CAR performs similarly to the standard Camem-BERT trained on the whole 138GB OSCAR. The only task with a large performance gap is NER, where "138GB" models are better by 0.9 F1 points. This could be due to the higher number of named entities present in the larger corpora, which is beneficial for this task. On the contrary, other tasks don't seem to gain from the additional data.</p><p>In other words, when trained on corpora such as OSCAR and CCNet, which are heterogeneous in terms of genre and style, 4GB of uncompressed text is large enough as pretraining corpus to reach state-of-the-art results with the BASE architecure, better than those obtained with mBERT (pretrained on 60GB of text). 17 This calls into question the need to use a very large corpus such as OSCAR or CCNet when training a monolingual Transformerbased language model such as BERT or RoBERTa. Not only does this mean that the computational (and therefore environmental) cost of training a state-of-the-art language model can be reduced, but it also means that CamemBERT-like models can be trained for all languages for which a Common-Crawl-based corpus of 4GB or more can be created. OSCAR is available in 166 languages, and provides such a corpus for 38 languages. Moreover, it is possible that slightly smaller corpora (e.g. down to 1GB) could also prove sufficient to train highperforming language models. We obtained our results with BASE architectures. Further research is needed to confirm the validity of our findings on larger architectures and other more complex natural language understanding tasks. However, even with a BASE architecture and 4GB of training data, the validation loss is still decreasing beyond 100k steps (and 400 epochs). This suggests that we are still under-fitting the 4GB pretraining dataset, training longer might increase downstream performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Since the pre-publication of this work <ref type="bibr" target="#b36">(Martin et al., 2019)</ref>, many monolingual language models have appeared, e.g. <ref type="bibr" target="#b34">(Le et al., 2019;</ref><ref type="bibr" target="#b61">Virtanen et al., 2019;</ref><ref type="bibr" target="#b13">Delobelle et al., 2020)</ref>, for as much as 30 languages <ref type="bibr" target="#b41">(Nozza et al., 2020)</ref>. In almost all tested configurations they displayed better results than multilingual language models such as mBERT <ref type="bibr" target="#b47">(Pires et al., 2019)</ref>. Interestingly, <ref type="bibr" target="#b34">Le et al. (2019)</ref> showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to Camem-BERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models. <ref type="foot" target="#foot_13">18</ref> As it was the case for English when BERT was first released, the availability of similar scale language models for French enabled interesting applications, such as large scale anonymization of legal texts, where CamemBERT-based models established a new state-of-the-art on this task <ref type="bibr" target="#b4">(Benesty, 2019)</ref>, or the first large question answering experiments on a French Squad data set that was released very recently <ref type="bibr">(d'Hoffschmidt et al., 2020)</ref> where the authors matched human performance using CamemBERT LARGE . Being the first pre-trained language model that used the opensource Common Crawl Oscar corpus and given its impact on the community, CamemBERT paved the way for many works on monolingual language models that followed. Furthermore, the availability of all its training data favors reproducibility and is a step towards better understanding such models. In that spirit, we make the models used in our experiments available via our website and via the huggingface and fairseq APIs, in addition to the base CamemBERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we investigated the feasibility of training a Transformer-based language model for lan-guages other than English. Using French as an example, we trained CamemBERT, a language model based on RoBERTa. We evaluated Camem-BERT on four downstream tasks (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference) in which our best model reached or improved the state of the art in all tasks considered, even when compared to strong multilingual models such as mBERT, XLM and XLM-R, while also having fewer parameters.</p><p>Our experiments demonstrate that using web crawled data with high variability is preferable to using Wikipedia-based data. In addition we showed that our models could reach surprisingly high performances with as low as 4GB of pretraining data, questioning thus the need for large scale pretraining corpora. This shows that state-of-the-art Transformer-based language models can be trained on languages with far fewer resources than English, whenever a few gigabytes of data are available. This paves the way for the rise of monolingual contextual pre-trained language-models for under-resourced languages. The question of knowing whether pretraining on small domain specific content will be a better option than transfer learning techniques such as fine-tuning remains open and we leave it for future work.</p><p>Pretrained on pure open-source corpora, Camem-BERT is freely available and distributed with the MIT license via popular NLP libraries (fairseq and huggingface) as well as on our website camembert-model.fr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Impact of Whole-Word Masking</head><p>In Table <ref type="table" target="#tab_13">8</ref>, we compare models trained using the traditional subword masking with whole-word masking. Whole-Word Masking positively impacts downstream performances for NLI (although only by 0.5 points of accuracy). To our surprise, this Whole-Word Masking scheme does not benefit much lower level task such as Name Entity Recognition, POS tagging and Dependency Parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Impact of model size</head><p>Table <ref type="table" target="#tab_13">8</ref> compares models trained with the BASE and LARGE architectures. These models were trained with the CCNet corpus (135GB) for practical reasons. We confirm the positive influence of larger models on the NLI and NER tasks. The LARGE architecture leads to respectively 19.7% error reduction and 23.7%. To our surprise, on POS tagging and dependency parsing, having three time more parameters doesn't lead to a significant difference compared to the BASE model. <ref type="bibr" target="#b59">Tenney et al. (2019)</ref> and <ref type="bibr" target="#b20">Jawahar et al. (2019)</ref> have shown that low-level syntactic capabilities are learnt in lower layers of BERT while higher level semantic representations are found in upper layers of BERT. POS tagging and dependency parsing probably do not benefit from adding more layers as the lower layers of the BASE architecture already capture what is necessary to complete these tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Impact of training dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Impact of number of steps</head><p>Figure <ref type="figure" target="#fig_0">1</ref> displays the evolution of downstream task performance with respect to the number of steps. All scores in this section are averages from at least 4 runs with different random seeds. For POS tagging and dependency parsing, we also average the scores on the 4 treebanks. We evaluate our model at every epoch (1 epoch equals 8360 steps). We report the masked language modelling perplexity along with downstream performances. Figure <ref type="figure" target="#fig_0">1</ref>, suggests that the more complex the task the more impactful the number of steps is. We observe an early plateau for dependency parsing and NER at around 22k steps, while for NLI, even if the marginal improvement with regard to pretraining steps becomes smaller, the performance is still slowly increasing at 100k steps.</p><p>In Table <ref type="table" target="#tab_13">8</ref>, we compare two models trained on CCNet, one for 100k steps and the other for 500k steps to evaluate the influence of the total number of steps. The model trained for 500k steps does not increase the scores much from just training for 100k steps in POS tagging and parsing. The increase is slightly higher for XNLI (+0.84).</p><p>Those results suggest that low level syntactic representation are captured early in the language model training process while it needs more steps to extract complex semantic information as needed for NLI. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Impact of number of pretraining steps on downstream performance for CamemBERT..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and Straková et al. (2019). 10  </figDesc><table><row><cell></cell><cell cols="2">GSD</cell><cell cols="2">SEQUOIA</cell><cell cols="2">SPOKEN</cell><cell cols="2">PARTUT</cell></row><row><cell>MODEL</cell><cell cols="2">UPOS LAS</cell><cell cols="2">UPOS LAS</cell><cell cols="2">UPOS LAS</cell><cell cols="2">UPOS LAS</cell></row><row><cell>mBERT (fine-tuned)</cell><cell>97.48</cell><cell>89.73</cell><cell>98.41</cell><cell>91.24</cell><cell>96.02</cell><cell>78.63</cell><cell>97.35</cell><cell>91.37</cell></row><row><cell>XLMMLM-TLM (fine-tuned)</cell><cell>98.13</cell><cell>90.03</cell><cell>98.51</cell><cell>91.62</cell><cell>96.18</cell><cell>80.89</cell><cell>97.39</cell><cell>89.43</cell></row><row><cell>UDify (Kondratyuk, 2019)</cell><cell>97.83</cell><cell>91.45</cell><cell>97.89</cell><cell>90.05</cell><cell>96.23</cell><cell>80.01</cell><cell>96.12</cell><cell>88.06</cell></row><row><cell>UDPipe Future (Straka, 2018)</cell><cell>97.63</cell><cell>88.06</cell><cell>98.79</cell><cell>90.73</cell><cell>95.91</cell><cell>77.53</cell><cell>96.93</cell><cell>89.63</cell></row><row><cell>+ mBERT + Flair (emb.) (Straka et al., 2019)</cell><cell>97.98</cell><cell>90.31</cell><cell>99.32</cell><cell>93.81</cell><cell>97.23</cell><cell>81.40</cell><cell>97.64</cell><cell>92.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>POS and dependency parsing scores on 4 French treebanks, reported on test sets assuming gold tokenization and segmentation (best model selected on validation out of 4). Best scores in bold, second best underlined.</figDesc><table><row><cell>tuned)</cell><cell>98.18</cell><cell>92.57</cell><cell>99.29</cell><cell>94.20</cell><cell>96.99</cell><cell>81.37</cell><cell>97.65</cell><cell>93.43</cell></row><row><cell>UDPipe Future + CamemBERT (embeddings)</cell><cell>97.96</cell><cell>90.57</cell><cell>99.25</cell><cell>93.89</cell><cell>97.09</cell><cell>81.81</cell><cell>97.50</cell><cell>92.32</cell></row><row><cell>Model</cell><cell>F1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SEM (CRF) (Dupont, 2017)</cell><cell>85.02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LSTM-CRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>NER scores on the FTB (best model selected on validation out of 4). Best scores in bold, second best underlined.</figDesc><table><row><cell>Model</cell><cell cols="2">Acc. #Params</cell></row><row><cell>mBERT (Devlin et al., 2019)</cell><cell>76.9</cell><cell>175M</cell></row><row><cell cols="2">XLMMLM-TLM (Lample and Conneau, 2019) 80.2</cell><cell>250M</cell></row><row><cell>XLM-RBASE</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>NLI</figDesc><table /><note><p>accuracy on the French XNLI test set (best model selected on validation out of 10). Best scores in bold, second best underlined.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Results on the four tasks using language models pre-trained on data sets of varying homogeneity and size, reported on validation sets (average of 4 runs for POS tagging, parsing and NER, average of 10 runs for NLI).</figDesc><table><row><cell></cell><cell></cell><cell>98.18</cell><cell>92.77</cell><cell>99.14</cell><cell>94.24</cell><cell>97.26</cell><cell>82.44</cell><cell>96.52</cell><cell>89.89</cell><cell>97.77</cell><cell>89.84</cell><cell>91.83</cell><cell>-</cell></row><row><cell>Corpus</cell><cell>Size</cell><cell cols="2">#tokens #docs</cell><cell></cell><cell>Tokens/doc</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Percentiles:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">5% 50% 95%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wikipedia</cell><cell>4GB</cell><cell>990M</cell><cell>1.4M</cell><cell cols="3">102 363 2530</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CCNet</cell><cell>135GB</cell><cell>31.9B</cell><cell cols="2">33.1M 128</cell><cell cols="2">414 2869</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSCAR</cell><cell>138GB</cell><cell>32.7B</cell><cell>59.4M</cell><cell>28</cell><cell cols="2">201 1946</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Statistics on the pretraining datasets used.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Table 8 compares models trained on CCNet and on OSCAR. The major difference between the two datasets is the additional filtering step of CCNet that favors Wikipedia-Like texts. The model pretrained on OSCAR gets slightly better results on POS tagging and dependency parsing, but gets a</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Performance reported on Test sets for all trained models (average over multiple fine-tuning seeds).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">GSD</cell><cell cols="2">SEQUOIA</cell><cell cols="2">SPOKEN</cell><cell cols="2">PARTUT</cell><cell>NER</cell><cell>NLI</cell></row><row><cell cols="2">DATASET MASKING</cell><cell cols="2">ARCH. #STEPS</cell><cell cols="2">UPOS LAS</cell><cell cols="2">UPOS LAS</cell><cell cols="2">UPOS LAS</cell><cell cols="2">UPOS LAS</cell><cell>F1</cell><cell>ACC.</cell></row><row><cell cols="2">Fine-tuning</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSCAR</cell><cell>Subword</cell><cell>BASE</cell><cell>100k</cell><cell>98.25</cell><cell>92.29</cell><cell>99.25</cell><cell>93.70</cell><cell>96.95</cell><cell>79.96</cell><cell>97.73</cell><cell>92.68</cell><cell>89.23</cell><cell>81.18</cell></row><row><cell>OSCAR</cell><cell>Whole-word</cell><cell>BASE</cell><cell>100k</cell><cell>98.21</cell><cell>92.30</cell><cell>99.21</cell><cell>94.33</cell><cell>96.97</cell><cell>80.16</cell><cell>97.78</cell><cell>92.65</cell><cell>89.11</cell><cell>81.92</cell></row><row><cell>CCNET</cell><cell>Subword</cell><cell>BASE</cell><cell>100k</cell><cell>98.02</cell><cell>92.06</cell><cell>99.26</cell><cell>94.13</cell><cell>96.94</cell><cell>80.39</cell><cell>97.55</cell><cell>92.66</cell><cell>89.05</cell><cell>81.77</cell></row><row><cell>CCNET</cell><cell>Whole-word</cell><cell>BASE</cell><cell>100k</cell><cell>98.03</cell><cell>92.43</cell><cell>99.18</cell><cell>94.26</cell><cell>96.98</cell><cell>80.89</cell><cell>97.46</cell><cell>92.33</cell><cell>89.27</cell><cell>81.92</cell></row><row><cell>CCNET</cell><cell>Whole-word</cell><cell>BASE</cell><cell>500k</cell><cell>98.21</cell><cell>92.43</cell><cell>99.24</cell><cell>94.60</cell><cell>96.69</cell><cell>80.97</cell><cell>97.65</cell><cell>92.48</cell><cell>89.08</cell><cell>83.43</cell></row><row><cell>CCNET</cell><cell cols="2">Whole-word LARGE</cell><cell>100k</cell><cell>98.01</cell><cell>91.09</cell><cell>99.23</cell><cell>93.65</cell><cell>97.01</cell><cell>80.89</cell><cell>97.41</cell><cell>92.59</cell><cell>89.39</cell><cell>85.29</cell></row><row><cell cols="7">Embeddings (with UDPipe Future (tagging, parsing) or LSTM+CRF (NER))</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OSCAR</cell><cell>Subword</cell><cell>BASE</cell><cell>100k</cell><cell>98.01</cell><cell>90.64</cell><cell>99.27</cell><cell>94.26</cell><cell>97.15</cell><cell>82.56</cell><cell>97.70</cell><cell>92.70</cell><cell>90.25</cell><cell>-</cell></row><row><cell>OSCAR</cell><cell>Whole-word</cell><cell>BASE</cell><cell>100k</cell><cell>97.97</cell><cell>90.44</cell><cell>99.23</cell><cell>93.93</cell><cell>97.08</cell><cell>81.74</cell><cell>97.50</cell><cell>92.28</cell><cell>89.48</cell><cell>-</cell></row><row><cell>CCNET</cell><cell>Subword</cell><cell>BASE</cell><cell>100k</cell><cell>97.87</cell><cell>90.78</cell><cell>99.20</cell><cell>94.33</cell><cell>97.17</cell><cell>82.39</cell><cell>97.54</cell><cell>92.51</cell><cell>89.38</cell><cell>-</cell></row><row><cell>CCNET</cell><cell>Whole-word</cell><cell>BASE</cell><cell>100k</cell><cell>97.96</cell><cell>90.76</cell><cell>99.23</cell><cell>94.34</cell><cell>97.04</cell><cell>82.09</cell><cell>97.39</cell><cell>92.82</cell><cell>89.85</cell><cell>-</cell></row><row><cell>CCNET</cell><cell>Whole-word</cell><cell>BASE</cell><cell>500k</cell><cell>97.84</cell><cell>90.25</cell><cell>99.14</cell><cell>93.96</cell><cell>97.01</cell><cell>82.17</cell><cell>97.27</cell><cell>92.28</cell><cell>89.07</cell><cell>-</cell></row><row><cell>CCNET</cell><cell cols="2">Whole-word LARGE</cell><cell>100k</cell><cell>98.01</cell><cell>90.70</cell><cell>99.23</cell><cell>94.01</cell><cell>97.04</cell><cell>82.18</cell><cell>97.31</cell><cell>92.28</cell><cell>88.76</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Comparing scores on the Validation sets of different design choices. POS tagging and parsing datasets are averaged. (average over multiple fine-tuning seeds).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://allennlp.org/elmo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://universaldependencies.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://deep-sequoia.inria.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Speech transcript uncased that includes annotated disfluencies without punctuation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>6 This dataset has only been stored and used on Inria's servers after signing the research-only agreement.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>https://github.com/google-research/ bert/blob/master/README.md</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>Evaluation of CamemBERTIn this section, we measure the performance of our models by evaluating them on the four aforementioned tasks: POS tagging, dependency parsing, NER and NLI.9 More details at https://github.com/pytorch/ fairseq/blob/master/examples/roberta/ README.glue.md.10 UDPipe Future is available at https://github. com/CoNLL-UD-2018/UDPipe-Future, and the code for nested NER is available at https://github.com/ ufal/acl2019_nested_ner.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_7"><p>XLMMLM-TLM is a lower-case model. Case is crucial for NER, therefore we do not report its low performance (84.37%)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_8"><p>We train our LARGE model with the CCNet corpus for practical reasons. Given that BASE models reach similar performance when using OSCAR or CCNet as pretraining corpus (Appendix Table8), we expect an OSCAR LARGE model to reach comparable scores.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_9"><p>https://dumps.wikimedia.org/ backup-index.html.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_10"><p>https://github.com/attardi/ wikiextractor.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_11"><p>We use the HEAD split, which corresponds to the top 33% of documents in terms of filtering perplexity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_12"><p>The OSCAR-4GB model gets slightly better XNLI accuracy than the full OSCAR-138GB model(81.88 vs. 81.55). This might be due to the random seed used for pretraining, as each model is pretrained only once.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_13"><p>We refer the reader to<ref type="bibr" target="#b34">(Le et al., 2019)</ref> for a comprehensive benchmark and details therein.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to thank Clémentine Fourrier for her proofreading and insightful comments, and Alix Chagué for her great logo. This work was partly funded by three French National funded projects granted to Inria and other partners by the Agence Nationale de la Recherche, namely projects PARSITI (ANR-16-CE33-0021), SoSweet (ANR-15-CE38-0011) and BASNUM (ANR-18-CE38-0003), as well as by the last author's chair in the PRAIRIE institute funded by the French national agency ANR as part of the "Investissements d'avenir" programme under the reference ANR-19-P3IA-0001.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In the appendix, we analyse different design choices of CamemBERT (Table <ref type="table">8</ref>), namely with respect to the use of whole-word masking, the training dataset, the model size, and the number of training steps in complement with the analyses of the impact of corpus origin an size (Section 6. In all the ablations, all scores come from at least 4 averaged runs. For POS tagging and dependency parsing, we average the scores on the 4 treebanks. We also report all averaged test scores of our different models in Table <ref type="table">7</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lionel Clément, and François Toussenel</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Abeillé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="165" to="187" />
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
	<note>Building a Treebank for French</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08-20">2018. August 20-26, 2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ando</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Correcting and validating syntactic dependency in the spoken French treebank rhapsodie</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Amélie</forename><surname>Botalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Kahane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2320" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ner algo benchmark: spacy, flair, m-bert and camembert on anonymizing french commercial legal cases</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Benesty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>De Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving generative statistical parsing with semisupervised word clustering</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT&apos;09</title>
		<meeting>of IWPT&apos;09<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep syntax annotation of the sequoia french treebank</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Perrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Ribeyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karën</forename><surname>Fort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation, LREC 2014<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2014-05-26">2014. May 26-31, 2014</date>
			<biblScope unit="page" from="2298" to="2305" />
		</imprint>
	</monogr>
	<note>Djamé Seddah, and Éric Villemonte de la Clergerie</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Le corpus sequoia : annotation syntaxique et exploitation pour l&apos;adaptation d&apos;analyseur par pont lexical (the sequoia corpus : Syntactic annotation and use for a parser lexical domain adaptation method)</title>
		<author>
			<persName><forename type="first">Marie</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference JEP-TALN-RECITAL 2012</title>
		<meeting>the Joint Conference JEP-TALN-RECITAL 2012<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<publisher>TALN</publisher>
			<date type="published" when="2012-06-04">2012. June 4-8, 2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="334" />
		</imprint>
	</monogr>
	<note>in french</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Branden</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Pietsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanay</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin</forename><forename type="middle">Man</forename><surname>Yeung</surname></persName>
		</author>
		<ptr target="https://deepset.ai/german-bert" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>German</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv preprint : 1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">XNLI: evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised sequence learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. 2015. December 7-12, 2015</date>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
	<note>Montreal</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">RobBERT: a Dutch RoBERTabased Language Model</title>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Delobelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Winters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bettina</forename><surname>Berendt</surname></persName>
		</author>
		<idno>ArXiv preprint 2001.06286</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/bert/blob/master/multilingual.md" />
	</analytic>
	<monogr>
		<title level="j">Multilingual</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06071</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Maxime</forename><surname>Martin D'hoffschmidt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wacim</forename><surname>Vidal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tom</forename><surname>Belblidia</surname></persName>
		</editor>
		<editor>
			<persName><surname>Brendlé</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019. 2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>French question answering dataset</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploration de traits pour la reconnaissance d&apos;entit&apos;es nomm&apos;ees du français par apprentissage automatique</title>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24e Conf&apos;erence sur le Traitement Automatique des Langues Naturelles (TALN)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC 2018<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-07">2018. May 7-12, 2018</date>
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04-03">2017. 2017. April 3-7, 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">What does BERT learn about the structure of language?</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djamé</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1356</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno>CoRR, abs/1907.10529</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fasttext.zip: Compressing text classification models</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>1612.03651</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno>1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">75 languages, 1 model: Parsing universal dependencies universally</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kondratyuk</surname></persName>
		</author>
		<idno>CoRR, abs/1904.02099</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Traum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<title level="s">Long Papers. Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
	<note>EMNLP 2018: System Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rhapsodie: a prosodicsyntactic treebank for spoken French</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lacheret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Kahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Beliao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Dister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Gerdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Obin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paola</forename><surname>Pietrandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atanas</forename><surname>Tchobanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)</title>
		<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC&apos;14)<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="295" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06-28">2001. 2001. June 28 -July 1, 2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Kaufmann</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno>CoRR, abs/1901.07291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ALBERT: A lite BERT for selfsupervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno>ArXiv preprint 1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Flaubert: Unsupervised language model pre-training for french</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loïc</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jibril</forename><surname>Frej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Segonne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Crabbé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Schwab</surname></persName>
		</author>
		<idno>ArXiv : 1912.05372</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno>ArXiv preprint : 1911.03894</idno>
	</analytic>
	<monogr>
		<title level="m">Tasty French Language Model</title>
		<meeting><address><addrLine>CamemBERT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Universal dependency annotation for multilingual parsing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Quirmbach-Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Bedini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Núria</forename><surname>Bertomeu Castelló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungmee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-07">2018. May 7-12, 2018</date>
		</imprint>
	</monogr>
	<note>LREC 2018</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jesus Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gashaw</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liesbeth</forename><surname>Augustinus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbu</forename><surname>Verginica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kepa</forename><surname>Bellato</surname></persName>
		</author>
		<author>
			<persName><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Riyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Biagetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogier</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Blokland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Bobicev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Börstell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriane</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aljoscha</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gülşen</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Cebiroglu Eryigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savas</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabricio</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayeol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çagrı</forename><surname>Collomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Çöltekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marine</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Courtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iakes</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koldo</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Memduh</forename><surname>Gojenola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Gökırmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berta</forename><forename type="middle">Gonzáles</forename><surname>Gómez Guinovart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matias</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Normunds</forename><surname>Grioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Grūzītis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Céline</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Guillot-Barbance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><surname>Hajič Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na-Rae</forename><surname>Hà Mỹ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dag</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbora</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslava</forename><surname>Hladká</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florinel</forename><surname>Hlaváčová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Hociung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><surname>Hohle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Irimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Jelínek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hüner</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Kaşıkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Václava</forename><surname>Kayadelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kotsyba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sookyoung</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Lambertino</surname></persName>
		</author>
		<author>
			<persName><surname>Lando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Septina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Larasati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lavrentiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phương</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lê H Ồng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saran</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Lertpradit</surname></persName>
		</author>
		<author>
			<persName><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Tae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Loginova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Lyashevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aibek</forename><surname>Macketanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Makazhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruli</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mȃrȃnduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><surname>Marheinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martínez</forename><surname>Héctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Mašek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Miekka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlin</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">Moreno</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjartur</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Moskalevskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yugo</forename><surname>Muischnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Murawaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinkey</forename><surname>Müürisep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Nainwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName><surname>Wallin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gunta Nešpore-Bērzkalne, Lương Nguy ễn Thi . , Huy ền Nguy ễn Thi</title>
		<title level="s">Faculty of Mathematics and Physics</title>
		<editor>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jana</forename><surname>Strnadová</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zsolt</forename><surname>Szántó</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuta</forename><surname>Takahashi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Takaaki</forename><surname>Tanaka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Tellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trond</forename><surname>Trosterud</surname></persName>
		</editor>
		<meeting><address><addrLine>Anna Nedoluzhko; Aaron Smith, Isabela Soares-Bastos, Antonio Stella; Anna Trukhina, Reut Tsarfaty, Francis Tyers; Jonathan North Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Gertjan van Noord</publisher>
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
	<note>Seyi Williams, Mats Wirén, Tsegay Woldemariam, Tak-sum Wong, Chunxiao Yan, Marat M. Yavrumyan, Zhuoran Yu, Zdeněk Žabokrtský, Amir Zeldes, Daniel Zeman, Manying Zhang, and Hanzhi Zhu. 2018. Universal dependencies 2.2. LIN-DAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What the [mask]? making sense of language-specific BERT models</title>
		<author>
			<persName><forename type="first">Debora</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Hovy</surname></persName>
		</author>
		<idno>CoRR, abs/2003.02912</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. Challenges in the Management of Large Corpora (CMLC-7) 2019</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-10-25">2014. 2014. October 25-29, 2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation, LREC 2012<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05-23">2012. May 23-25, 2012</date>
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">How multilingual is multilingual bert?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01502</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Annotation référentielle du corpus arboré de Paris 7 en entités nommées (referential named entity annotation of the paris 7 french treebank)</title>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference JEP-TALN-RECITAL 2012</title>
		<meeting>the Joint Conference JEP-TALN-RECITAL 2012<address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<publisher>ATALA/AFCP</publisher>
			<date type="published" when="2012-06-04">2012. June 4-8, 2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
	<note>in french</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">PartTUT: The Turin University Parallel Treebank</title>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-14206-7_3</idno>
	</analytic>
	<monogr>
		<title level="m">Harmonization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project</title>
		<title level="s">Studies in Computational Intelligence</title>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Basili</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rodolfo</forename><surname>Delmonte</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Moschitti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria</forename><surname>Simi</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">589</biblScope>
			<biblScope unit="page" from="51" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Japanese and korean voice search</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="5149" to="5152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Universal morpho-syntactic parsing and the contribution of lexica: Analyzing the onlp lab submission to the conll 2018 shared task</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Seker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers. The Association for Computer Linguistics</publisher>
			<date type="published" when="2016-08-07">2016. August 7-12, 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Udpipe 2.0 prototype at conll 2018 ud shared task</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Evaluating contextualized embeddings on 54 languages in POS tagging, lemmatization and dependency parsing</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<idno>1908.07448</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Korhonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">cloze procedure&quot;: A new tool for measuring readability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journalism Bulletin</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="433" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1452</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09">2017. 2017, 4-9 December 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multilingual is not enough: Bert for finnish</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Ilo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jouni</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juhani</forename><surname>Luotolahti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tapio</forename><surname>Salakoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<idno>ArXiv preprint 1912.07076</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno>ArXiv preprint 1911.00359</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R'emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno>CoRR, abs/1904.09077</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
