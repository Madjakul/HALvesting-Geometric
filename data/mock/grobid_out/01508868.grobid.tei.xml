<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Extraction of TEI Structures in Digitized Lexical Resources using Conditional Random Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Khemakhem</surname></persName>
							<email>mohamed.khemakhem@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria -ALMAnaCH</orgName>
								<address>
									<addrLine>2 Rue Simone IFF</addrLine>
									<postCode>75012</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Centre Marc Bloch</orgName>
								<address>
									<postCode>191 10117</postCode>
									<settlement>Friedrichstrasse, Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University Paris Diderot</orgName>
								<address>
									<addrLine>5 Rue Thomas Mann</addrLine>
									<postCode>75013</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luca</forename><surname>Foppiano</surname></persName>
							<email>luca.foppiano@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria -ALMAnaCH</orgName>
								<address>
									<addrLine>2 Rue Simone IFF</addrLine>
									<postCode>75012</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria -ALMAnaCH</orgName>
								<address>
									<addrLine>2 Rue Simone IFF</addrLine>
									<postCode>75012</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Centre Marc Bloch</orgName>
								<address>
									<postCode>191 10117</postCode>
									<settlement>Friedrichstrasse, Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Berlin-Brandenburgische Akademie der Wissenschaften</orgName>
								<address>
									<addrLine>Jaegerstrasse 22-23 10117</addrLine>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Extraction of TEI Structures in Digitized Lexical Resources using Conditional Random Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6B9F74A69DF967BB68269301EE2D9A16</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic structuring</term>
					<term>digitized dictionaries</term>
					<term>TEI</term>
					<term>machine learning</term>
					<term>CRF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an open source machine learning system for structuring dictionaries in digital format into TEI (Text Encoding Initiative) encoded resources. The approach is based on the extraction of overgeneralised TEI structures in a cascading fashion, by means of CRF (Conditional Random Fields) sequence labelling models. Through the experiments carried out on two different dictionary samples, we aim to highlight the strengths as well as the limitations of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>An important number of digitized lexical resources remain unexploited due to their unstructured content. Manually structuring such resources is a costly task given their multifold complexity. Our goal is to find an approach to automatically structure digitized dictionaries, independently of the language or the lexicographic school or style. In this paper we present a first version of GROBID-Dictionaries<ref type="foot" target="#foot_0">1</ref> , an open source machine learning system for lexical information extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head><p>By observing how the lexical information is organised in different paper dictionaries, it is clear that the majority of these lexical resources share the same visual layout to represent the same categories of text information. That served as our starting point to develop our approach for dismantling the content of digitized dictionaries. We tried to build cascading models for automatically extracting TEI (Text Encoding Initiative) <ref type="bibr">(Budin et al., 2012)</ref> constructs and make sure that the final output is aligned with current efforts to unify the TEI representations of lexical resources. To be easily adaptable to new dictionary samples, we chose machine learning over rule-based techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Cascading extraction models</head><p>We followed a divide-and-conquer strategy to dismantle text constructs in a digitized dictionary, based initially on observations of their layout. Main pages (see Figure1) in almost any dictionary share three blocks: a header (green), a footer (blue) and a body (orange). The body is, in its turn, made of several entries (red). Each lexical entry can be further broken down (see Figure <ref type="figure" target="#fig_0">2</ref>) into: form (green), etymology (blue), sense (red) or/and related entry.</p><p>Layout features become less relevant when the segmentation process reaches a deeper information level and we consequently give them up for the corresponding models. The same logic could be applied further for each extracted block, as long as the finest TEI elements are not yet reached. But in the scope of this paper, we focus just on the first six models, details which are given below.</p><p>Fig. <ref type="figure">1</ref>: First and second segmentation levels of a dictionary page Such a cascading approach ensures a better understanding of the learning process' output and consequently simplifies the feature selection process. Limited exclusive text blocks per level help significantly to diagnose the cause of prediction errors. Moreover, it would be possible to detect and replace early on any irrelevant selected features that can bias a trained model. In such a segmentation, it becomes more straightforward to notice that, for instance, the token position in the page is very relevant to detect headers and footers but has almost no relevance for capturing a sense in a lexical entry, which is very often split over two pages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Towards a more unified TEI modelling</head><p>Our choice for TEI, as the encoding format for the detected structures, is based on its widespread use in lexicographic projects, as well as on some technical factors which will be detailed in the following section. The domination of the lexicographic landscape by TEI is due to the fact that this initiative has provided the lexicographic community with diverse alternatives for encoding different kinds of lexical resources, as well as for modelling the same lexical information. However, the flexibility that this standard ensures has led to an explosion of TEI schemes and, consequently, limited the possibilities for exchange and exploitation.</p><p>Our cascading models are conceived in a way to support the encoding of the detected structures in multiple TEI schemes. But to avoid falling into the diversity trap, we are adopting a format that generalises over existing encoding practices. The final scheme has not yet been finalised, but we are continuously refining our guidelines as we move deeper with our models and apply them to new dictionary samples. We are aiming to ensure a maximal synchronisation with existing research efforts in this direction, by collaborating with COST ENeL and ISO committee TC 37/SC 4.</p><p>Presenting the details of our encoding choices is beyond the scope of this paper, since we are still shaping them, especially for fine grained information. But we aim to highlight about some constraining decisions we made for the upper levels, to give an idea about our modelling direction. A lexical entry, for instance, is always encoded using &lt;entry&gt; exclusively, which means we do not make use of any possible alternatives, such as &lt;super-Entry&gt; and &lt;entryFree&gt;. The semantic loss is not important in this case, since the nature of the entry could be inferred from the elements it contains. As for lexical entries, they can be completely encoded using 5 main elements: &lt;form&gt; for morphological and grammatical information of the whole entry, &lt;etym&gt; for etymological information, &lt;sense&gt; for semantic and syntactic information, &lt;re&gt; for related entries and &lt;dictScrap&gt; for any text that does not belong to the previous elements. Note here that we are trying to use the more generic elements to encode the lexical information in each level, which will be more refined in the following levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GROBID-Dictionaries</head><p>To implement our approach, we took up the available infrastructure from GROBID <ref type="bibr">(Lopez and Romary, 2015)</ref> and we adapted it to the specificity of the use case of digitized dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GROBID</head><p>GROBID (GeneRation Of Bibliographic Data) is a machine learning system for parsing and extracting bibliographic metadata from scholar articles, mainly text documents in PDF format. It relies on CRF <ref type="bibr">(Lavergne et al., 2010)</ref> to perform a multi-level sequence labelling of text blocks in a cascade fashion which are then extracted and encoded in TEI elements. Such an approach has been very accurate for that use case and the system's Java API has been one of the most used by bibliography research platforms and research bodies worldwide.</p><p>We have been struck by the analogy between the structures that can be extracted by GROBID, in the case of full scientific articles, and the actual constructs we wanted to extract from a digitized dictionary. At its first extraction level, GROBID detects the main blocks of a paper such as the header, the body, the references, annexes, etc. These main parts will be further structured at the following level, like the references which will be extracted in separate items and then parsed one by one to detect the titles, the authors and the other publication details. By recalling the segmentation steps presented in the previous section, there is a clear analogy between the case of a reference in a scientific document and a lexical entry in a dictionary. This correspondence is reinforced by the fact that GROBID relies on layout, as well as text features, to perform the supervised classification of the parsed text and generates a TEI compliant encoding where the various segmentation levels are associated with an appropriate XML tessellation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GROBID-Dictionaries</head><p>Due to the above-mentioned similarities, we undertook the adaptation of GROBID for the case of digitized dictionaries in order to build a system, which uses the core utilities of GROBID and applies them for lexical information processing. In building GROBID-Dictionaries, we faced several challenges, the three major ones being detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">TEI cascade modelling</head><p>After having fully encoded a lexical entry, the task became more specific and more challenging when it comes to defining the TEI structures to be extracted by each model. It is a question of finding the appropriate mapping between the TEI elements and the labels to be set for the models that share the task of structuring the text in cascade. In addition, the process is at the same time constrained by the need to avoid having structures from different hierarchy levels being extracted at once. In fact, the CRF models, as they could be used from GROBID core, do not allow the labelling of nested text sequences. We clarify this technical point by explaining how the sequence labelling process works in the case of segmenting a lexical entry.</p><p>The following matrix represents the set of feature vectors corresponding to the lexical entry condenser, which will be labelled by a first version of the "Lexical Entry" model. The latter has the task of detecting the 5 main blocks in a lexical entry, if they exist. For Each vertical column is a specific feature for all the tokens of the lexical entry and each horizontal line corresponds to all the features of each token. For this model, a set of features is going to be assigned to each token based on criteria we chose in the feature selection process. In the second phase, comes the role of the trained model to give a prediction of a suitable label for each token, based on all its feature values. A structure corresponds then to the sequence of tokens having the same label, where the I-Label marks the beginning of a new sequence. Following this technique, it is obviously not possible in this model to structure the example "le froid condense la vapeur d'eau"(see Figures <ref type="figure" target="#fig_0">2</ref> and<ref type="figure" target="#fig_1">3</ref>) in the sense, since just one label is allowed per token. Therefore, the segmentation of the examples should be delegated to another model that follows the current one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Sample annotation</head><p>This is the phase where the previous rules will be applied on different instances, to annotate data for training the models. An adjustment of the directives is necessary to make the models more general, as soon as new instances appear to show the modelling limits of our current guidelines. To illustrate such a case, we could take the example of the previously defined "Lexical Entry" model and apply it to the lexical entry aid.</p><p>The TEI encoding for this entry with the "Lexical Entry" model is the following (see Figure <ref type="figure">5</ref>): We could notice that the model presented in Figure <ref type="figure" target="#fig_1">3</ref> is no longer valid to perform the segmentation of senses aggregated by part of speech (POS), with respect to avoiding nested constructs. This issue could be fixed by having a first model that does not find the boundaries of the senses of a part of speech in this level. The labelling and extraction of the TEI structures should be performed further for the other blocks, by following the same approach. For the case of the aid entry, a dedicated model should be used to segment the &lt;form&gt; block by extracting the morphological and grammatical information and decide about of the parent of the latter. In the current case, the &lt;gramGrp&gt; will be the direct child node of the entry, since it carries information Fig. <ref type="figure">7</ref>: Structured output of the "Sense" model about the sense of the entry given a POS, and not about the lemma. The &lt;gramGrp&gt; block will, in its turn, have another specific model to structure its content. Figure <ref type="figure" target="#fig_4">8</ref> shows the final output generated by our cascading model tree. Annotation guidelines seem to be mandatory here to guide the process since an annotator, especially with a linguistic or lexicographic background, could be easily biased by the TEI practices and tags which are used differently in our cascading approach but will converge in the final output. We noticed this issue after having lexicographers annotate a few samples and we therefore, defined a first version of the guidelines<ref type="foot" target="#foot_1">2</ref> , which we are actively maintaining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Feature selection</head><p>In this phase, the cumulated data will be used for generating features that will be used by the models to discriminate between their labels. For the first model, we kept the line based features used in GROBID's first model<ref type="foot" target="#foot_2">3</ref> . Our choice was based simply on the assumption of the general nature of such features. Moreover, the experiments on several samples showed a high and fast performance.</p><p>As explained in our approach, we tried to rely on a restricted list of features for the rest of the models, where we drop the ones that are most likely to produce bias. We chose to use features on the token level to structure the lexical information. For the first version of our system, we are experimenting the use of one list with 16 features<ref type="foot" target="#foot_3">4</ref> : 8 based on the text and the rest carrying the layout aspects of each token, such as the change of font or line breaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Models</head><p>The resulting models and their corresponding labels are the following:</p><p>• Dictionary Segmentation: This is the first model and has as its goal the segmentation of each dictionary page into three main blocks, where each block corresponds to a TEI label: &lt;fw type="header"&gt; for information in the header, &lt;ab type="page"&gt; for all the text in the body of a page and &lt;fw type="footer"&gt; for footer information.</p><p>For the sake of simplicity, for training the models (see Section 4.3) we use: &lt;head-note&gt; to refer to &lt;fw type="header"&gt;, &lt;body&gt; referring to &lt;ab type="page"&gt; and &lt;footer&gt; to refer to &lt;fw type="footer"&gt;. But we respect the original labels for the final TEI output.</p><p>• Dictionary Body Segmentation: The second model gets the page body, recognized by the first model, and processes it to recognize the boundaries of each lexical entry by labelling each sequence with &lt;entry&gt; label.</p><p>• Lexical Entry: The third model parses each lexical entry, recognized by the second model, to segment it into four main blocks: &lt;form&gt; for morphological and grammatical information, &lt;etym&gt; for etymology, &lt;sense&gt; for all sense information, &lt;re&gt; for related entries.</p><p>• Form: This model analyses the &lt;form&gt; block, generated by the Lexical Entry model, and segments its contained information . We have for the moment three labels for this model: &lt;orth&gt; for the lemma, &lt;pron&gt; for pronounciation and &lt;gramGrp&gt; for grammatical information, such as part of speech, gender, number,.etc.</p><p>• Sense: The Sense model has two goals. First, to extract the grammatical information &lt;gramGrp&gt;, that could exist. Second, to segment the first level senses, by structuring them in &lt;sense&gt; sequences. For each model, we reserved two extra labels: &lt;pc&gt; for punctuation such as separators between text information or any markup text. A second label, &lt;dictScrap&gt;, is used to contain any information that couldn't be classified in one of the main labels of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Lexical Samples</head><p>We carried out our experiments by applying our models to several dictionaries and given the inconstancy that some presented, mainly due to digitization issues, we selected two resources that represent several differences on many levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Digital dictionary</head><p>"Easier English Basic Dictionary" (EEBD, 2009) is a monolingual dictionary for English which contains over 5,000 entries, published in 2009. For our experiments, we used the 370 pages containing the body of the dictionary. The version which we used, is a digitally born one. In other words, no OCR processing has been performed to generate the resource in its electronic format. As Figure <ref type="figure" target="#fig_6">10</ref> illustrates, the dictionary has a very modern and basic layout and its markup system is spread over the entries to mark the transition of the lexical information presented. We chose this digital sample to be our baseline, since it contains very clean text and clear lexical information modelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Digitized dictionary</head><p>To take the experiments to the next level, we chose a dictionary that has been OCRized and that encloses totally different lexical information. The dictionary was published in 1964 but later digitized. The version we have is of relatively good quality but still presents some anomalies, where some text blocks are unextractable from the PDF.</p><p>The Fang-French &amp; French-Fang dictionary (Galley, 1964) is a bilingual dictionary having over 500 pages of lexical entries split into two parts. As Figure <ref type="figure">11</ref> shows, the markup system is totally different from the EEBD, where field transition is mostly marked with a change of font rather than with specific markers. For our experiments, we worked on the first part, Fang-French Dictionary (FFD), containing over 390 pages</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>For the sake of conciseness, in this paper we present an evaluation of just 4 selected models out of 6 implemented, for each dictionary. We used the benchmark module provided by GROBID to measure the precision, recall and F1 scores.</p><p>In the following tables, token level gathers the measures for each different token, field level is for each continuous sequence of the same label (so a field, a sequence of several tokens which all belongs to the same labelled chunk, e.g. a lexical entry). For both dictionaries, we annotated 7 pages, which we split into 4 for training and 3 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Dictionary Body Segmentation</head><p>For EEBD, we annotated 5 pages, which we split into 50 lexical entries for training and 27 for evaluation. For FFD, we annotated 7 pages with 91 lexical entries for training and 45 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Lexical Entry</head><p>For EEBD, we annotated 8 pages, which we split into 76 entries for training and 24 for evaluation. For FFD, we annotated 3 pages, which we split into 47 for training and 24 for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Sense</head><p>For EEBD, we annotated 6 pages, which we split into 15 sense blocks for training and 15 for evaluation.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>The evaluation on both dictionaries shows a high performance by the first and second models to detect, respectively, the body part of a page and the boundaries of lexical entries. The header and punctuation predictions for the first two models are however low for the digitized sample. This could be explained by the quality of the text which sometimes led to the generation of feature values that bias the learning.</p><p>For the "Lexical Entry" model, the performance of the system remains high for the extraction of grammatical and morphological information on the English dictionary but with low precision on the Fang-French sample. The detection of related entries, which are contained only in the English dictionary, shows the limitation of our model to extract these constructs with the actual setup. We hypothesis that it is related, firstly, to a lack of annotated data and, secondly, to a lack of discriminative features. Nonetheless, the model performs relatively well for sense block detection on the English dictionary and slightly worse on the bilingual dictionary. The detection of the punctuation, representing the transition between the main fields of the model, is also limited in this model.</p><p>The results of the final model reflect the reliability of our features to structure the sense information, when it has to focus on the boundaries of senses. But for the case of the senses aggregated by POS, more discriminative features should be added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Works</head><p>This work takes place within the context of studies on lexicography and digital humanities fields, targeting the exploitation of digitized dictionaries. Most previous research <ref type="bibr">(Khemakhem et al., 2009;</ref><ref type="bibr">Fayed et al., 2014;</ref><ref type="bibr">Mykowiecka et al., 2012)</ref> remained limited to the costly manual elaboration of lexical patterns, based on observing the organisation of the lexical information in a specific sample.</p><p>There have been, however, strong pointers to the usefulness of machine learning techniques, CRF in particular, to address the issue of decoding the complexity of lexical resources. Crist presented experiments for processing and automatically tagging linear text of two bilingual dictionaries, using CRF models. The goal has been purely experimental, proving the appropriateness of CRF for tagging tokens in digitized dictionaries. His exhaustive study also stressed the other processing issues, which are very important to the effectiveness and the evaluation of any parsing technique. Another recent study <ref type="bibr">(Bago and Ljubešić, 2015)</ref>, has addressed the issue of using CRF models to perform automatic language and structure annotation in a multilingual dictionary. The technique again has a very high accuracy in much less time than would be required for manual annotation.</p><p>Both of the mentioned machine learning approaches apply one CRF model to label the all the tokens of a dictionary. In such a bottom-up technique, the learner is overwhelmed by the number of labels to choose from at once, which increases the number of prediction errors. A huge amount of training data is also required per model to cover middle and high complexity dictionaries.</p><p>The novelty in our approach is that we reduce the scope of each bottom-up model by splitting the task over different models that process the lexical information in a top down fashion. Moreover, our system does not stop at the level of tagging the tokens, but enables the construction of blocks of lexical information in a format that facilitates the processing as well as the exchange of the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>GROBID-Dictionaries in its first version has shown the promise of CRF cascading models to structure digitally born and digitized dictionaries, independently of the language and lexicographic style. Our experiments had the goal of, firstly, verifying our assumptions and, secondly, highlighting the strengths and the limitations of the implemented models.</p><p>It is obvious that more focus should be given to the feature selection process, in order to reinforce the prediction of the models for certain labels and fields. Feature tuning should also be applied on larger annotated data with more varied instances. Therefore, we are planning to build a smart annotation tool with strong guidelines, to simplify the annotation process.</p><p>Our open source system could be used, after more tuning, to radically speed up the structuring of many digitized dictionaries in a unified scheme or to measure the structurability of OCRized lexical resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Example of the segmentation performed by the Lexical Entry model</figDesc><graphic coords="4,154.37,74.25,283.47,88.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Sequence labelling using a first version of the "Lexical Entry" segmentation model</figDesc><graphic coords="6,83.50,74.26,425.20,294.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Lexical entry having more than one POS</figDesc><graphic coords="7,83.50,263.28,425.20,101.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Structured output of the "Lexical Entry" model's adjusted version</figDesc><graphic coords="7,83.50,495.69,425.20,103.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Final output of all the models</figDesc><graphic coords="8,83.50,359.51,425.21,219.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Selected models</figDesc><graphic coords="10,154.37,242.77,283.46,333.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Two pages from EEBD side by side</figDesc><graphic coords="12,83.50,245.84,425.20,326.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,154.37,194.84,283.48,387.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,83.50,74.25,425.20,316.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="15,154.37,295.68,283.46,193.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of "Dictionary Segmentation" model on EEBD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of the "Dictionary Segmentation" model on FFD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of the "Dictionary Body Segmentation" model on EEBD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of the "Dictionary Body Segmentation" model on FFD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Evaluation of the "Lexical Entry" model on EEBD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Evaluation of the "Lexical Entry" model on FFD</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Evaluation of the "Sense" model on EEBD For FFD, we annotated 4 pages, which we split into 71 sense blocks for training and 19 for evaluation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Evaluation of the "Sense" model on FFD</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/MedKhem/grobid-dictionaries</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/MedKhem/grobid-dictionaries/wiki/How-to-Annotate%3F</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/kermitt2/grobid/blob/master/grobid-core/src/main/java/org/grobid/ core/features/FeaturesVectorSegmentation.java</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/MedKhem/grobid-dictionaries/blob/master/src/main/java/org/grobid/ core/features/FeatureVectorLexicalEntry.java</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work was supported by PARTHENOS. We would like to thank Patrice Lopez, the main designer of GROBID, for his continuous support and valuable advice. <ref type="bibr">Bago, P. and Ljubešić, N. (2015)</ref>. Using machine learning for language and structure annotation in an 18th century dictionary. In Electronic lexicography in the 21st century: linking lexical data in the digital age. Budin, G., Majewski, S., and Mörth, K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">References</head></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
