<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Usability for Automatically Structuring Digitised Dictionaries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Khemakhem</surname></persName>
							<email>mohamed.khemakhem@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Centre Marc Bloch</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris Diderot ¶ École Pratique des Hautes Études</orgName>
								<address>
									<settlement>Paris ‡ Berlin-Brandenburgische Akademie der Wissenschaften, Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Axel</forename><surname>Herold</surname></persName>
							<email>axel.herold@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Centre Marc Bloch</orgName>
								<address>
									<settlement>Berlin</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Usability for Automatically Structuring Digitised Dictionaries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">405F135C212B53D64E3326C4FA05485D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>electronic lexicography</term>
					<term>usability</term>
					<term>digitised dictionaries</term>
					<term>TEI</term>
					<term>Docker</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The last decade has seen a sharp rise in the number of NLP tools that have been made available to the community. The usability of several e-lexicography tools represents a serious obstacle for researchers with little or no background in computer science. In this paper we present our efforts to overcome this issue in the case of a machine learning system for the automatic segmentation and semantic annotation of digitised dictionaries. Our approach is based on limiting the burdens of managing the tool's setup in different execution environments and reducing the complexity of the training process. We illustrate the possibility to reach this goal by adapting existing functionalities and using out-of-the box software deployment tools. We also report on the community's feedback after exposing the new setup to real users from different professional backgrounds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Web applications have been the main deployment solution for many NLP tool designers to shortcut the need to deal with installation and configuration issues that many desktop applications continue to represent for end users. A web architecture does not rely on the user being familiar with local software tools such as command line shells or software development environments that allow expert and more personalised use of some advanced libraries. A strong current development is the integration of sets of tools into unified web-based working environments for general Humanities research such as the European CLARIN<ref type="foot" target="#foot_0">1</ref> and DARIAH<ref type="foot" target="#foot_1">2</ref> initiatives. In the more specialised field of lexicography, tools such as the Lexonomy<ref type="foot" target="#foot_2">3</ref> dictionary writing system <ref type="bibr" target="#b3">(Měchura, 2017)</ref> represent a typical class of web-based applications. While much of this high level way of accessing NLP tools also accounts for desktop applications, locally installed tools and possibly other software they rely on still have to be updated regularly. Different tools may even form a complex "eco-system" with subtle dependencies between individual modules. The main concern for users with regard to webbased tools is the security and possibly the confidentiality of their data. Therefore desktop applications still exist after the general movement towards web-based solutions. GROBID-Dictionaries<ref type="foot" target="#foot_3">4</ref> is a machine learning system which has been developed to serve as a web application for structuring digitised dictionaries <ref type="bibr" target="#b1">(Khemakhem et al., 2017)</ref>. It also exhibits the desktop functionality required for the preprocessing of data during the training process. Although it has a decent documentation, the process of setting up the desktop version of the tool remains very challenging for users with limited programming knowledge. Annotating the preprocessed XML data also represented a serious challenge in earlier versions of the tool because initially it did not provide mechanisms for sanity checks or for visualising annotations for humans.</p><p>In this paper we focus on the desktop functionality built into GROBID-Dictionaries. We present new features which have been implemented to enhance the usability of the tool. In Section 2. we provide an overview of the architecture and setup of the system. We detail the different stages of the training process in Section 3. We then address the technical challenges related to the installation of the system as well as the annotation process and present our solution to overcome them in Section 4. In Section 5. we report on first experiences with the new setup and features based on feedback collected from users who were previously not familiar with GROBID-Dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">GROBID-Dictionaries</head><p>The work carried out by <ref type="bibr" target="#b1">Khemakhem et al. (2017)</ref> resulted in a successful adaptation and extension of GROBID -an existing machine learning platform <ref type="bibr" target="#b2">(Lopez and Romary, 2015)</ref> -to be used for the automatic identification of lexical information in digitised lexical resources. The resulting system is called GROBID-Dictionaries to reflect the dependency with the parent project. GROBID-Dictionaries has been tested using several lexical resources with promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Architecture</head><p>The system's architecture is cascaded. Textual and typographical information are processed by means of multi-level classifications performed by machine learning models. Figure <ref type="figure" target="#fig_0">1</ref> sums up the architecture described in <ref type="bibr" target="#b1">Khemakhem et al. (2017)</ref>. Each blue object represents a conditional random field (CRF) model. These models are used to classify For the sake of simplicity, Figure <ref type="figure" target="#fig_0">1</ref> does not include all possible tags for the Form and Grammatical Group models. A complete description of all possible TEI structures resulting from these two models can be found in the TEI P5 dictionary chapter 56 in <ref type="bibr" target="#b1">Budin et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Configuration</head><p>GROBID-Dictionaries depends on core utilities and libraries provided by GROBID 7 . The installation of the system must be preceded by the installation and setup of the parent project. Therefore GROBID-Dictionaries needs to be cloned as an extension module within GROBID's project structure and must be built after its parent project. Due to differences in technical preferences of the project leaders, two different automation build technologies need to be used for building each project: Gradle 8 for GROBID and Maven<ref type="foot" target="#foot_4">9</ref> for GROBID-Dictionaries. Successful builds of the system are packaged as Java libraries in two formats:</p><p>• a JAR (Java ARchive): this file is required for all processing stages which precede the training of each model, and same when run on other operating systems. However, there is no explicit guarantee for such uniform behaviour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MATTER Annotation Workflow</head><p>The annotation workflow in GROBID-Dictionaries follows the MATTER methodology (Model-Annotate-Train-Test-Evaluate-Revise, see Figure <ref type="figure" target="#fig_1">2</ref>) introduced by <ref type="bibr" target="#b4">Pustejovsky and Stubbs (2012)</ref>. Projected onto GROBID-Dictionaries and the processing of lexical resources, the individual steps are as follows:</p><p>Model: define a CRF model for predicting different text structures at one stage and determine the corresponding feature set. This phase requires the involvement of a programmer to create the defined models and integrate them into the cascading architecture.</p><p>Annotate: assign a TEI tag to each text block representing a lexical entity defined within a model's scope. This task must be performed on an XML representation of the data and must be strictly synchronised with the corresponding feature set file. The annotation guidelines 10 need to be respected.</p><p>Train: use each annotated batch of data to train a corresponding model. The cascading architecture of the models should be respected here.</p><p>Test: this step gives just a rough idea about how the trained model behaves on unseen data. There are many ways to accomplish this goal. The easiest one is to run the corresponding web service from the web application on a held-out sample.</p><p>Evaluate: a precise evaluation with different measures is enabled at the end of the training process as long as annotated data are provided under the dedicated location in the dataset.</p><p>Revise: the last stage is about reviewing the modelling and annotation steps that have been described in the guidelines. Four possible measures are the outcome of this step:</p><p>• annotate more data when an improvement in the results was achieved,</p><p>• refine the annotation guidelines for new variations noticed in the last training batch</p><p>• proof-read the performed annotations when minor anomalies are noticed</p><p>• think about redefining the modelling when the results represent unexplainable anomalies. This could be translated either into a simple feature engineering process or into a change of the logic behind and the scope of the models or their architecture.</p><p>10 https://github.com/MedKhem/ grobid-dictionaries/wiki/How-to-Annotate%3F </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Enhanced Usability</head><p>Section 2. presented a detailed picture of the technical setup required to install and execute the different parts of the system. Thus it is clear that a certain expertise and understanding of the system architecture is mandatory to successfully install the tool. Section 3. highlighted the challenges of the iterative training cycle which involves costly manual work in terms of carrying out data annotation. Such requirements impose a twofold obstacle: on the one hand, the tool's target community mostly consists of users, such as lexicographers or linguists, who have limited programming skills. If these users are not able to get technical support, the tool will not be usable for a large proportion of its target community. In the other hand, the GROBID-Dictionaries project aims to constantly improve its architecture and to provide more fine-grained lexical information. In the long term, the goal of the project is to provide generic machine learning models which will be able to exploit different types of digitised dictionaries. Collecting and working with different types of lexical data (or at least samples thereof) drawn from a preferably diverse user community is a crucial step in the further development of GROBID-Dictionaries. The usability of the tool is a vital aspect as this enables a broad user community to productively make use of GROBID-Dictionaries. Therefore, issues of usability are of similar importance to the tool's earlier defined purpose and the research challenges it encounters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Unified Execution Environment</head><p>As a first measure, we have investigated different ways for streamline the setup process and to guarantee a unique behaviour of the system across different execution environments.</p><p>One possible solution would have been to use a system image runnable on a virtual machine. Such an image should have a Linux based operating system, a Java development kit (JDK) and the different automated build systems installed. GROBID and GROBID-Dictionaries should also already be cloned and built correctly. This type of solution suffers from two main issues. Firstly, the size of the image would be huge as it would include several unnecessary tools and system files that are still part of the operating system. Secondly, the static nature of such an image would make it complicated to update after a new version of GROBID-Dictionaries is released. Updates to GROBID-Dictionaries are published frequently since the tool is under continuous development. However, a system image containing the above mentioned components can be built in a more efficient way using a different technique. Docker<ref type="foot" target="#foot_5">11</ref> is a state of the art software technology which is also based on the virtualisation of the execution environment. In contrast to the static image approach sketched out initially, Docker allows for the flexible composition of an image. An image is shaped by instructions written in a Docker file<ref type="foot" target="#foot_6">12</ref> . These instructions ensure that only the required components are included in the image. Moreover, several alternatives are available to efficiently update a build within an image starting from pushing a newly created image to the online Docker Hub repository<ref type="foot" target="#foot_7">13</ref> , to linking the corresponding GitHub and Docker Hub repositories coupled with activating the automatic build to synchronise the image after each update of the code. It is also possible to synchronise files on the host machine with a running image in the Docker container. This feature allows the tool hosted inside a Docker container to directly interact with files stored on the host machine. We took advantage of this alternative to make the dataset directory shared between the two environments. With this mechanism, the user can exploit the full functionality of the tool living in the Docker image to train the machine learning models on the data residing locally on his machine.</p><p>In addition, thanks to the self-contained nature of the tool's web application coupled with its fluid setup and manipulation through the Docker image, using the GROBID-Dictionaries image enables both of the desktop and web based functionality to be run on the user's local machine. Such a feature represents an asset for researchers who are concerned about the security of their data and experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Lightening MATTER Process</head><p>The second major category of improvements specifically targets the annotation workflow. Annotating data for the training process involves challenging manual work and requires precautionary measures to ensure data integrity and validity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Creating Training Data</head><p>To train a model in GROBID-Dictionaries based on a PDF file containing the raw text and the typographical features of a lexical resource, two additional files are necessary: a TEI document containing the corresponding reference encoding and a feature file describing textual and typographical information of each printed line or token.</p><p>To generate the training files, embedded functionalities of the tool should be used following one of the two following options:</p><p>• pre-annotated training data: this used to be the default mode for automatically creating training data, inherited directly from GROBID's core functionality. This mode is useful when a model was trained on a substantial amount of data. The task of the annotator is then to correct the automatically placed TEI tags by moving, adding or removing them.</p><p>• raw training data: this constitutes new functionality we have implemented to shortcut the checkout and cleaning of the tags automatically generated by using the default mode. The idea is simply to create training data without pre-annotations. Despite being obvious, starting to annotate a document from scratch was not possible before integrating this new feature. Such a mode breaks with the old practice of correcting the predictions made by a model trained on different samples, to make it possible to start annotating totally fresh data.</p><p>Besides giving more choices to the annotator, such a mode saves time and efforts especially if an old model was trained with multiple TEI elements.</p><p>A legitimate question remains as yet unanswered: how can a user generate training data based on a selection of specific pages from possibly hundreds of pages a dictionary may comprise? After annotating different lexical samples in PDF format, we could qualify splitting an existing document into separate pages, or sequences of pages, as a very critical step. With some supposedly dedicated PDF manipulation tools producing damaged pages, we found only one tool reliably useful for the purpose of separating PDF pages<ref type="foot" target="#foot_8">14</ref> which seems to produce a quality split as good as the original document. Using workaround solutions for this purpose, such as the print-to-file functionality in web browsers, is also not recommended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Training Data Annotation</head><p>As previously stated, GROBID-Dictionaries generates a preprocessed XML representation from PDF files containing the raw text of a lexical resource. To create training data for the tool, the user is then required to introduce semantic mark-up for the different models. Typically, an XML aware editor should be used to perform this task. Some advanced editors such as oXygen<ref type="foot" target="#foot_9">15</ref> allow for the visual annotating of XML files (see Figure <ref type="figure" target="#fig_3">4</ref> for an example).</p><p>We aimed take advantage of the visual feature to avoid performing inline annotation directly on the text of the XML elements. This is catered for by a new feature in GROBID-Dictionaries that for each model now provides both a schema description (in Relax NG) <ref type="foot" target="#foot_10">16</ref> and a presentational stylesheet (in CSS). The schema description enables the editing software to check or even enforce schema compliance of the training data. The stylesheet can be exploited by the editing software to allow users to mark up the training data semantically by highlighting portions of the text and then enclosing the highlighted portion with a suitable XML tag.</p><p>The colours attributed to each element can be customised by a simple modification in the stylesheet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">User Experience</head><p>We had the opportunity to expose the system with its new setup and features to a mixed group of users in the course of a winter school on lexicography that was held at the Berlin-Brandenburg Academy of Sciences and Humanities at the end of 2017 17 . During this event we collected information about the usability of the tool. Additionally, we asked participants to respond to a questionnaire after the winter school to gain further insight into their experience of working with the tool. Given the relatively small number of participants, the responses to the questionaire do not allow for a rigid quantitative evaluation. Nevertheless, based on the responses and our own experiences during the tutorial we are able to present a qualitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setup</head><p>A group of nine users participated in the experiment which was carried out during three hands-on sessions of four hours each. The users were free to join one or more sessions of the tutorial. The goal of the tutorial was to familiarise the participants with the MATTER workflow as implemented in GROBID-Dictionaries, while excluding the first modelling step which requires programming skills. Note that none of the participants was familiar with the tool prior to the tutorial.</p><p>After a short introduction to the architecture of the system, the users were guided through the process of installing and running the docker image 18 . Once the docker image was running, the participants were then able to reproduce the results reported in <ref type="bibr" target="#b1">Khemakhem et al. (2017)</ref> which are based on a modern English monolingual dictionary. As the next step, several users used the possibility to experiment with their own lexical samples by repeating the workflow they had learnt and crafting new models for their individual datasets. Two of the participants succeeded in training and using all of the implemented models for their own datasets, thus adapting all of the functionality currently implemented in GROBID-Dictionaries. 19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Gathered Insights</head><p>We asked the participants of our tutorial to respond to a questionnaire after the winter school. The questionnaire was created as a Google Form 20 . The results of the inquiry can be summed up by the following points:</p><p>Tool setup / user profile The first three questions focus on establishing the professional background of the participants. linguists, computational linguists, a computer scientist, a web developer and a philologist. Participants were free to name more than one field of expertise. Of the nine respondents, seven reported previous knowledge of machine learning techniques but only four of them had actually worked with machine learning tools before.</p><p>When asked whether they encountered any problems with actually running the tool from the docker image, the majority of the participants (seven) responded that this was not the case. The setup failed once on a Windows based computer with insufficiently sized memory that was running an advanced version of the operating system. Consequently there was not enough memory left to run the Docker software which requires more than the 1 GB of free memory. The participant could still continue the tutorial by sharing a machine with her colleague. Without taking into account the answer of another respondent who involuntarily reported encountering an installation issue, almost 90% of the users were able to launch the tool without any problem.</p><p>Sample data / Initial training The lexical resources brought to the tutorial were considerably varied. They included different types of dictionaries (some digitised, some born digital with no explicit semantic markup) such as general monolingual, bilingual and etymological dictionaries as well as a dictionary from a language documentation field project (see Table <ref type="table" target="#tab_2">1</ref>).</p><p>We asked the participants whether they successfully trained at least the first two models and thus were able to perform the general dictionary segmentation (page segmentation) and the dictionary segmentation (entry recognition). Despite the variety of their datasets, 100% of the answers were positive. This supports the assumption of the implemented cascading approach to be sample independent. Note that two participants worked on the same resource and another two used the resource that we provided.</p><p>Creating training data Two questions focus on the usability of the graphical annotation of the training data using oXygen's author mode. None of the participants found graphically marking the training data a hard task and six described it as a straightforward process. Compared to creating the training data by manipulating the XML structure directly with a text editor, most of the participants (seven) confirmed that the graphical approach was easier.</p><p>Training workflow Although just two participants could finish the training for all models of the tool, all those who were not able to train the remaining models during the tutorial expect to be able to complete the training on their own. Moreover, all the participants reported being confident that they were able to re-apply what they had learnt on other lexical resources. It's important though to clarify why some users could not successfully train all of the models until the end of the tutorial. This was mainly due to the fact that the participants were free to attend only parts of the tutorial sessions and due to the considerably long time spent downloading the huge Docker image with the available internet connection.</p><p>Future use of the tool Based on the apparently successful mastering of the training workflow, all but one participant were willing to continue using GROBID-Dictionaries after the tutorial. It is worth noting that the participant who does not intend to continue using GROBID-Dictionaries is working with non-lexical data and still plans to adapt the parent project GROBID to his type of data.</p><p>Having motivated inter-disciplinary experts participating in the tutorial as well as testing the tool on new lexical samples provided us with the opportunity to spot some issues and several possible improvements. We were able fix some of the minor triggered implementation issues in the course of the tutorial. Other issues have been filed as new tickets on GitHub, e. g. issues concerning the treatment of lexical entries that stretch over more than two pages in print. Some technical issues related to the GROBID core still need to be resolved such as support for some classes of special characters which are wrongly encoded in the preprocessing of the raw input text. The annotation guidelines should also be further refined to provide clearer definitions of constructs to be annotated, such as related entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Whereas <ref type="bibr" target="#b1">Khemakhem et al. (2017)</ref> presented the basis of the approach to implement GROBID-Dictionaries and initial experimental results, this paper provides a more in-depth description of the machine learning system, with the focus on its architecture, technical setup and the training workflow.</p><p>Enhancing the usability of the tool has been addressed as a fundamental feature given the fact that the tool is in its early development stage and the involvement of end users is a key factor in the evolution of the tool. Therefore several measures have been implemented to guarantee a straightforward installation and user-friendly annotation process.</p><p>The exposure of the tool to real users has confirmed many of our choices to alleviate the challenges of a complex ML workflow. This experiment also provided us with the possibility to promote the tool as well as to collect in-depth feedback, which will help us to efficiently set our priorities.</p><p>The recent version and setup of the tool, presented in this paper, does not only enhance its usability but also supports the reproducibility of findings resulting from its use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: General architecture of GROBID-Dictionaries</figDesc><graphic coords="3,52.16,69.16,234.84,118.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Implemented MATTER Workflow</figDesc><graphic coords="4,52.16,69.17,234.85,170.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A GROBID-Dictionaries image in a Docker container To run a Docker image of GROBID-Dictionaries (see Figure3), a user needs to install the version of the Docker software corresponding to the user's operating system and pull the latest image of the tool from Docker Hub. The pulled image (orange box) will not be run directly on top of the operating system of the host machine but rather inside a Docker controlled container (yellow box). Thus testing the tool on Docker is enough to guarantee a unified behaviour, regardless of the particular system configuration of a user's computer environment. It is also possible to synchronise files on the host machine with a running image in the Docker container. This feature allows the tool hosted inside a Docker container to directly interact with files stored on the host machine. We took advantage of this alternative to make the dataset directory shared between the two environments. With this mechanism, the user can exploit the full functionality of the tool living</figDesc><graphic coords="4,304.87,309.31,234.85,161.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training data annotation in oXygen author mode for the first model: page headers vs. page body</figDesc><graphic coords="5,304.87,386.80,234.85,171.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The tutorial group consisted of lexicographers, detailed description of the conditions of the experiment can be found in a blogpost at https://digilex. hypotheses.org/250 as shared by of one of the participants.</figDesc><table><row><cell cols="4">17 https://lexmc.sciencesconf.org/</cell></row><row><cell>18 see</cell><cell>instructions</cell><cell>at</cell><cell>https://github.com/</cell></row><row><cell cols="4">MedKhem/grobid-dictionaries/wiki/Docker_</cell></row><row><cell cols="2">Instructions</cell><cell></cell><cell></cell></row><row><cell>19 A more</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>20 </p>https://goo.gl/Zt2gDy</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dictionaries experimented with during the tutorial.</figDesc><table><row><cell>Type</cell><cell cols="2">Language(s)</cell><cell>Size</cell></row><row><cell cols="3">general, bilingual Greek, English</cell><cell>≈ 17 000 entries</cell></row><row><cell>general, monolin-</cell><cell>Basque</cell><cell></cell><cell>≈ 16 000 pages</cell></row><row><cell>gual</cell><cell></cell><cell></cell></row><row><cell>etymological,</cell><cell cols="2">Hittite (a lan-</cell><cell>≈ 470 pages</cell></row><row><cell>bilingual</cell><cell cols="2">guages of the</cell></row><row><cell></cell><cell>ancient</cell><cell>Near</cell></row><row><cell></cell><cell cols="2">East), English</cell></row><row><cell>lang. documenta-</cell><cell cols="2">French, Yemba</cell><cell>≈ 2 1000 entries</cell></row><row><cell>tion</cell><cell>(an</cell><cell>African</cell></row><row><cell></cell><cell cols="2">language family)</cell></row><row><cell>lang. documenta-</cell><cell cols="2">German (Bavarian</cell><cell>≈ 75 000 entries</cell></row><row><cell>tion</cell><cell cols="2">dialects in Aus-</cell></row><row><cell></cell><cell>tria)</cell><cell></cell></row><row><cell>general, monolin-</cell><cell>English</cell><cell></cell><cell>≈ 370 pages</cell></row><row><cell>gual</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.clarin.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.dariah.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.lexonomy.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/MedKhem/ grobid-dictionaries</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>https://maven.apache.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5"><p>https://www.docker.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>https://github.com/MedKhem/ grobid-dictionaries/blob/master/Dockerfile</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>https://hub.docker.com/r/medkhem/ grobid-dictionaries/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8"><p>http://community.coherentpdf.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_9"><p>https://www.oxygenxml.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_10"><p>http://www.relaxng.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgements</head><p>This work has been supported by the "Pooling Activities, Resources and Tools for Heritage E-research Networking, Optimization and Synergies" (PARTHENOS) project. We would like to thank Karim Ben Ammar B.Sc. for his valuable technical advice regarding the efficient use of the Docker technology.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic Extraction of TEI Structures in Digitized Lexical Resources using Conditional Random Fields</title>
		<author>
			<persName><forename type="first">G</forename><surname>Budin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mörth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Foppiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Text Encoding Initiative</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012">2012. 2017. 2017. September</date>
			<pubPlace>Leiden, Netherlands</pubPlace>
		</imprint>
	</monogr>
	<note>Creating lexical resources in tei p5. a schema for multi-purpose digital dictionaries</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Grobid -information extraction from scientific publications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ERCIM News</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Introducing Lexonomy: an opensource dictionary writing and publishing system</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Měchura</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<pubPlace>Leiden</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Natural Language Annotation for Machine Learning: A guide to corpusbuilding for applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stubbs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>O&apos;Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
