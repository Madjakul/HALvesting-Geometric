<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pedro</forename><forename type="middle">Javier</forename><surname>Ortiz Suárez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
							<email>benoit.sagot@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1897CD77754F481322B5124FDADB1D6C</idno>
					<idno type="DOI">10.18653/v1/2020.acl-</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCARbased and Wikipedia-based ELMo embeddings for these languages on the part-ofspeech tagging and parsing tasks. We show that, despite the noise in the Common-Crawlbased OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the crosslingual benefit of multilingual embedding architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the key elements that has pushed the state of the art considerably in neural NLP in recent years has been the introduction and spread of transfer learning methods to the field. These methods can normally be classified in two categories according to how they are used:</p><p>• Feature-based methods, which involve pretraining real-valued vectors ("embeddings") at the word, sentence, or paragraph level; and using them in conjunction with a specific architecture for each individual downstream task.</p><p>• Fine-tuning methods, which introduce a minimal number of task-specific parameters, and instead copy the weights from a pre-trained network and then tune them to a particular downstream task.</p><p>Embeddings or language models can be divided into fixed, meaning that they generate a single representation for each word in the vocabulary; and contextualized, meaning that a representation is generated based on both the word and its surrounding context, so that a single word can have multiple representations, each one depending on how it is used.</p><p>In practice, most fixed embeddings are used as feature-based models. The most notable examples are word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>, GloVe <ref type="bibr" target="#b25">(Pennington et al., 2014)</ref> and fastText <ref type="bibr" target="#b21">(Mikolov et al., 2018)</ref>. All of them are extensively used in a variety of applications nowadays. On the other hand, contextualized word representations and language models have been developed using both featurebased architectures, the most notable examples being ELMo and Flair <ref type="bibr" target="#b26">(Peters et al., 2018;</ref><ref type="bibr" target="#b0">Akbik et al., 2018)</ref>, and transformer based architectures, that are commonly used in a fine-tune setting, as is the case of GPT-1, GPT-2 <ref type="bibr" target="#b28">(Radford et al., 2018</ref><ref type="bibr" target="#b29">(Radford et al., , 2019))</ref>, BERT and its derivatives <ref type="bibr">(Devlin et al., 2018;</ref><ref type="bibr" target="#b18">Liu et al., 2019;</ref><ref type="bibr" target="#b17">Lan et al., 2019)</ref> and more recently T5 <ref type="bibr" target="#b30">(Raffel et al., 2019)</ref>. All of them have repeatedly improved the state-of-the art in many downstream NLP tasks over the last year.</p><p>In general, the main advantage of using language models is that they are mostly built in an unsupervised manner and they can be trained with raw, unannotated plain text. Their main drawback is that enormous quantities of data seem to be required to properly train them especially in the case of contextualized models, for which larger corpora are thought to be needed to properly address polysemy and cover the wide range of uses that commonly exist within languages.</p><p>For gathering data in a wide range of languages,</p><p>Wikipedia is a commonly used option. It has been used to train fixed embeddings <ref type="bibr" target="#b1">(Al-Rfou et al., 2013;</ref><ref type="bibr" target="#b3">Bojanowski et al., 2017)</ref> and more recently the multilingual BERT <ref type="bibr">(Devlin et al., 2018)</ref>, hereafter mBERT. However, for some languages, Wikipedia might not be large enough to train good quality contextualized word embeddings. Moreover, Wikipedia data all belong to the same specific genre and style. To address this problem, one can resort to crawled text from the internet; the largest and most widespread dataset of crawled text being Common Crawl. 1 Such an approach generally solves the quantity and genre/style coverage problems but might introduce noise in the data, an issue which has earned the corpus some criticism, most notably by Trinh and Le (2018) and <ref type="bibr" target="#b29">Radford et al. (2019)</ref>. Using Common Crawl also leads to data management challenges as the corpus is distributed in the form of a large set of plain text each containing a large quantity of unclassified multilingual documents from different websites.</p><p>In this paper we study the trade-off between quantity and quality of data for training contextualized representations. To this end, we use the OSCAR corpus <ref type="bibr">(Ortiz Suárez et al., 2019)</ref>, a freely available 2 multilingual dataset obtained by performing language classification, filtering and cleaning of the whole Common Crawl corpus. 3 OS-CAR was created following the approach of <ref type="bibr" target="#b11">Grave et al. (2018)</ref> but proposing a simple improvement on their filtering method. We then train OSCARbased and Wikipedia-based ELMo contextualized word embeddings <ref type="bibr" target="#b26">(Peters et al., 2018)</ref> for 5 languages: Bulgarian, Catalan, Danish, Finnish and Indonesian. We evaluate the models by attaching them to the to UDPipe 2.0 architecture <ref type="bibr" target="#b32">(Straka, 2018;</ref><ref type="bibr" target="#b34">Straka et al., 2019)</ref> for dependency parsing and part-of-speech (POS) tagging. We show that the models using the OSCAR-based ELMo embeddings consistently outperform the Wikipediabased ones, suggesting that big high-coverage noisy corpora might be better than small high-quality narrow-coverage corpora for training contextualized language representations 4 . We also establish a new state of the art for both POS tagging and dependency parsing in 6 different treebanks covering 1 https://commoncrawl.org 2 https://oscar-corpus.com 3 Snapshot from November 2018 4 Both the Wikipedia-and the OSCAR-based embeddings for these 5 languages are available at: https://oscarcorpus.com/#models.</p><p>all 5 languages.</p><p>The structure of the paper is as follows. In Section 2 we describe the recent related work. In Section 3 we present, compare and analyze the corpora used to train our contextualized embeddings, and the treebanks used to train our POS tagging and parsing models. In Section 4 we examine and describe in detail the model used for our contextualized word representations, as well as the parser and the tagger we chose to evaluate the impact of corpora in the embeddings' performance in downstream tasks. Finally we provide an analysis of our results in Section 5 and in Section 6 we present our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since the introduction of word2vec <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>, many attempts have been made to create multilingual language representations; for fixed word embeddings the most remarkable works are those of <ref type="bibr" target="#b1">(Al-Rfou et al., 2013)</ref> and <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref> who created word embeddings for a large quantity of languages using Wikipedia, and later <ref type="bibr" target="#b11">(Grave et al., 2018)</ref> who trained the fast-Text word embeddings for 157 languages using Common Crawl and who in fact showed that using crawled data significantly increased the performance of the embeddings especially for mid-to low-resource languages.</p><p>Regarding contextualized models, the most notable non-English contribution has been that of the mBERT <ref type="bibr">(Devlin et al., 2018)</ref>, which is distributed as (i) a single multilingual model for 100 different languages trained on Wikipedia data, and as (ii) a single multilingual model for both Simplified and Traditional Chinese. Four monolingual fully trained ELMo models have been distributed for Japanese, Portuguese, German and Basque<ref type="foot" target="#foot_0">5</ref> ; 44 monolingual ELMo models<ref type="foot" target="#foot_1">6</ref> where also released by the HIT-SCIR team <ref type="bibr" target="#b6">(Che et al., 2018)</ref> during the CoNLL 2018 Shared Task <ref type="bibr" target="#b40">(Zeman et al., 2018)</ref>, but their training sets where capped at 20 million words. A German BERT <ref type="bibr" target="#b5">(Chan et al., 2019)</ref> as well as a French BERT model (called CamemBERT) <ref type="bibr" target="#b19">(Martin et al., 2019)</ref> have also been released. In general no particular effort in creating a set of highquality monolingual contextualized representations has been shown yet, or at least not on a scale that is comparable with what was done for fixed word embeddings.</p><p>For dependency parsing and POS tagging the most notable non-English specific contribution is that of the CoNLL 2018 Shared Task <ref type="bibr" target="#b40">(Zeman et al., 2018)</ref>, where the 1 st place (LAS Ranking) was awarded to the HIT-SCIR team <ref type="bibr" target="#b6">(Che et al., 2018)</ref> who used <ref type="bibr" target="#b10">Dozat and Manning (2017)</ref>'s Deep Biaffine parser and its extension described in <ref type="bibr" target="#b10">(Dozat et al., 2017)</ref>, coupled with deep contextualized ELMo embeddings <ref type="bibr" target="#b26">(Peters et al., 2018)</ref>   <ref type="bibr" target="#b32">(Straka, 2018)</ref>, with mBERT greatly improving the scores of the original model, and UDify <ref type="bibr" target="#b15">(Kondratyuk and Straka, 2019)</ref>, which adds an extra attention layer on top of mBERT plus a Deep Bi-affine attention layer for dependency parsing and a Softmax layer for POS tagging. UDify is actually trained by concatenating the training sets of 124 different UD treebanks, creating a single POS tagging and dependency parsing model that works across 75 different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Corpora</head><p>We train ELMo contextualized word embeddings for 5 languages: Bulgarian, Catalan, Danish, Finnish and Indonesian. We train one set of embeddings using only Wikipedia data, and another set using only Common-Crawl-based OSCAR data. We chose these languages primarily because they are morphologically and typologically different from one another, but also because all of the OSCAR datasets for these languages were of a sufficiently manageable size such that the ELMo pre-training was doable in less than one month. Contrary to HIT-SCIR team <ref type="bibr" target="#b6">(Che et al., 2018)</ref>, we do not impose any cap on the amount of data, and instead use the entirety of Wikipedia or OSCAR for each of our 5 chosen languages. open collaboration model, its text tends to be of very high-quality in comparison to other free online resources. This is why Wikipedia has been extensively used in various NLP applications <ref type="bibr" target="#b39">(Wu and Weld, 2010;</ref><ref type="bibr" target="#b20">Mihalcea, 2007;</ref><ref type="bibr" target="#b1">Al-Rfou et al., 2013;</ref><ref type="bibr" target="#b3">Bojanowski et al., 2017)</ref>. We downloaded the XML Wikipedia dumps<ref type="foot" target="#foot_2">7</ref> and extracted the plaintext from them using the wikiextractor.py script<ref type="foot" target="#foot_3">8</ref> from Giuseppe Attardi. We present the number of words and tokens available for each of our 5 languages in Table <ref type="table" target="#tab_1">1</ref>. We decided against deduplicating the Wikipedia data as the corpora are already quite small. We tokenize the 5 corpora using UD-Pipe <ref type="bibr" target="#b33">(Straka and Straková, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Wikipedia</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OSCAR</head><p>Common Crawl is a non-profit organization that produces and maintains an open, freely available repository of crawled data from the web. Common Crawl's complete archive consists of petabytes of monthly snapshots collected since 2011. Common Crawl snapshots are not classified by language, and contain a certain level of noise (e.g. one-word "sentences" such as "OK" and "Cancel" are unsurprisingly very frequent). This is what motivated the creation of the freely available multilingual OSCAR corpus <ref type="bibr">(Ortiz Suárez et al., 2019)</ref>, extracted from the November 2018 snapshot, which amounts to more than 20 terabytes of plain-text. In order to create OSCAR from this Common Crawl snapshot, Ortiz Suárez et al. ( <ref type="formula">2019</ref>) reproduced the pipeline proposed by <ref type="bibr" target="#b11">(Grave et al., 2018)</ref> to process, filter and classify Common Crawl. More precisely, language classification was performed using the fastText linear classifier <ref type="bibr" target="#b13">(Joulin et al., 2016</ref><ref type="bibr" target="#b14">(Joulin et al., , 2017))</ref>, which was trained by <ref type="bibr" target="#b11">Grave et al. (2018)</ref> to recognize 176 languages and was shown to have an extremely good accuracy to processing time trade-off. The filtering step as performed by <ref type="bibr" target="#b11">Grave et al. (2018)</ref> consisted in only keeping the lines exceeding 100  bytes in length.<ref type="foot" target="#foot_4">9</ref> However, considering that Common Crawl is a mutilingual UTF-8 encoded corpus, this 100-byte threshold creates a huge disparity between ASCII and non-ASCII encoded languages. The filtering step used to create OSCAR therefore consisted in only keeping the lines containing at least 100 UTF-8-encoded characters. Finally, as in <ref type="bibr" target="#b11">(Grave et al., 2018)</ref>, the OSCAR corpus is deduplicated, i.e. for each language, only one occurrence of a given line is included.</p><p>As we did for Wikipedia, we tokenize OSCAR corpora for the 5 languages we chose for our study using UDPipe. Table <ref type="table" target="#tab_3">2</ref> provides quantitative information about the 5 resulting tokenized corpora.</p><p>We note that the original Common-Crawl-based corpus created by <ref type="bibr" target="#b11">Grave et al. (2018)</ref> to train fast-Text is not freely available. Since running the experiments described in this paper, a new architecture for creating a Common-Crawl-based corpus named CCNet <ref type="bibr" target="#b38">(Wenzek et al., 2019)</ref> has been published, although it includes specialized filtering which might result in a cleaner corpus compared to OSCAR, the resulting CCNet corpus itself was not published. Thus we chose to keep OSCAR as it remains the only very large scale, Common-Crawl-based corpus currently available and easily downloadable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Noisiness</head><p>We wanted to address (Trinh and Le, 2018) and <ref type="bibr" target="#b29">(Radford et al., 2019)</ref>'s criticisms of Common Crawl, so we devised a simple method to measure how noisy the OSCAR corpora were for our 5 languages. We randomly extract a number of lines from each corpus, such that the resulting random sample contains one million words. <ref type="foot" target="#foot_5">10</ref> We test if the words are in the corresponding GNU Aspell<ref type="foot" target="#foot_6">11</ref> dictionary. We repeat this task for each of the 5 languages, for both the OSCAR and the Wikipedia  corpora. We compile in Table <ref type="table" target="#tab_5">3</ref> the number of out-of-vocabulary tokens for each corpora.</p><p>As expected, this simple metric shows that in general the OSCAR samples contain more out-ofvocabulary words than the Wikipedia ones. However the difference in magnitude between the two is strikingly lower that one would have expected in view of the criticisms by Trinh and Le (2018) and <ref type="bibr" target="#b29">Radford et al. (2019)</ref>, thereby validating the usability of Common Crawl data when it is properly filtered, as was achieved by the OSCAR creators. We even observe that, for Danish, the number of out-of-vocabulary words in OSCAR is lower than that in Wikipedia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head><p>The main goal of this paper is to show the impact of training data on contextualized word representations when applied in particular downstream tasks. To this end, we train different versions of the Embeddings from Language Models (ELMo) <ref type="bibr" target="#b26">(Peters et al., 2018)</ref> for both the Wikipedia and OSCAR corpora, for each of our selected 5 languages. We save the models' weights at different number of epochs for each language, in order to test how corpus size affect the embeddings and to see whether and when overfitting happens when training elmo on smaller corpora.</p><p>We take each of the trained ELMo models and use them in conjunction with the UDPipe 2.0 <ref type="bibr" target="#b32">(Straka, 2018;</ref><ref type="bibr" target="#b34">Straka et al., 2019)</ref> architecture for dependency parsing and POS-tagging to test our models. We train UDPipe 2.0 using gold tokenization and segmentation for each of our ELMo models, the only thing that changes from training to training is the ELMo model as hyperparameters always remain at the default values (except for number of training tokens) <ref type="bibr" target="#b26">(Peters et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Contextualized word embeddings</head><p>Embeddings from Language Models (ELMo) <ref type="bibr" target="#b26">(Peters et al., 2018</ref>) is an LSTM-based language model. More precisely, it uses a bidirectional language model, which combines a forward and a backward LSTM-based language model. ELMo also computes a context-independent token representation via a CNN over characters.</p><p>We train ELMo models for Bulgarian, Catalan, Danish, Finnish and Indonesian using the OSCAR corpora on the one hand and the Wikipedia corpora on the other. We train each model for 10 epochs, as was done for the original English ELMo <ref type="bibr" target="#b26">(Peters et al., 2018)</ref>. We save checkpoints at 1 st , 3 rd and 5 th epoch in order to investigate some concerns about possible overfitting for smaller corpora (Wikipedia in this case) raised by the original ELMo authors. 12 4.2 UDPipe 2.0</p><p>For our POS tagging and dependency parsing evaluation, we use UDPipe 2.0, which has a freely available and ready to use implementation. 13 This architecture was submitted as a participant to the 2018 CoNLL Shared Task <ref type="bibr" target="#b40">(Zeman et al., 2018)</ref>, obtaining the 3 rd place in LAS ranking. UDPipe 2.0 is a multi-task model that predicts POS tags, lemmas and dependency trees jointly.</p><p>The original UDPipe 2.0 implementation calculates 3 different embeddings, namely:</p><p>• Pre-trained word embeddings: In the original implementation, the Wikipedia version of fast-Text embeddings is used <ref type="bibr" target="#b3">(Bojanowski et al., 2017)</ref>; we replace them in favor of the newer Common-Crawl-based fastText embeddings trained by <ref type="bibr" target="#b11">Grave et al. (2018)</ref>.</p><p>• Trained word embeddings: Randomly initialized word representations that are trained with the rest of the network.</p><p>• Character-level word embeddings: Computed using bi-directional GRUs of dimension 256. They represent every UTF-8 encoded character with two 256 dimensional vectors, one for the forward and one for the backward layer. This two vector representations are concatenated and are trained along the whole network.</p><p>After the CoNLL 2018 Shared Task, the UD-Pipe 2.0 authors added the option to concatenate contextualized representations to the embedding section of the network <ref type="bibr" target="#b34">(Straka et al., 2019)</ref>, we use this new implementation and we concatenate our pretrained deep contextualized ELMo embeddings to the three embeddings mentioned above.</p><p>Once the embedding step is completed, the concatenation of all vector representations for a word are fed to two shared bidirectional LSTM (Hochreiter and Schmidhuber, 1997) layers. The output of these two BiLSTMS is then fed to two separate specific LSTMs:</p><p>• The tagger-and lemmatizer-specific bidirectional LSTMs, with Softmax classifiers on top, which process its output and generate UPOS, XPOS, UFeats and Lemmas. The lemma classifier also takes the character-level word embeddings as input.</p><p>• The parser-specific bidirectional LSTM layer, whose output is then passed to a bi-affine attention layer <ref type="bibr" target="#b10">(Dozat and Manning, 2017)</ref> producing labeled dependency trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Treebanks</head><p>To train the selected parser and tagger (cf. Section 4.2) and evaluate the pre-trained language models in our 5 languages, we run our experiments using the Universal Dependencies (UD) 14 paradigm and its corresponding UD POS tag set <ref type="bibr" target="#b27">(Petrov et al., 2012)</ref>. We use all the treebanks available for our five languages in the UD treebank collection version 2.2 <ref type="bibr">(Nivre et al., 2018)</ref>, which was used for the CoNLL 2018 shared task, thus we perform our evaluation tasks in 6 different treebanks (see Table <ref type="table" target="#tab_6">4</ref> for treebank size information).</p><p>• Bulgarian BTB: Created at the Institute of Information and Communication Technologies, Bulgarian Academy of Sciences, it consists of legal documents, news articles and fiction pieces.</p><p>• Catalan-AnCora: Built on top of the Spanish-Catalan AnCora corpus <ref type="bibr" target="#b36">(Taulé et al., 2008)</ref>, it contains mainly news articles.</p><p>• Danish-DDT: Converted from the Danish Dependency Treebank <ref type="bibr" target="#b4">(Buch-Kromann, 2003)</ref>. It includes news articles, fiction and non fiction texts and oral transcriptions.</p><p>• Finnish-FTB: Consists of manually annotated grammatical examples from VISK 15 (The Web Version of the Large Grammar of Finnish).</p><p>• Finnish-TDT: Based on the Turku Dependency Treebank (TDT). Contains texts from Wikipedia, Wikinews, news articles, blog entries, magazine articles, grammar examples, Europarl speeches, legal texts and fiction.</p><p>• Indonesian-GSD: Includes mainly blog entries and news articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results &amp; Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Parsing and POS tagging results</head><p>We use UDPipe 2.0 without contextualized embeddings as our baseline for POS tagging and dependency parsing. However, we did not train the model without contextualized word embedding ourselves. We instead take the scores as they are reported in <ref type="bibr" target="#b15">(Kondratyuk and Straka, 2019)</ref>. We also compare our UDPipe 2.0 + ELMo models against the state-of-the-art results (assuming gold tokenization) for these languages, which are either UDify <ref type="bibr" target="#b15">(Kondratyuk and Straka, 2019)</ref> or UDPipe 2.0 + mBERT <ref type="bibr" target="#b34">(Straka et al., 2019)</ref>. Results for UPOS, UAS and LAS are shown in Table <ref type="table" target="#tab_8">5</ref>. We obtain the state of the art for the three metrics in each of the languages with the UDPipe 2.0 + ELMo OSCAR models. We also see that in every single case the UDPipe 2.0 + ELMo OSCAR result surpasses the UDPipe 2.0 + ELMo Wikipedia one, suggesting that the size of the pre-training data plays an important role in downstream task results. This is also supports our hypothesis that the OSCAR corpora, being multi-domain, exhibits a better coverage of the different styles, genres and uses present at least in these 5 languages.</p><p>Taking a closer look at the results for Danish, we see that ELMo Wikipedia , which was trained with a mere 300MB corpus, does not show any sign  of overfitting, as the UDPipe 2.0 + ELMo Wikipedia results considerably improve the UDPipe 2.0 baseline. This is the case for all of our ELMo Wikipedia models as we never see any evidence of a negative impact when we add them to the baseline model.</p><p>In fact, the results of UDPipe 2.0 + ELMo Wikipedia give better than previous state-of-the-art results in all metrics for the Finnish-FTB and in UPOS for the Finnish-TDT. The results for Finnish are actually quite interesting, as mBERT was pre-trained on Wikipedia and here we see that the multilingual setting in which UDify was fine-tuned exhibits subbaseline results for all metrics, and that the UD-Pipe + mBERT scores are often lower than those of our UDPipe 2.0 + ELMo Wikipedia . This actually suggests that even though the multilingual approach of mBERT (in pre-training) or UDify (in pre-training and fine-tuning) leads to better performance for high-resource languages or languages that are closely related to high-resource languages, it might also significantly degrade the representations for more isolated or even simply more morphologically rich languages like Finnish. In contrast, our monolingual approach with UDPipe 2.0 + ELMo OSCAR improves the previous SOTA considerably, by more than 2 points for some metrics. Note however that Indonesian, which might also be seen as a relatively isolated language, does not behave in the same way as Finnish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of the number of training epochs</head><p>An important topic we wanted to address with our experiments was that of overfitting and the number of epochs one should train the contextualized embeddings for. The ELMo authors have expressed that increasing the number of training epochs is generally better, as they argue that training the ELMo model for longer reduces held-out perplexity and further improves downstream task performance. 16 This is why we intentionally fully pre-trained the ELMo Wikipedia to the 10 epochs of the original ELMo paper, as its authors also expressed concern over the possibility of overfitting for smaller corpora. We thus save checkpoints for 16 Their comments on the matter can be found here.</p><p>each of our ELMo model at the 1, 3, 5 and 10 epoch marks so that we can properly probe for overfitting.</p><p>The scores of all checkpoints are reported in Table <ref type="table" target="#tab_9">6</ref>. Here again we do not train the UDPipe 2.0 baselines without embedding, we just report the scores published in <ref type="bibr" target="#b15">Kondratyuk and Straka (2019)</ref>.</p><p>The first striking finding is that even though all our Wikipedia data sets are smaller than 1GB in size (except for Catalan), none of the ELMo Wikipedia models show any sign of overfitting, as the results continue to improve for all metrics the more we train the ELMo models, with the best results consistently being those of the fully trained 10 epoch ELMos. For all of our Wikipedia models, but those of Catalan and Indonesian, we see sub-baseline results at 1 epoch; training the model for longer is better, even if the corpora are small in size.</p><p>ELMo OSCAR models exhibit exactly the same behavior as ELMo Wikipedia models where the scores continue to improve the longer they are pre-trained, except for the case of Finnish. Here we actually see an unexpected behavior where the model performance caps around the 3 rd to 5 th epoch. This is surprising because the Finnish OSCAR corpus is more than 20 times bigger than our smallest Wikipedia corpus, the Danish Wikipedia, that did not exhibit this behavior. As previously mentioned Finnish is morphologically richer than the other languages in which we trained ELMo, we hypothesize that the representation space given by the ELMo embeddings might not be sufficiently big to extract more features from the Finnish OSCAR corpus beyond the 5 th epoch mark, however in order to test this we would need to train a larger language model like BERT which is sadly beyond our computing infrastructure limits (cf. Subsection 5.3). However we do note that pre-training our current language model architectures in a morphologically rich language like Finnish might actually better expose the limits of our existing approaches to language modeling.</p><p>One last thing that it is important to note with respect to the number of training epochs is that even though we fully pre-trained our ELMo Wikipedia 's and ELMo OSCAR 's to the recommended 10 epoch mark, and then compared them against one another, the number of training steps between both pre-trained models differs drastically due to the big difference in corpus size (for Indonesian, for instance, 10 epochs correspond to 78K steps for ELMo Wikipedia and to 2.6M steps for OSCAR; the complete picture is provided in the Appendix, in Table <ref type="table">8</ref>). In fact, we can see in Table <ref type="table" target="#tab_9">6</ref> that all the UDPipe 2.0 + ELMo OSCAR(1) perform better than the UDPipe 2.0 + ELMo Wikipedia(1) models across all metrics. Thus we believe that talking in terms of training steps as opposed to training epochs might be a more transparent way of comparing two pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Computational cost and carbon footprint</head><p>Considering the discussion above, we believe an interesting follow-up to our experiments would be training the ELMo models for more of the languages included in the OSCAR corpus. However training ELMo is computationally costly, and one way to estimate this cost, as pointed out by <ref type="bibr" target="#b35">Strubell et al. (2019)</ref>, is by using the training times of each model to compute both power consumption and CO 2 emissions.</p><p>In our set-up we used two different machines, each one having 4 NVIDIA GeForce GTX 1080 Ti graphic cards and 128GB of RAM, the difference between the machines being that one uses a single Intel Xeon Gold 5118 processor, while the other uses two Intel Xeon E5-2630 v4 processors. One GeForce GTX 1080 Ti card is rated at around </p><formula xml:id="formula_0">p t = 1.58t(cp c + p r + gp g )<label>1000</label></formula><p>Where c and g are the number of CPUs and GPUs respectively, p c is the average power draw (in Watts) from all CPU sockets, p r the average power draw from all DRAM sockets, and p g the average power draw of a single GPU. We estimate the total power consumption by adding GPU, CPU and DRAM consumptions, and then multiplying by the Power Usage Effectiveness (PUE), which accounts for the additional energy required to support the compute infrastructure. We use a PUE coefficient of 1.58, the 2018 global average for data centers <ref type="bibr" target="#b35">(Strubell et al., 2019)</ref>. In table <ref type="table" target="#tab_10">7</ref> we report the training times in both hours and days, as well as the total power draw (in Watts) of the system used to train each individual ELMo model. We use this in-  <ref type="bibr">processor-e5-2630-v4-25m-cache-2-20-ghz. html</ref> formation to compute the total power consumption of each ELMo, also reported in table <ref type="table" target="#tab_10">7</ref>.</p><p>We can further estimate the CO 2 emissions in kilograms of each single model by multiplying the total power consumption by the average CO 2 emissions per kWh in France (where the models were trained). According to the RTE (Réseau de transport d'électricité / Electricity Transmission Network) the average emission per kWh were around 51g/kWh in November 2019,<ref type="foot" target="#foot_7">20</ref> when the models were trained. Thus the total CO 2 emissions in kg for one single model can be computed as:</p><p>CO 2 e = 0.051p t All emissions for the ELMo models are also reported in table 7.</p><p>We do not report the power consumption or the carbon footprint of training the UDPipe 2.0 architecture, as each model took less than 4 hours to train on a machine using a single NVIDIA Tesla V100 card. Also, this machine was shared during training time, so it would be extremely difficult to accurately estimate the power consumption of these models.</p><p>Even though it would have been interesting to replicate all our experiments and computational cost estimations with state-of-the-art fine-tuning models such as BERT, XLNet, RoBERTa or AL-BERT, we recall that these transformer-based architectures are extremely costly to train, as noted by the BERT authors on the official BERT GitHub repository,<ref type="foot" target="#foot_8">21</ref> and are currently beyond the scope of our computational infrastructure. However we believe that ELMo contextualized word embeddings remain a useful model that still provide an extremely good trade-off between performance to training cost, even setting new state-of-the-art scores in parsing and POS tagging for our five chosen languages, performing even better than the multilingual mBERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we have explored the use of the Common-Crawl-based OSCAR corpora to train ELMo contextualized embeddings for five typologically diverse mid-resource languages. We have compared them with Wikipedia-based ELMo embeddings on two classical NLP tasks, POS tagging and parsing, using state-of-the-art neural architectures. Our goal was to explore whether the noisiness level of Common Crawl data, often invoked to criticize the use of such data, could be compensated by its larger size; for some languages, the OSCAR corpus is several orders of magnitude larger than the corresponding Wikipedia. Firstly, we found that when properly filtered, Common Crawl data is not massively noisier than Wikipedia. Secondly, we show that embeddings trained using OSCAR data consistently outperform Wikipedia-based embeddings, to the extent that they allow us to improve the state of the art in POS tagging and dependency parsing for all the 6 chosen treebanks. Thirdly, we observe that more training epochs generally results in better embeddings even when the training data is relatively small, as is the case for Wikipedia.</p><p>Our experiments show that Common-Crawlbased data such as the OSCAR corpus can be used to train high-quality contextualized embeddings, even for languages for which more standard textual resources lack volume or genre variety. This could result in better performances in a number of NLP tasks for many non highly resourced languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Wikipedia is the biggest online multilingual open encyclopedia, comprising more than 40 million articles in 301 different languages. Because articles are curated by language and written in an Size of Wikipedia corpora, measured in bytes, thousands of tokens, words and sentences.</figDesc><table><row><cell>Language</cell><cell>Size</cell><cell cols="3">#Ktokens #Kwords #Ksentences</cell></row><row><cell>Bulgarian</cell><cell>609M</cell><cell>64,190</cell><cell>54,748</cell><cell>3,685</cell></row><row><cell>Catalan</cell><cell>1.1G</cell><cell>211,627</cell><cell>179,108</cell><cell>8,293</cell></row><row><cell>Danish</cell><cell>338M</cell><cell>60,644</cell><cell>52,538</cell><cell>3,226</cell></row><row><cell>Finnish</cell><cell>669M</cell><cell>89,580</cell><cell>76,035</cell><cell>6,847</cell></row><row><cell cols="2">Indonesian 488M</cell><cell>80,809</cell><cell>68,955</cell><cell>4,298</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Size of OSCAR subcorpora, measured in bytes, thousands of tokens, words and sentences.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Number of out-of-vocabulary words in random samples of 1M words for OSCAR and Wikipedia.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Size of treebanks, measured in thousands of tokens and sentences.</figDesc><table><row><cell>12 https://github.com/allenai/bilm-tf/</cell></row><row><cell>issues/135</cell></row><row><cell>13 https://github.com/CoNLL-UD-2018/</cell></row><row><cell>UDPipe-Future</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note><p><p><p><p><p><p><p>Scores from UDPipe 2.0 (from</p><ref type="bibr" target="#b15">Kondratyuk and Straka, 2019)</ref></p>, the previous state-of-the-art models UDPipe 2.0+mBERT</p><ref type="bibr" target="#b34">(Straka et al., 2019)</ref> </p>and UDify</p><ref type="bibr" target="#b15">(Kondratyuk and Straka, 2019)</ref></p>, and our ELMoenhanced UDPipe 2.0 models. Test scores are given for UPOS, UAS and LAS in all five languages. Best scores are shown in bold, second best scores are underlined.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>UPOS, UAS and LAS scores for the UDPipe 2.0 baseline reported by<ref type="bibr" target="#b15">(Kondratyuk and Straka, 2019)</ref>, plus the scores for checkpoints at 1, 3, 5 and 10 epochs for all the ELMo OSCAR and ELMo Wikipedia . All scores are test scores. Best ELMo OSCAR scores are shown in bold while best ELMo Wikipedia scores are underlined.</figDesc><table><row><cell>Treebank</cell><cell>Model</cell><cell>UPOS UAS</cell><cell>LAS</cell><cell>Treebank</cell><cell>Model</cell><cell>UPOS UAS</cell><cell>LAS</cell></row><row><cell></cell><cell>UDPipe 2.0</cell><cell cols="2">98.98 93.38 90.35</cell><cell></cell><cell>UDPipe 2.0</cell><cell cols="2">96.65 90.68 87.89</cell></row><row><cell></cell><cell>+ELMoWikipedia(1)</cell><cell cols="2">98.81 93.60 90.21</cell><cell></cell><cell>+ELMoWikipedia(1)</cell><cell cols="2">95.86 89.63 86.39</cell></row><row><cell></cell><cell>+ELMoWikipedia(3)</cell><cell cols="2">99.01 94.32 91.36</cell><cell></cell><cell>+ELMoWikipedia(3)</cell><cell cols="2">96.76 91.02 88.27</cell></row><row><cell></cell><cell>+ELMoWikipedia(5)</cell><cell cols="2">99.03 94.32 91.38</cell><cell></cell><cell>+ELMoWikipedia(5)</cell><cell cols="2">96.97 91.66 89.04</cell></row><row><cell>Bulgarian BTB</cell><cell>+ELMoWikipedia(10)</cell><cell cols="2">99.17 94.93 92.05</cell><cell>Finnish-FTB</cell><cell>+ELMoWikipedia(10)</cell><cell cols="2">97.27 92.05 89.62</cell></row><row><cell></cell><cell>+ELMoOSCAR(1)</cell><cell cols="2">99.28 95.45 92.98</cell><cell></cell><cell>+ELMoOSCAR(1)</cell><cell cols="2">97.91 93.41 91.43</cell></row><row><cell></cell><cell>+ELMoOSCAR(3)</cell><cell cols="2">99.34 95.58 93.12</cell><cell></cell><cell>+ELMoOSCAR(3)</cell><cell cols="2">98.00 93.99 91.98</cell></row><row><cell></cell><cell>+ELMoOSCAR(5)</cell><cell cols="2">99.34 95.63 93.25</cell><cell></cell><cell>+ELMoOSCAR(5)</cell><cell cols="2">98.15 93.98 92.24</cell></row><row><cell></cell><cell>+ELMoOSCAR(10)</cell><cell cols="2">99.40 96.01 93.56</cell><cell></cell><cell>+ELMoOSCAR(10)</cell><cell cols="2">98.13 93.81 92.02</cell></row><row><cell></cell><cell>UDPipe 2.0</cell><cell cols="2">98.88 93.22 91.06</cell><cell></cell><cell>UDPipe 2.0</cell><cell cols="2">97.45 89.88 87.46</cell></row><row><cell></cell><cell>+ELMoWikipedia(1)</cell><cell cols="2">98.93 93.24 91.21</cell><cell></cell><cell>+ELMoWikipedia(1)</cell><cell cols="2">96.73 89.11 86.33</cell></row><row><cell></cell><cell>+ELMoWikipedia(3)</cell><cell cols="2">99.02 93.75 91.93</cell><cell></cell><cell>+ELMoWikipedia(3)</cell><cell cols="2">97.55 90.84 88.50</cell></row><row><cell></cell><cell>+ELMoWikipedia(5)</cell><cell cols="2">99.04 93.86 92.05</cell><cell></cell><cell>+ELMoWikipedia(5)</cell><cell cols="2">97.55 91.11 88.88</cell></row><row><cell cols="2">Catalan-AnCora +ELMoWikipedia(10)</cell><cell cols="2">99.05 93.99 92.24</cell><cell>Finnish-TDT</cell><cell>+ELMoWikipedia(10)</cell><cell cols="2">97.65 91.60 89.34</cell></row><row><cell></cell><cell>+ELMoOSCAR(1)</cell><cell cols="2">99.07 93.92 92.29</cell><cell></cell><cell>+ELMoOSCAR(1)</cell><cell cols="2">98.27 93.03 91.29</cell></row><row><cell></cell><cell>+ELMoOSCAR(3)</cell><cell cols="2">99.10 94.29 92.69</cell><cell></cell><cell>+ELMoOSCAR(3)</cell><cell cols="2">98.38 93.60 91.83</cell></row><row><cell></cell><cell>+ELMoOSCAR(5)</cell><cell cols="2">99.07 94.38 92.75</cell><cell></cell><cell>+ELMoOSCAR(5)</cell><cell cols="2">98.39 93.57 91.80</cell></row><row><cell></cell><cell>+ELMoOSCAR(10)</cell><cell cols="2">99.06 94.49 92.88</cell><cell></cell><cell>+ELMoOSCAR(10)</cell><cell cols="2">98.36 93.54 91.77</cell></row><row><cell></cell><cell>UDPipe 2.0</cell><cell cols="2">97.78 86.88 84.31</cell><cell></cell><cell>UDPipe 2.0</cell><cell cols="2">93.69 85.31 78.99</cell></row><row><cell></cell><cell>+ELMoWikipedia(1)</cell><cell cols="2">97.47 86.98 84.15</cell><cell></cell><cell>+ELMoWikipedia(1)</cell><cell cols="2">93.70 85.81 79.46</cell></row><row><cell></cell><cell>+ELMoWikipedia(3)</cell><cell cols="2">98.03 88.16 85.81</cell><cell></cell><cell>+ELMoWikipedia(3)</cell><cell cols="2">93.90 86.04 79.72</cell></row><row><cell></cell><cell>+ELMoWikipedia(5)</cell><cell cols="2">98.15 88.24 85.96</cell><cell></cell><cell>+ELMoWikipedia(5)</cell><cell cols="2">94.04 85.93 79.97</cell></row><row><cell>Danish-DDT</cell><cell>+ELMoWikipedia(10)</cell><cell cols="2">98.45 89.05 86.92</cell><cell cols="2">Indonesian-GSD +ELMoWikipedia(10)</cell><cell cols="2">93.94 86.16 80.10</cell></row><row><cell></cell><cell>+ELMoOSCAR(1)</cell><cell cols="2">98.50 89.47 87.43</cell><cell></cell><cell>+ELMoOSCAR(1)</cell><cell cols="2">93.95 86.25 80.23</cell></row><row><cell></cell><cell>+ELMoOSCAR(3)</cell><cell cols="2">98.59 89.68 87.77</cell><cell></cell><cell>+ELMoOSCAR(3)</cell><cell cols="2">94.00 86.21 80.14</cell></row><row><cell></cell><cell>+ELMoOSCAR(5)</cell><cell cols="2">98.59 89.46 87.64</cell><cell></cell><cell>+ELMoOSCAR(5)</cell><cell cols="2">94.23 86.37 80.40</cell></row><row><cell></cell><cell>+ELMoOSCAR(10)</cell><cell cols="2">98.62 89.84 87.95</cell><cell></cell><cell>+ELMoOSCAR(10)</cell><cell cols="2">94.12 86.49 80.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Average power draw (Watts), training times (in both hours and days), mean power consumption (KWh) and CO 2 emissions (kg) for each ELMo model trained.250 W, 17 the Xeon Gold 5118 processor is rated at 105 W, 18 while one Xeon E5-2630 v4 is rated at 85 W.19 For the DRAM we can use the work of<ref type="bibr" target="#b7">Desrochers et al. (2016)</ref> to estimate the total power draw of 128GB of RAM at around 13W. Having this information, we can now use the formula proposed by<ref type="bibr" target="#b35">Strubell et al. (2019)</ref> in order to compute the total power required to train one ELMo model:</figDesc><table><row><cell>Language</cell><cell>Power</cell><cell>Hours</cell><cell cols="2">Days KWh•PUE</cell><cell>CO2e</cell></row><row><cell cols="3">OSCAR-Based ELMos</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bulgarian</cell><cell cols="3">1183 515.00 21.45</cell><cell>962.61</cell><cell>49.09</cell></row><row><cell>Catalan</cell><cell cols="2">1118 199.98</cell><cell>8.33</cell><cell>353.25</cell><cell>18.02</cell></row><row><cell>Danish</cell><cell cols="2">1183 200.89</cell><cell>8.58</cell><cell>375.49</cell><cell>19.15</cell></row><row><cell>Finnish</cell><cell cols="3">1118 591.25 24.63</cell><cell>1044.40</cell><cell>53.26</cell></row><row><cell>Indonesian</cell><cell cols="3">1183 694.26 28.93</cell><cell>1297.67</cell><cell>66.18</cell></row><row><cell cols="3">Wikipedia-Based ELMos</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Bulgarian</cell><cell>1118</cell><cell>15.45</cell><cell>0.64</cell><cell>27.29</cell><cell>1.39</cell></row><row><cell>Catalan</cell><cell>1118</cell><cell>51.08</cell><cell>2.13</cell><cell>90.22</cell><cell>4.60</cell></row><row><cell>Danish</cell><cell>1118</cell><cell>14.56</cell><cell>0.61</cell><cell>25,72</cell><cell>1.31</cell></row><row><cell>Finnish</cell><cell>1118</cell><cell>21.79</cell><cell>0.91</cell><cell>38.49</cell><cell>1.96</cell></row><row><cell>Indonesian</cell><cell>1118</cell><cell>20.28</cell><cell>0.84</cell><cell>35.82</cell><cell>1.82</cell></row><row><cell cols="2">TOTAL EMISSIONS</cell><cell></cell><cell></cell><cell></cell><cell>216.78</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>https://allennlp.org/elmo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>https://github.com/HIT-SCIR/ ELMoForManyLangs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>XML dumps from April 4, 2019.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"><p>Available here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4"><p>Script available here.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_5"><p>We remove tokens that are capitalized or contain less than 4 UTF-8 encoded characters, allowing us to remove bias against Wikipedia, which traditionally contains a large quantity of proper nouns and acronyms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_6"><p>http://aspell.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_7"><p>https://www.rte-france.com/fr/ eco2mix/eco2mix-co2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_8"><p>https://github.com/google-research/ bert</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to thank Ganesh Jawahar for his insightful comments and suggestions during the early stages of this project. This work was partly funded by the French national ANR grant BASNUM (ANR-18-CE38-0003), as well as by the last author's chair in the PRAIRIE institute, 22 funded by the French national ANR as part of the "Investissements d'avenir" programme under the reference ANR-19-P3IA-0001. The authors are grateful to Inria Sophia Antipolis -Méditerranée "Nef" 23 computation cluster for providing resources and support.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018</title>
		<meeting>the 27th International Conference on Computational Linguistics, COLING 2018<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08-20">2018. August 20-26, 2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ClustersSophia for multilingual NLP</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<ptr target="http://prairie-institute.fr/23https://wiki.inria.fr/wikis/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
	<note>Polyglot: Distributed word representations 22</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Morphosyntactic tagging with a meta-BiLSTM model over context sensitive token encodings</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonçalo</forename><surname>Simões</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2642" to="2652" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The danish dependency treebank and the dtag treebank tool</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Buch-Kromann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Workshop on Treebanks and Linguistic Theories (TLT)</title>
		<meeting><address><addrLine>Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Branden</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Pietsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanay</forename><surname>Soni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin</forename><forename type="middle">Man</forename><surname>Yeung</surname></persName>
		</author>
		<ptr target="https://deepset.ai/german-bert" />
	</analytic>
	<monogr>
		<title level="j">German BERT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A validation of dram rapl power measurements</title>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Desrochers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chad</forename><surname>Paradis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">M</forename><surname>Weaver</surname></persName>
		</author>
		<idno type="DOI">10.1145/2989081.2989088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Memory Systems, MEMSYS &apos;16</title>
		<meeting>the Second International Symposium on Memory Systems, MEMSYS &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="455" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/bert/blob/master/multilingual.md" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</editor>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2018. 2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stanford&apos;s graph-based neural dependency parser at the CoNLL 2017 shared task</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-3002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="20" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning word vectors for 157 languages</title>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Language Resources and Evaluation Conference</title>
		<meeting>the 11th Language Resources and Evaluation Conference<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resource Association</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fasttext.zip: Compressing text classification models</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>CoRR, abs/1612.03651</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">75</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<idno type="arXiv">arXiv:1904.02099</idno>
		<title level="m">Languages, 1 Model: Parsing Universal Dependencies Universally</title>
		<imprint/>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv eprints</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Éric Villemonte de la Clergerie, Djamé Seddah, and Benoît Sagot</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><forename type="middle">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03894</idno>
	</analytic>
	<monogr>
		<title level="m">CamemBERT: a Tasty French Language Model</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using Wikipedia for automatic word sense disambiguation</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="196" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-07">2018. May 7-12, 2018</date>
		</imprint>
	</monogr>
	<note>LREC 2018</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>NIPS&apos;13</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Iakes Goenaga, Koldo Gojenola</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Željko</forename><surname>Agić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jesus Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gashaw</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitziber</forename><surname>Atutxa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liesbeth</forename><surname>Augustinus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Badmaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esha</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbu</forename><surname>Verginica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kepa</forename><surname>Bellato</surname></persName>
		</author>
		<author>
			<persName><surname>Bengoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Riyaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eckhard</forename><surname>Biagetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogier</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Blokland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Bobicev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Börstell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gosse</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Bouma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriane</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aljoscha</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Burchardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Candito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauthier</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gülşen</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><forename type="middle">G A</forename><surname>Cebiroglu Eryigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savas</forename><surname>Celano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabricio</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><surname>Chalub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongseok</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayeol</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvie</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélie</forename><surname>Cinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çagrı</forename><surname>Collomb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriam</forename><surname>Çöltekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marine</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Courtin</surname></persName>
		</author>
		<author>
			<persName><surname>Davidson ; Memduh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Gökırmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berta</forename><forename type="middle">Gonzáles</forename><surname>Gómez Guinovart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matias</forename><surname>Saavedra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Normunds</forename><surname>Grioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Grūzītis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Céline</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nizar</forename><surname>Guillot-Barbance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Habash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><surname>Hajič Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na-Rae</forename><surname>Hà Mỹ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dag</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbora</forename><surname>Haug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslava</forename><surname>Hladká</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florinel</forename><surname>Hlaváčová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petter</forename><surname>Hociung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><surname>Hohle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Irimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Jelínek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hüner</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Kaşıkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Kahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Kanerva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Václava</forename><surname>Kayadelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Kettnerová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kotsyba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sookyoung</forename><surname>Krek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronika</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Laippala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Lambertino</surname></persName>
		</author>
		<author>
			<persName><surname>Lando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Septina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Larasati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lavrentiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phương</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lê H Ồng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saran</forename><surname>Lenci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herman</forename><surname>Lertpradit</surname></persName>
		</author>
		<author>
			<persName><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Cheuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungtae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Ljubešić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Loginova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Lyashevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Lynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aibek</forename><surname>Macketanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Makazhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Mandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruli</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlina</forename><surname>Manurung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mȃrȃnduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><surname>Marheinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martínez</forename><surname>Héctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Mašek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Miekka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cȃtȃlin</forename><surname>Missilä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Mititelu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Montemagni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">Moreno</forename><surname>More</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinsuke</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjartur</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohdan</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadri</forename><surname>Moskalevskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yugo</forename><surname>Muischnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaili</forename><surname>Murawaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinkey</forename><surname>Müürisep</surname></persName>
		</author>
		<author>
			<persName><surname>Nainwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tiina Puolakainen, Sampo Pyysalo, Andriela Rääbis, Alexandre Rademaker, Loganathan Ramasamy, Taraka Rama, Carlos Ramisch, Vinit Ravishankar, Livy Real</title>
		<title level="s">Faculty of Mathematics and Physics</title>
		<editor>
			<persName><forename type="first">Jana</forename><surname>Strnadová</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Umut</forename><surname>Sulubacak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zsolt</forename><surname>Szántó</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dima</forename><surname>Taji</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yuta</forename><surname>Takahashi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Takaaki</forename><surname>Tanaka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabelle</forename><surname>Tellier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trond</forename><surname>Trosterud</surname></persName>
		</editor>
		<meeting><address><addrLine>Anna Nedoluzhko; Aaron Smith, Isabela Soares-Bastos, Antonio Stella, Milan Straka; Anna Trukhina, Reut Tsarfaty, Francis Tyers; Lars Wallin, Jonathan North Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Veronika Vincze</publisher>
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
	<note>Gunta Nešpore-Bērzkalne, Lương Nguy ễn Thi . , Huy ền Nguy ễn Thi. Seyi Williams, Mats Wirén, Tsegay Woldemariam, Tak-sum Wong, Chunxiao Yan, Marat M. Yavrumyan, Zhuoran Yu, Zdeněk Žabokrtský, Amir Zeldes, Daniel Zeman, Manying Zhang, and Hanzhi Zhu. 2018. Universal dependencies 2.2. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (ÚFAL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. Challenges in the Management of Large Corpora (CMLC-7) 2019</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ortiz</forename><surname>Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">T</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul<address><addrLine>Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2012-05-23">2012. May 23-25, 2012</date>
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI Blog</orgName>
		</respStmt>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">82 treebanks, 34 models: Universal dependency parsing with multi-treebank models</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miryam</forename><surname>De Lhoneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Stymne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">UDPipe 2.0 prototype at CoNLL 2018 UD shared task</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-2020</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-3009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="88" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Evaluating contextualized embeddings on 54 languages in POS tagging, lemmatization and dependency parsing</title>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<idno>CoRR, abs/1908.07448</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ancora: Multilevel annotated corpora for catalan and spanish</title>
		<author>
			<persName><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">Antònia</forename><surname>Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Language Resources and Evaluation</title>
		<meeting>the International Conference on Language Resources and Evaluation<address><addrLine>Morocco</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2008-05-26">2008. 2008, 26 May -1 June 2008</date>
		</imprint>
	</monogr>
	<note>LREC</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">CC-Net: Extracting High Quality Monolingual Datasets from Web Crawl Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open information extraction using Wikipedia</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">CoNLL 2018 shared task: Multilingual parsing from raw text to universal dependencies</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Daniel Zeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><surname>Petrov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
