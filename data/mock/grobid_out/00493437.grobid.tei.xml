<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HUMB: Automatic Key Term Extraction from Scientific Articles in GROBID</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
							<email>patricelopez@hotmail.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">INRIA &amp; HUB-IDSL</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HUMB: Automatic Key Term Extraction from Scientific Articles in GROBID</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">886EA13198AE5F0EEC87EC4ED16C98C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Semeval task 5 was an opportunity for experimenting with the key term extraction module of GROBID, a system for extracting and generating bibliographical information from technical and scientific documents. The tool first uses GROBID's facilities for analyzing the structure of scientific articles, resulting in a first set of structural features. A second set of features captures content properties based on phraseness, informativeness and keywordness measures. Two knowledge bases, GRISP and Wikipedia, are then exploited for producing a last set of lexical/semantic features. Bagged decision trees appeared to be the most efficient machine learning algorithm for generating a list of ranked key term candidates. Finally a post ranking was realized based on statistics of cousage of keywords in HAL, a large Open Access publication repository.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Key terms (or keyphrases or keywords) are metadata providing general information about the content of a document. Their selection by authors or readers is, to a large extent, subjective which makes automatic extraction difficult. This is, however, a valuable exercise, because such key terms constitute good topic descriptions of documents which can be used in particular for information retrieval, automatic document clustering and classification. Used as subject headings, better keywords can lead to higher retrieval rates of an article in a digital library.</p><p>We view automatic key term extraction as a subtask of the general problem of extraction of technical terms which is crucial in technical and scientific documents <ref type="bibr" target="#b0">(Ahmad and Collingham, 1996)</ref>.</p><p>Among the extracted terms for a given scientific document in a given collection, which key terms best characterize this document?</p><p>This article describes the system realized for the Semeval 2010 task 5, based on GROBID's (GeneRation Of BIbilographic Data) module dedicated to key term extraction. GROBID is a tool for analyzing technical and scientific documents, focusing on automatic bibliographical data extraction (header, citations, etc.) <ref type="bibr" target="#b3">(Lopez, 2009)</ref> and structure recognition (section titles, figures, etc).</p><p>As the space for the system description is very limited, this presentation focuses on key aspects. We present first an overview of our approach, then our selection of features (section 3), the different tested machine learning models (section 4) and the final post-ranking (section 5). We briefly describe our unsuccessful experiments (section 6) and we conclude by discussing future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Bases</head><p>Principle As most of the successful works for keyphrase extraction, our approach relies on Machine Learning (ML). The following steps are applied to each document to be processed:</p><p>1. Analysis of the structure of the article.</p><p>2. Selection of candidate terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Calculation of features.</head><p>4. Application of a ML model for evaluating each candidate term independently.</p><p>5. Final re-ranking for capturing relationships between the term candidates.</p><p>For creating the ML model, steps 1-3 are applied to the articles of the training set. We view steps 1 and 5 as our main novel contributions. The structure analysis permits the usage of reliable features in relation to the logical composition of the article to be processed. The final re-ranking exploits general relationships between the set of candidates which cannot be captured by the ML models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate term selection</head><p>In the following, word should be understood as similar to token in the sense of MAF 1 .</p><p>Step 2 has been implemented in a standard manner, as follows:</p><p>1. Extract all n-grams up to 5 words, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Structural features</head><p>One of the goals of GROBID is to realize reliable conversions of technical and scientific documents in PDF to fully compliant TEI 3 documents. This conversion implies first the recognition of the different sections of the document, then the extraction of all header metadata and references. The analysis is realized in GROBID with Conditional Random Fields (CRF) <ref type="bibr" target="#b7">(Peng and McCallum, 2004</ref>) exploiting a large amount of training data. We added to this training set a few ACM documents manually annotated and obtained a very high performance for field recognitions, between 97% (section titles, reference titles) and 99% (title, abstract) accuracy for the task's collection. Authors commonly introduce the main concepts of a written communication in the header (title, abstract, table of contents), the introduction, the section titles, the conclusion and the reference list.</p><p>Similarly human readers/annotators typically focus their attention on the same document parts. We introduced thus the following 6 binary features characterizing the position of a term with respect to the document structure for each candidate: present in the title, in the abstract, in the introduction, in at least one section titles, in the conclusion, in at least one reference or book title.</p><p>In addition, we used the following standard feature: the position of the first occurrence, calculated as the number of words which precede the first occurrence of the term divided by the number of words in the document, similarly as, for instance, <ref type="bibr" target="#b9">(Witten et al., 1999)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Content features</head><p>The second set of features used in this work tries to captures distributional properties of a term relatively to the overall textual content of the document where the term appears or the collection.</p><p>Phraseness The phraseness measures the lexical cohesion of a sequence of words in a given document, i.e. the degree to which it can be considered as a phrase. This measure is classically used for term extraction and can rely on different techniques, usually evaluating the ability of a sequence of words to appear as a stable phrase more often than just by chance. We applied here the Generalized Dice Coeficient (GDC) as introduced by <ref type="bibr" target="#b6">(Park et al., 2002)</ref>, applicable to any arbitrary ngram of words (n ≥ 2). For a given term T , | T | being the number of words in T , f req(T ) the frequency of occurrence of T and f req(w i ) the frequency of occurrence of the word w i , we have:</p><formula xml:id="formula_0">GDC(T ) = | T | log 10 (f req(T ))f req(T ) w i ∈T f req(w i )</formula><p>We used a default value for a single word, because, in this case, the association measure is not meaningful as it depends only on the frequency.</p><p>Informativeness The informativeness of a term is the degree to which the term is representative of a document given a collection of documents. Once again many measures can be relevant, and we opt for the standard TF-IDF value which is used in most of the keyphrase extraction systems, see for instance <ref type="bibr" target="#b9">(Witten et al., 1999)</ref> or <ref type="bibr" target="#b4">(Medelyan and Witten, 2008)</ref>. The TF-IDF score for a Term T in document D is given by:</p><formula xml:id="formula_1">TF-IDF(T, D) = f req(T, D) | D | × -log 2 count(T ) N</formula><p>where | D | is the number of words in D, count(T ) is the number of occurrence of the term T in the global corpus, and N is the number of documents in the corpus.</p><p>Keywordness Introduced by <ref type="bibr" target="#b9">(Witten et al., 1999)</ref>, the keywordness reflects the degree to which a term is selected as a keyword. In practice, it is simply the frequency of the keyword in the global corpus. The efficiency of this feature depends, however, on the amount of training data available and the variety of technical domains considered. As the training set of documents for this task is relatively large and narrow in term of technical domains, this feature was relevant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Lexical/Semantic features</head><p>GRISP is a large scale terminological database for technical and scientific domains resulting from the fusion of terminological resources (MeSH, the Gene Ontology, etc.), linguistic resources (part of WordNet) and part of Wikipedia. It has been created for improving patent retrieval and classification <ref type="bibr" target="#b2">(Lopez and Romary, 2010)</ref>. The assumption is that a phrase which has been identified as controlled term in these resources tend to be a more important keyphrase. A binary feature is used to indicate if the term is part of GRISP or not. We use Wikipedia similarly as the Wikipedia keyphraseness in Maui <ref type="bibr" target="#b5">(Medelyan, 2009)</ref>. The Wikipedia keyphraseness of a term T is the probability of an appearance of T in a document being an anchor <ref type="bibr" target="#b5">(Medelyan, 2009)</ref>. We use Wikipedia Miner 4 for obtaining this value.</p><p>Finally we introduced an additional feature commonly used in keyword extraction, the length of the term candidate, i.e. its number of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Machine learning model</head><p>We experimented different ML models: Decision tree (C4.5), Multi-Layer perceptron (MLP) and Support Vector Machine (SVM). In addition, we combined these models with boosting and bagging techniques. We used WEKA <ref type="bibr" target="#b8">(Witten and Frank, 2005)</ref> for all our experiments, except for SVM 4 http://wikipedia-miner.sourceforge.net where LIBSVM <ref type="bibr" target="#b1">(Chang and Lin, 2001)</ref> was used. We failed to obtain reasonable results with SVM. Our hypothesis is that SVM is sensitive to the very large number of negative examples compared to the positive ones and additional techniques should be used for balancing the training data. Results for decision tree and MLP were similar but the latter is approx. 57 times more time-consuming for training. Bagged decision tree appeared to perform constantly better than boosting (+8,4% for F-score). The selected model for the final run was, therefore, bagged decision tree, similarly as, for instance, in <ref type="bibr" target="#b5">(Medelyan, 2009)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Post-ranking</head><p>Post-ranking uses the selected candidates as a whole for improving the results, while in the previous step, each candidate was selected independently from the other. If we have a ranked list of term T 1-N , each having a score s(T i ), the new score s ′ for the term T i is obtained as follow:</p><formula xml:id="formula_2">s ′ (T i ) = s(T i ) + α -1 j =i P (T j |T i )s(T j )</formula><p>where α is a constant in [0 -1] for controlling the re-ranking factor. α has been set experimentally to 0.8. P (T j |T i ) is the probability that the keyword T j is chosen by the author when the keyword T i has been selected. For obtaining these probabilities, we use statistics for the HAL<ref type="foot" target="#foot_0">5</ref> research archive. HAL contains approx. 139,000 full texts articles described by a rich set of metadata, often including author's keywords. We use the keywords appearing in English and in the Computer Science domain (a subset of 29,000 articles), corresponding to a total of 16,412 different keywords. No smoothing was used. The usage of open publication repository as a research resource is in its infancy and very promising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Our system was ranked first of the competition among 19 participants. 7 What did not work</p><p>The previously described features were selected because they all had a positive impact on the extraction accuracy based on our experiments on the task's collection. The following intuitively pertinent ideas appeared, however, to deteriorate or to be neutral for the results.</p><p>Noun phrase filtering We applied a filtering of noun phrases based on a POS tagging and extraction of all possible NP based on typical patterns. This filtering lowered both the recall and the precision (-7.6% for F-score at top 15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Term variants</head><p>We tried to apply a post-ranking by conflating term variants using FASTR 6 , resulting in a disappointing -11.5% for the F-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global keywordness</head><p>We evaluated the keywordness using also the overall HAL keyword frequencies rather than only the training corpus. It had no impact on the results.</p><p>Language Model deviation We experimented the usage of HMM deviation using LingPipe 7 as alternative informativeness measure, resulting in -3.7% for the F-score at top 15.</p><p>Wikipedia term Relatedness Using Wikipedia Miner, we tried to apply as post-ranking a boosting of related terms, but saw no impact on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future work</head><p>We think that automatic key term extraction can be highly valuable for assisting self-archiving of research papers by authors in scholarly repositories such as arXiv or HAL. We plan to experiment keyword suggestions in HAL based on the present system. Many archived research papers are currently not associated with any keyword. We also plan to adapt our module to a large collection of approx. 2.6 million patent documents in 6 http://perso.limsi.fr/jacquemi/FASTR 7 http://alias-i.com/lingpipe the context of CLEF IP 2010. This will be the opportunity to evaluate the relevance of the extracted key terms for large scale topic-based IR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Table1presents our official results (Precision, Recall, F-score) for combined keywords and reader keywords, together with the scores of the systems ranked second (WINGNUS and KX FBK). Performance of our system (HUMB) and of the systems ranked second.</figDesc><table><row><cell>Set</cell><cell>System</cell><cell>top 5</cell><cell>top 10</cell><cell>top 15</cell></row><row><cell cols="2">Comb. HUMB</cell><cell cols="3">P:39.0 R:13.3 F:19.8 F:32.0 R:21.8 F:25.9 P:27.2 R:27.8 F:27.5</cell></row><row><cell></cell><cell cols="4">WINGNUS P:40.2 R:13.7 F:20.5 P:30.5 R:20.8 F:24.7 P:24.9 R:25.5 F:25.2</cell></row><row><cell cols="2">Reader HUMB</cell><cell cols="3">P:30.4 R:12.6 F:17.8 P:24.8 R:20.6 F:22.5 P:21.2 R:26.4 F:23.5</cell></row><row><cell></cell><cell>KX FBK</cell><cell cols="3">P:29.2 R:12.1 F:17.1 P:23.2 R:19.3 F:21.1 P:20.3 R:25.3 F:22.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>HAL (Hyper Article en Ligne) is the French Institutional repository for research publications: http://hal.archivesouvertes.fr/index.php?langue=en</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pointer project final report</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Collingham</surname></persName>
		</author>
		<ptr target="http://www.computing.surrey.ac.uk/ai/pointer/report" />
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Surrey</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GRISP: A Massive Multilingual Terminological Database for Scientific and Technical Domains</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh international conference on Language Resources and Evaluation (LREC)</title>
		<meeting><address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECDL 2009, 13th European Conference on Digital Library</title>
		<meeting>ECDL 2009, 13th European Conference on Digital Library<address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domainindependent automatic keyphrase indexing with small training sets</title>
		<author>
			<persName><forename type="first">O</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1026" to="1040" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Medelyan</surname></persName>
		</author>
		<title level="m">Human-competitive automatic topic indexing</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic glossary extraction: beyond terminology identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Boguraev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on Computational linguistics</title>
		<meeting>the 19th international conference on Computational linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate information extraction from research papers using conditional random fields</title>
		<author>
			<persName><forename type="first">F</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>San Francisco, 2</pubPlace>
		</imprint>
	</monogr>
	<note>nd edition edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">KEA: Practical automatic keyphrase extraction</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Paynter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Nevill-Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM conference on Digital libraries</title>
		<meeting>the fourth ACM conference on Digital libraries</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">255</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
