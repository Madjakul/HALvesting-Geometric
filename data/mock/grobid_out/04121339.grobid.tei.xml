<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale Machine-Learning analysis of scientific PDF for monitoring the production and the openness of research data and software in France</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aricia</forename><surname>Bassinet</surname></persName>
							<idno type="ORCID">0000-0003-1723-7517</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Lorraine</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laetitia</forename><surname>Bracco</surname></persName>
							<idno type="ORCID">0000-0002-9959-9441</idno>
							<affiliation key="aff0">
								<orgName type="institution">University of Lorraine</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anne</forename><surname>L'hôte</surname></persName>
							<idno type="ORCID">0000-0002-0756-0508</idno>
							<affiliation key="aff1">
								<orgName type="institution">French Ministry of Higher Education and Research</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Jeangirard</surname></persName>
							<idno type="ORCID">0000-0002-3767-7125</idno>
							<affiliation key="aff1">
								<orgName type="institution">French Ministry of Higher Education and Research</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Patrice</forename><surname>Lopez</surname></persName>
							<email>patrice.lopez@science-miner.com</email>
							<idno type="ORCID">0000-0002-9959-9441</idno>
							<affiliation key="aff2">
								<orgName type="institution">science-miner</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<idno type="ORCID">0000-0002-0756-0508</idno>
							<affiliation key="aff3">
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-scale Machine-Learning analysis of scientific PDF for monitoring the production and the openness of research data and software in France</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7FCA5A734C709EBFE98EB9E4CB1C349C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>research data</term>
					<term>research software</term>
					<term>open access</term>
					<term>open science</term>
					<term>scientometrics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is today no standard way to reference research datasets and software in scientific communication. Emerging editorial workflows and supporting infrastructures dedicated to research datasets and software are still poorly adopted in current publishing practices and are highly fragmented. To better follow the production of research datasets and software, we present a text mining method applied to scientific publications at scale and implemented at the French national level. Our approach relies on state-of-the-art Machine Learning and document engineering techniques to ensure reliable accuracy across multiple research areas and document types. The annotations produced by our system are used by the French Open Science Monitor (BSO) platform to follow the production and the openness of research datasets and software in the context of the French second National Plan for Open Science. The source code and the data of the French Open Science Monitor, as well as all the associated tools and training data, are all available under open licenses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction 1.Motivations</head><p>Research datasets and software are today core elements of the research activities. 90-95% of researchers in the US and the UK rely upon software, and more than 60% would be unable to continue working if such software stopped functioning <ref type="bibr" target="#b43">(Philippe et al., 2019)</ref>.</p><p>Nearly half of the researchers commonly use data generated by other scientists (Science staff, 2011) and the vast majority of researchers support data sharing <ref type="bibr" target="#b53">(Tenopir et al., 2015)</ref>.</p><p>The critical role of research data and software is today broadly acknowledged, in particular for improving the reuse, the reproducibility and the transparency of research results <ref type="bibr" target="#b32">(Laurinavichyute et al., 2022)</ref>.</p><p>Following several recommendations such as the San Francisco Declaration on Research Assessment, pro-active policies to enforce higher standards of openness and visibility for all research results have been introduced in the last years. Significant national open science policies are currently implemented. Recently, the U.S. OSTP (White House Office of Science and Technology Policy) launched the Year of Open Science to advance national open science policies across the federal US government in 2023 <ref type="bibr" target="#b20">(House, 2023;</ref><ref type="bibr" target="#b40">Nelson, 2022)</ref>. This new policy introduces in particular a mandate for free, immediate public access to government-funded research to take effect by the end of 2025, including research datasets, and the development of key performance indicators.</p><p>Another example is the French National Plan for Open Science, a long-term, forerunner effort supported by 5 million euros annual budget in 2018-2021 to promote every aspects of Open Science in France. Under this framework, a public dashboard to follow the Openness of scientific publication with key performance indicators has been developed and deployed already since 2019, called the French Open Science Monitor (BSO -Baromètre de la Science Ouverte). This plan was extended in a second phase in 2022-2024 with an 1 increased budget to 15 million euros per year, giving additional attention to the openness of research datasets and software.</p><p>To evaluate, adapt and maximize the adoption of these policies, their effects must be measured. Monitoring tools and dashboards are crucial to follow the evolution of practice regarding openness and sharing of research datasets and software. However, in contrast to the well-established practice of citing scholarly publications, the visibility of these research products is considered largely insufficient and challenging. <ref type="bibr" target="#b41">(Park et al., 2018)</ref> shows that most data citations are informal (mentions without crossreference with a bibliographical section and without identifiers) and found mostly in footnotes, acknowledgements or supplementary materials sections of research articles. Similarly, software are not cited in scholarly publications in a consistent and easily readable manner <ref type="bibr" target="#b21">(Howison &amp; Bullard, 2016)</ref>. Multiple initiatives took place in the last decade to address this issue, in particular focusing on improving research datasets and software cataloging <ref type="bibr" target="#b11">(Elger et al., 2016;</ref><ref type="bibr" target="#b15">Garijo et al., 2019)</ref>, standards for data citation and software citation <ref type="bibr" target="#b27">(Katz et al., 2021;</ref><ref type="bibr" target="#b51">Silvello, 2018)</ref> and advocacy efforts. The main advocacy effort regarding dataset and software identification is currently focusing on using PIDs similar to the successful Crossref DOIs for the research publications.</p><p>The usage of PID, combined with PID authority agencies and infrastructures, offers indeed a comprehensive technical solution for the identification and the citation of any research entities in scholar communication <ref type="bibr" target="#b6">(Cousijn et al., 2019;</ref><ref type="bibr" target="#b26">Juty et al., 2020)</ref>. However, <ref type="bibr" target="#b9">(Du et al., 2022)</ref> failed to observe any usages of PID associated to software when they exist in random samples of software mentions from recent articles on COVID-19. Regarding dataset citations, <ref type="bibr" target="#b17">(He &amp; Han, 2017)</ref> report that less than 10% of publications that include data mentions contain any PID. According to the study of <ref type="bibr" target="#b36">(Macgregor et al., 2022)</ref>, PID associated to data have a much more limited awareness among researchers as compared to publication PID. With their current adoption and impact, PID cannot provide today realistic measurements of the usage, creation and sharing of research data and software.</p><p>One immediately available alternative to PIDs for datasets and software is to rely on a text mining approach to automatically detect their mentions in scholarly full texts and their role in the described research work. If the corpus of scientific full texts is comprehensive enough, this approach can provide a realistic snapshot of the actual practices regarding research datasets and software at a given time. Such continuous source of data could then lead to trustful indicators for monitoring the impact of open science policies with limited latency.</p><p>For extending the French Open Science Monitor (BSO) platform <ref type="bibr" target="#b1">(Bracco et al., 2022)</ref> to measure the openness of research datasets and software, we implemented a large text mining pipeline applied to a comprehensive corpus of scientific publications including at least one author with a French affiliation. After describing how we created this corpus and harvested full texts, we will show that recent advances in scientific text mining make finegrained extraction approaches effective. Our models capture reliably research datasets and software mentions directly from the full text publications, but also characterize these mentions in term of usage, creation and sharing. Finally we will present the indicators developed from these text mining results and how they are integrated in the French Open Science Monitor infrastructure and dashboards, offering at the same time country-wide measurements and custom monitors at the level of an institution or a scientific field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">The French Open Science Monitor</head><p>The French Open Science Monitor 1 , also called BSO for Baromètre de la Science Ouverte, is a tool for steering the public policy introduced by the first French National Plan for Open Science <ref type="bibr" target="#b38">(MESR, 2018)</ref>. A first version was released in 2019 providing a collection of measurements and dashboards on the rate of Open Access publications produced by 1 https://frenchopensciencemonitor.esr.gouv.fr all public French research entities <ref type="bibr" target="#b25">(Jeangirard, 2019)</ref>. In 2021, the new version of BSO was extended to better encompass the field of health, with additional information on the openness of clinical trials and observational studies.</p><p>Following efforts initiated by the University of Lorraine in 2020, the national monitor also offers tools and training materials to support the publication by individual institutions of their own Open Science monitor from the national data.</p><p>While opening scientific publications is a crucial aspect of Open Science, other research outputs need to be considered. The follow-up second Plan for Open Science <ref type="bibr" target="#b38">(MESR, 2021)</ref>, which started in 2022, includes a particular focus on research datasets and software, supporting numerous projects and initiatives to broaden their openness. Although the French Open Science Monitor is updated every year <ref type="bibr" target="#b1">(Bracco et al., 2022)</ref>, measurements related to research datasets and software were not covered yet and required specific indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Quality criteria for Open Science indicators</head><p>Before discussing existing works and describing our approach, we introduce the main quality criteria that we think should be considered when designing open science indicators. We also highlight some of the main challenges associated to these quality criteria when applied to research datasets and research software.</p><p>• Coverage: Ideally the indicators should cover all the research outputs of interest in the context of Open Science policies. This is very challenging for research datasets and software, because, to a large extend, they are not identified nor indexed as traditional scholarly articles. A high coverage is however mandatory to create reliable estimates that are inclusive for all scientific fields and all types of institutions.</p><p>• Accuracy: Indicators should be reliable, in particular avoiding false positive and duplicates. This criteria supposes thorough and reproducible evaluations in term of standard accuracy metrics (e.g. precision, recall, F1-score) and to rely on reliable authoritative sources. • Fairness: Indicators should maintain consistency in terms of domains and languages. As much as possible, we want to avoid exclusion of some research areas and languages. This aspect is challenging for example with Social Sciences and Humanities, where publications are more incompletely referenced by large bibliographical index, and frequently published in languages other than English.</p><p>• Understandability and interpretability: Indicators should present measurements easy to understand for researchers and for the public. For example, if expressed as a percentage, the indicator maximum value (100%) should be clear and correspond directly to a goal of the evaluated public policy.</p><p>• Consistency maintained over time: Indicators produced for a given year must be directly comparable with the indicators from previous years, in order to follow correctly the evaluation of research activity over time. The consistency should be valid in terms of measurement methodology, corpus, and presentation.</p><p>• Independence and trustfulness for the researchers: to maximize trust by researchers, public indicators should preferably be independent from proprietary resources and interests. Open source, open data and open access documentation are therefore to be prioritized. In addition, we think methods and data sources for creating the indicators should be fully documented to meet this criterion. Relatively to datasets, if we focus on France, this observatory references only 958 datasets affiliated to an organization in the country for the period 2014-2023, with 373 reported as Open Access (as of May 2023). It also reports 16,863 Open Access datasets in the country's repositories (for reference, there are a total of 42M item registered on DataCite worldwide). Similarly, 533 software are affiliated to an organization in France, with 111 identified as Open source. As visible, the coverage is extremely limited. Coverage of research domains is similarly incomplete and not consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Existing Open</head><p>The method for generating the indicators is currenly not documented, beyond an indication that it relies on the OpenAire Research graph. This includes a "PID graph" aggregating open metadata databases such as CrossRef, PubMed or arXiv, but we were not able to find additional information about the involved "intelligent linking". We assume dataset and software metadata sources mainly correspond to those implementing manual references via PID with sufficient metadata, such as parsed affiliations. As such, we think the OpenAire Observatory illustrates the limit of an approach relaying on PID and on manual referencing for research datasets and software. All in all, it lacks transparency and might be more a proof of concept than a usable service.</p><p>As a result, such fragmentary coverage leads to biased and unreliable indicators. Under these conditions, we think that the published dashboards are counter-productive. Publishing aggregated dashboards on so little and non-representative data can lead to the disengagement of the public for the tool, false interpretation, wrong public policy decisions and inability to assess public the application of Open Science policies. The PLOS method relies on DataSeer text mining tools <ref type="bibr">("DataSeer project", 2019</ref><ref type="bibr">("DataSeer project", -2023))</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2">PLOS Open Science indicators</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In</head><p>As compared to the OpenAire dashboards, the PLOS Open Science indicators correspond to a significantly higher quality considering the criteria we introduced in section 1.3. The corpus is well delimited and the indicators are regularly updated. The text mining process is comprehensive because applied to all the publications. The indicators can be adapted by geographical areas and institutions. The relative ratios over time are easy to interpret and directly capture the relevant output for PLOS Open Science policy.</p><p>However, in terms of scale, diversity and complexity, the PLOS collection is narrow in comparison with the national ambition of the BSO. These indicators only cover one publisher and are mostly limited to the Life Science domain. In addition, the extraction process took advantage of the Data Availability statements exceptionally present in all PLOS publications over the considered period, and of the existing JATS XML format, which is easier to process than PDF. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Identification of countrywide research publications</head><p>We emphasized in section 1.3 the importance of the coverage and freshness criteria to produced meaningful indicators. The examples of the OpenAire and PLOS indicators illustrate the challenge of addressing these criteria.</p><p>In this section, we explain how the corpus of French research publication is created. Identifying in a reliable and comprehensive manner the global set of French research publications is the mandatory basis for developing targeted indicators. A detailed presentation of the approach is given in <ref type="bibr" target="#b1">(Bracco et al., 2022)</ref>.</p><p>In 2018 and 2019, the following two principles have been established when launching the French Open Science Monitor project:</p><p>1. The French research publication corpus is defined as all the publications with at least one author with a French affiliation.</p><p>2. The French Open Science Monitor is a sovereign tool to steer public policies, it is independent from proprietary database and providers, therefore the project only uses open data for creating the reference set of French research publication.</p><p>The first principle supposes to be able to access the affiliation information of every authors for all the publications. However, open bibliographic databases such as Crossref and PubMed lack metadata related to publication country and affiliations. To solve this limitation without relying on proprietary data such as Web of Science (WoS) and Scopus, the French Open Science Monitor has developed an original data ingestion and extraction pipeline running every year <ref type="bibr" target="#b1">(Bracco et al., 2022)</ref>:</p><p>1. Start with all CrossRef DOI for a given publication time range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Collect additional open metadata relative to each DOI, from several sources:</p><p>• harvest at scale and parse every HTML landing pages associated to a DOI to extract additional affiliation raw strings,</p><p>• harvest repositories metadata (PubMed, HAL),</p><p>• when the full-text is available, use GROBID <ref type="bibr">("GROBID", 2008</ref><ref type="bibr">("GROBID", -2023) )</ref> to extract affiliations metadata. This approach appears to be highly competitive and reliable when compared to proprietary data such as WoS and Scopus, while relying only on transparent open data sources.</p><p>Studying French publications in 2019, <ref type="bibr" target="#b3">(Chaignon &amp; Egret, 2022)</ref> shows that the open-data method implemented by the French Open Science Monitor (BSO) is able to identify the largest share of French publications as compared to the combined aggregated corpus of Scopus, Web of Science (WoS), HAL archive, the Astrophysics Data System Abstract Service (ADS), PubMed and Microsoft Academic Graph (MAG). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Full text harvesting</head><p>Once the bibliographical set of French publications has been created, the next step is to harvest the largest possible amount of full texts to apply text mining.</p><p>In the case of Open Access publications, we identify one or more possible download URL using Unpaywall, which is broadly acknowledged as the most advanced database for this purpose <ref type="bibr" target="#b12">(Else, 2018)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Machine Learning for mention detection and characterization</head><p>As discussed in the section 1.1, PID and metadata driven approach related to research datasets and software cannot lead currently to realistic evaluations and indicators due to low adoptions and lack of awareness. In contrast, automatic recognition of these mentions from full texts potentially offers a factual and comprehensive approach, directly usable to estimate quantitatively these research outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Advantages of text mining publications</head><p>We think that mining research datasets and software mentions in scientific publications can provide solutions for most of the quality criteria for indicators introduced in section 1.3:</p><p>• In terms of coverage and freshness, a corpus of scientific publications can offer a trustful snapshot of the scientific production if the text mining is applied to a very significant amount of scientific publications. First the ratio of Open Access publications has now reached more than 50% of the all publications. Second, copyright exception for text mining for subscription-based publications makes it possible to harvest legally a large corpus of closed-access publications. Accessing a comprehensive corpus close to completeness is thus realistic.</p><p>• When enough training data is available, the accuracy of modern machine learning techniques, in particular based on a Deep Learning architecture, has improved significantly. We will see that large scale datasets of manually annotated research datasets and software mentions have been recently released, and we can expect reaching a satisfactory level of accuracy;</p><p>• The adaptability to different geographical and organizational levels and different scientific and technical domains have already been addressed with high reliability at the level of publications in the previous version of the French Open Access Monitor <ref type="bibr" target="#b1">(Bracco et al., 2022)</ref>.</p><p>• With respect to fairness, the systematic application of text mining on a comprehensive corpus can cover research domains where awareness of metadata and PID referencing is very low, because online access to full-texts publications is today a universal practice, avoiding their exclusions.</p><p>• Consistency: an automated text mining solution can be re-applied to a full corpus regularly, including back files, and produce consistent indicators over any period. The process is independent from a manual referencing of research datasets and software that could happen to already published articles, certain research fields, from certain publishers and is applied to all types of publications.</p><p>• Independence and trustfulness: we think that this criteria can be fulfilled if the software used to obtain these indicators are open source, publicly documented and transparently evaluated.</p><p>The first text-mining approaches for capturing information on data and software have been rule-based techniques. We discuss in the next section their different limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Limitation of rule-based tools</head><p>In the context of Open Science and bibliometrics, some pattern matching tools have been developed to capture statements about data and software openness in a given corpus. For example, <ref type="bibr" target="#b31">(Larregue et al., 2020)</ref> uses keyword matching applied to the data availability statements and obtained an approximation of the data sharing status for 7,394 COVID-19 articles deposited on medRxiv. However, data sharing statements are, to a very large extend, not limited to data availability statements <ref type="bibr" target="#b41">(Park et al., 2018)</ref>, COVID-19 articles are not representative of general data sharing practices <ref type="bibr" target="#b50">(Sevryugina &amp; Dicks, 2022</ref>) and keyword matching is far from a state of the art automation technique. Such a work can be seen as a one shot exercise for reporting interesting results in a limited scope, but not as a method that can be generalized and re-used for mining data usage in full texts.</p><p>A more general purpose tool, ODDPub <ref type="bibr" target="#b45">(Riedel et al., 2020)</ref> • ODDPub produces information about openness statements related to data and software used in the work described in a publication. The tool does not produce more fine-grained information about their re-use, production and sharing. The global volume of novel data and code is unknown. This makes impossible to estimate the key ratio of created data and software openly shared, which is the measurement that Open Science policies need to monitor and maximize.</p><p>• Rule-based approaches are useful when no training data is available or when transparency for auditing error is relevant, but they are technically outdated. They perform with lower accuracy and portability to other domains that modern machine learning when quality training data is available <ref type="bibr" target="#b4">(Chiticariu et al., 2010;</ref><ref type="bibr" target="#b54">Trienes et al., 2020)</ref>.</p><p>We will see that such quality training data exists today for software and dataset mentions.</p><p>• Reported ODDPub F1-scores are 0.73 for data openness recognition and 0.64 for code openness recognition at document level. As indicated in <ref type="bibr" target="#b30">(Lafia et al., 2022)</ref>, performance metrics for the open code detection are unreliable because of a lack of data for significant evaluation (open code in the evaluation data was found in only 11 publications<ref type="foot" target="#foot_1">3</ref> ). ODDPub was designed to be part of a workflow including a manual validation step. In this context, accuracy of the automated step can be balanced to ensure an expected quality level. However, for a fully automated text mining at scale, the reported accuracy values are more challenging.</p><p>• ODDPub often has no clear distinction between code and data -including in the evaluation data of the tool. Data present on a GitHub repository for example will often be classified as open software.</p><p>• ODDPub produces global screening information about openness at document level, but the tool does not have the technical capacity to identify mentions (dataset and software names), nor to provide usable information at software and dataset levels (dataset and software attributes and mention contexts). We think that the level of complexity of the rules required to extract mentions and mention-level characterizations would make the approach very hard and time-consuming to extend to such recognition, if doable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Machine Learning for mention detection and characterization</head><p>The automatic recognition approach raises crucial challenges that are necessary to tackle and evaluate rigorously for a valid application:</p><p>• processing PDF as input for ensuring coverage, freshness and fairness,</p><p>• accuracy and sparsity to address accuracy and trustfulness,</p><p>• robustness, speed and production-level technical capacities for supporting freshness, consistency and accuracy requirements.</p><p>We discuss these three points in the next sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Processing PDF as input</head><p>To perform text mining on scientific articles, we cannot assume the availability of clean and structured text. The most widespread and easily available scientific publication format is raw PDF, a presentation-oriented format that destroys the semantics and the original structure of data, introducing noise in text encoding and text order stream. This format raises issues for text mining applications, both in terms of significant source of errors and technical feasibility <ref type="bibr" target="#b56">(Westergaard et al., 2017)</ref>. <ref type="foot" target="#foot_2">4</ref>Often presented as an alternative, publisher structured XML including the text body is text-mining friendly, but they have limited availability. First, XML are costly to produce, many small, medium and academic publishers do not have the resources today to produce formats beyond PDF. Second, recent publications are critical for developing Open Science indicators, but the availability of XML is delayed and uncertain as compared to PDF (e.g. for conference proceedings or preprints initially in PDF). Third, accessing XML versions often require additional subscription-based API services even when the Open Source PDF version is available. Last, processing XML only would create a strong dependency on publishers. Even when available, XML full-texts are in a variety of different native publisher XML formats <ref type="bibr" target="#b2">(Bretel et al., 2010)</ref>, often incomplete and inconsistent from one to another, involving the development of many custom parsers when considered at scale. Thus, supporting PDF by using a layout-aware parsing and conversion tool appeared very early as a key requirement for any scalable scientific text mining task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Accuracy and sparsity</head><p>Accuracy of the recognition of software and dataset mentions from within scholar full texts is challenging, firstly due to the high sparsity of these mentions. This work requires the application of state-of-the-art ML methods to millions of published PDFs across different scientific domains, where dataset and software mentions only represent a few relevant tokens out of several thousands in each document.</p><p>Considering the Softcite dataset version 1.0 <ref type="bibr" target="#b8">(Du et al., 2021)</ref>, the 4,971 full-texts contain a total of around 46 million tokens, but only 15,280 tokens are relevant as a software mention.</p><p>Around one token is positively labeled for each 3,000 "negative" tokens, with a ratio as low as one token per 17,500 tokens for publishers and URL fields. An Imbalance Ratio value above 500 is usually already considered to be extreme <ref type="bibr" target="#b33">(Lee &amp; Deleris, 2020)</ref>. With the higher observed Imbalance Ratio here, from 1:3,000 to 1:17,500, an ML approach to finding new unseen software mentions can be very challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Robustness, speed and production-level capacities</head><p>Given the technical environment of the French Ministry of Higher Education and Research, which is operating the French Open Science Monitor, and the volume of full texts downloaded reported in section 3, the following technical constraints needed to be covered:</p><p>• scaling to at least 1 million PDF in manageable time and standard computing infrastructure,</p><p>• secure storage for harvested full texts,</p><p>• repeatable processing every year for the complete corpus,</p><p>• high robustness to avoid service failure, in particular in the common case of illformed and corrupted PDF,</p><p>• fully automated process for deployment on a Kubernetes cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Research software</head><p>Automatic recognition of software mentions has attracted a lot of interest in parallel with the development of research software citation advocacy in the last decade. <ref type="bibr" target="#b29">(Krüger &amp; Schindler, 2020)</ref> presents these first approaches, based mainly on gazetteers and rules.</p><p>Machine Learning promises significantly higher accuracy and coverage, but appeared however first limited by the lack of manually annotated training data necessary for reliable models -the largest public dataset until 2020 being limited to only 85 annotated documents <ref type="bibr" target="#b10">(Duck et al., 2015)</ref>. With the development of large annotated gold corpus, the Softcite dataset <ref type="bibr" target="#b8">(Du et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Prior work</head><p>To our knowledge, three systems have used modern Deep Learning techniques and the recent large annotated gold corpora for software mention recognition. <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref>, <ref type="bibr" target="#b47">(Schindler et al., 2022)</ref> and <ref type="bibr" target="#b23">(Istrate et al., 2022)</ref>. All are using fine-tuned SciBERT models <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref>.</p><p>The Softcite software mention recognizer <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref> is the earliest published The SoMeSci recognizer <ref type="bibr" target="#b47">(Schindler et al., 2022)</ref> is trained with the SoMeSci dataset <ref type="bibr" target="#b46">(Schindler et al., 2021)</ref>, which is smaller and limited to Life Sciences, but has a more comprehensive set of annotations than <ref type="bibr" target="#b8">(Du et al., 2021)</ref>, including relationships between mentioned software entities. The model uses a more complex architecture to also predict these relationships. Training and evaluation are realized with the annotated corpus, which only includes the positive sentences (sentences with at least one software mention) for 787 PLoS articles, method sections for 480 PLoS articles (a total of around 3,200 paragraphs) and complete content for 100 articles. The distribution of software mentions is thus oversampled as compared to the actual distribution. The system supports JATS XML documents as input.</p><p>The CZI recognizer <ref type="bibr" target="#b23">(Istrate et al., 2022)</ref> is the latest published system, but also the simplest, with a more limited scope. It is a SciBERT model fine-tuned with the Softcite dataset <ref type="bibr" target="#b8">(Du et al., 2021)</ref>. The trained model used software name and version information and does not cover the other annotated attributes available in the dataset (url, publisher). The model is only trained and evaluated on the annotated examples of the dataset, which are all positive paragraphs (the paragraphs with at least one software mention). The model can then be used in a pipeline starting from XML documents as input, relying on GNU parallel command lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Comparing model predictions with real mention destributions</head><p>The accuracy and sparsity aspect is important to stress when comparing these tools. While software mentions are extremely sparse in actual scholarly full texts, they are represented in either every paragraph <ref type="bibr" target="#b23">(Istrate et al., 2022)</ref> or in around one third of the paragraphs <ref type="bibr" target="#b47">(Schindler et al., 2022)</ref> of their training data. According to our evaluations, this leads to model unrealistic ratio of software mentions. These models tend to predict software mentions in many sentences/paragraphs, leading to a very high amount of false positives.</p><p>Because these models also report evaluation scores produced on partitions of the annotated data, the reported evaluations present similarly unrealistic figures, over-representing expected software mentions. The invalid evaluation of a model for an unbalanced classification problem after sampling the training data is a known issue discussed in <ref type="bibr" target="#b55">(Vandewiele et al., 2021)</ref>. This shows that a parser like <ref type="bibr" target="#b23">(Istrate et al., 2022)</ref> trained on unrealistic over-sampled distributions will perform with much lower accuracy when applied to real full documents, producing a high rate of false positive. For this reason, in <ref type="bibr" target="#b23">(Istrate et al., 2022)</ref>, manual corrections have been realized at very large scale on the extracted results.</p><p>We think that a similar observation applies to <ref type="bibr" target="#b47">(Schindler et al., 2022)</ref>. The recognizer is trained and evaluated on the SoMeSci corpus. This corpus is over-representing software mentions due its construction. However, differences in the definitions of "software mentions" between the SoMeSci and the Softcite datasets challenge the validity of this evaluation. False positives can be the consequence of the broader definition of "software mentions" in SoMeSci as compared to Softcite, and not only due to very high ratio of software mentions in the training data of the SoMeSci recognizer.</p><p>To address false positives, <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref> has explored several sampling ratios and techniques to adapt the model to the actual extremely sparse distribution of mentions.</p><p>With F1-score around 80% on actual mention distribution and full article content, the results do not require manual corrections for tasks which are robust enough to cope with some uncertainty, like statistical analysis, indicators or knowledge base construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Production considerations</head><p>The Softcite software mention recognizer <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref> has the advantage of being integrated with GROBID to allow the processing of PDF documents, while the other tools work with XML full text inputs and would require further development to support PDF parsing and structuring.</p><p>GROBID is used for parsing, extracting, and structuring the content of scientific articles in PDF, but also to drive entity recognition in relevant sections. This tool provides in particular clean text paragraphs, solving issues like character encoding, character composition, hyphenation, reading order, identification of reference markers, footnotes, headnotes, tables and figures, which improve any subsequent text mining process. In addition, GROBID parses bibliographical references, match them to CrossRef DOI and identifies citation contexts in the text body associated to dataset and citation mentions.</p><p>The Softcite software mention recognizer includes production-ready releases, with docker images, REST API service and a python client for parallel processing. It is thus ready to deploy in the existing cloud-based BSO infrastructure and service pipeline. The system also includes a web GUI console for the service that display annotations on PDF for easy visual inspection.</p><p>We have therefore chosen to extend the system of <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref> for the production of the French Open Science indicators, considering its accuracy, the support of PDF and its engineering maturity. Improvements to the system developed in this work includes: a new extended and enriched Softcite corpus, refinement of software types and relationships, automatic characterization of mention context in term of usage/creation/sharing and some improvement of models using LinkBERT fine-tuning <ref type="bibr" target="#b57">(Yasunaga et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Data model and training data</head><p>Definition A "software" entity can be defined as a collection of computer programs that provides the instructions for telling a computer what to do and how to do it. In the present work, we mostly reused existing annotation guidelines and tried to follow consensus on the definition of "software". We consider as software: programs, packages, scripts, plug-ins, workflow scripts, API, OS, programming environments, macros and embedded software.</p><p>We exclude databases, models (simulation, machine learning), algorithms/methods, programming languages and file formats.</p><p>Mentions are however often ambiguous. For example, it is quite frequent that the name of an algorithm and its implementation (as software) are used in papers in an interchangeable manner. The mention to all these entities should be interpreted in context, checking if the statement refers to the software implementation of an algorithm. Similarly, mentions to devices, databases, models, etc. require to check in context if the mention actually refers to the software part of these entities or not.</p><p>For a more detailed discussion and more examples, see the Annotation Guidelines coming with the public dataset <ref type="bibr" target="#b22">(Howison et al., 2023)</ref>.</p><p>A software mention contains at least a software substantive, which could be a proper name or an implicit name like script, program, etc., and optional attributes. In our framework, we considered as attributes: URL, publisher (organization or individual which publishes the software), version, programming language and one or several bibliographical references.</p><p>In order to characterize software contributions in research papers, we need to identify precisely the created software parts. For example when a script is developed and should be run in a particular environment, we distinguish the script as one sharable software component distinct from its software environment. For this, we introduce a typing for software mention:</p><p>• standalone software: a software expressed in the mention context without dependency to another software and which does not require code.</p><p>• software environment: a software requiring some code/scripts to realize the research task, expressed in the mention context with or without dependency to another software.</p><p>• named software component: a named software depending on another software environment to run, the software environment being expressed in the mention context.</p><p>• implicit software component: an unnamed software depending on another soft-ware environment to run, the software environment being expressed in the mention context, and where the software refering expression is a generic term for program.</p><p>such as program, code, script, macro, package, library, etc.</p><p>Annotations The Softcite dataset is encoded in TEI XML <ref type="bibr">(TEI Consortium, 2010)</ref>. In its first version, software mentions were all encoded as standalone software. To produce indicators on software and code sharing associated to research works, we extended the annotations for refining the type of the software mentions and to encode the dependencies between mentioned software. With the additional dependency annotations between software, the new Softcite dataset version is comparable to SoMeSci in term of annotation comprehensiveness, with the benefit of a larger amount of annotated documents and a coverage not limited to Life Science. Like the previous versions of the Softcite dataset, the additional annotations have been produced in parallel by 2 annotators and a reconciliation phase has been realized by a curator for all cases of disagreements.</p><p>Table <ref type="table" target="#tab_5">4</ref> presents some statistics about this new revised version (2.0) of the Softcite dataset.</p><p>A significant amount of software mentions have been broken down into software parts to encode their relationships. The latest version of the Softcite dataset is available on Zenodo <ref type="bibr" target="#b22">(Howison et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5">Evaluation</head><p>The following tables present the evaluation of the two main models involved in the identification of software mentions, based on version 0.7.2 of the Softcite software mention recognizer and the updated version 2.0 of the Softcite dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software mentions and attributes</head><p>Similarly to the work in <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref>, to evaluate the model on the actual distribution of software mentions in scientific articles, we have reused a holdout set containing the complete full text of 20% the Softcite Dataset documents (994 articles). This set reproduces the overall distribution of documents with annotation (29.0%), the distribution between Biomedicine and Economics fields, and the overall distribution of mentions per document. The holdout set therefore captures a realistic distribution of software mentions and can be used to produce stable evaluations using different version of the training data.</p><p>We used the remaining 80% of documents (3,977 articles), divided at paragraph-level into positive (1,886 paragraphs with at least one manual annotation) and negative (612,597 paragraphs without manual annotations). Optimizing the balance between positive and negative sampling, the final model was trained on the 1,886 positive paragraphs of the Softcite dataset and a set a of 50,000 negative paragraphs selected with active sampling as described in <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref>.</p><p>Our best model is a fine-tuned SciBERT base model <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref> with an additional CRF activation layer, which performs slightly but consistently better for this task than a fine-tuned LinkBERT model <ref type="bibr" target="#b57">(Yasunaga et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software type refinement</head><p>We report the evaluation of our best model for the typing of software mentions, once again We also mention the DataSeer project ("DataSeer project", 2019-2023), funded by the Sloan Foundation, which focused on a sentence-level classification task related to data and data type prediction. Although the dataset and models of the project were not made publicly available, the development was entirely realized in Open Source.</p><p>To our knowledge there is currently no dataset and work with a broad coverage in terms of scientific domains. The exiting works usually ignore unnamed mention of data (with the exception of ODDPub and DataSeer). Although almost all are based upon a fine-tuned BERT, each recognizer greatly depends on domains, definition of datasets and on the distribution of dataset mentions in the training data. These differences make the reported accuracy values impossible to compare, as the distribution of mentions in the evaluation data varies considerably, being not realistic in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Data model and training data</head><p>Dataset mentions are usually less complex than software mentions, but the exact scope of "research datasets" varies significantly from one existing work to another and needs some clarifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implicit versus explicit research datasets</head><p>Although the production of various data is very common in a scientific work, a large amount of the data discussed in scholarly articles are actually not named, not curated and not shared. For example:</p><p>The 47 tilapia DNA samples were sent to Beijing Genome Institute (BGI) for 101 bp paired-end sequencing using Illumina Hiseq 2500. (10.1038/srep14168)</p><p>The Illumina Hiseq 2500 device produces sequencing data, which are part of the described study. However, this sequencing data is not named and not shared, it remains implicit. Knowledge about the field and the involved devices are necessary to infer that data is produced. We define this sort of research datasets as implicit datasets. These data are generally not considered as valuable by the researchers and often not in a sharable state (proprietary format, embedded as project resources). We consider fully automatic recognition of this kind of implicit datasets currently beyond what is feasible given the current existing annotated corpora and machine learning technique accuracy.</p><p>In contrast, in this work we focus on explicitly mentioned datasets, which we define as:</p><p>1. all named datasets, 2. unnamed datasets explicitly mentioned as existing data.</p><p>Explicitly mentioned datasets are research data identified as such by the authors, discussed and considered valuable with respect to the scientific claims of the publication.</p><p>They are usually available in a form that makes sharing possible. Datasets related to 2) are unnamed data mentioned as used, produced or shared. They are usually referred to with generic substantive data or dataset, which can be annotated:</p><p>The data has been collected by the UN Comtrade organization Similarly to software, a reference to a named dataset can be ambiguous and should be examined in context. For example, is a mention to a database the same as a mention to a dataset? If research data has been loaded and shared via this database, we can consider in general that this data are packaged as a dataset. On the other hand, a "dataset" can exist independently of a database management system and is not ambiguous with respect to its storage and software access environment. So, for deciding if a mention to a database can be considered as a valid dataset mention, one has to clarify if the research work is referring to the data stored in the database or more generally to the software or the service associated to the database.</p><p>In addition, we have defined the following main guidelines:</p><p>• Data sharing initiative/project: we currently exclude the mentions to initiatives, collaborations (HEP or Astronomy) and projects from our dataset extraction. Only mentions to individual research dataset or collection are considered as a "dataset".</p><p>• Accession number: the references to an entry in a database, for instance via a unique identifier such as an accession number, is considered as a valid mention to a dataset.</p><p>Training Developing an annotated corpus of dataset mentions entirely from scratch would not have been possible for this project. The Softcite dataset for example took several years of development and involved a total of 38 different human annotators. To train our dataset mention recognizer, we reused existing annotated corpus, re-annotated them to follow our guidelines and produced some additional annotations for a limited subset.</p><p>As noted in section 4.5.1, the datasets of many existing work covering dataset mention recognition have not been made publicly available for reuse. In the case of the Coleridge Kaggle competition ("Coleridge Initiative -Show US the Data", 2021), we found that the annotations were unreliable, inconsistent and highly incomplete (less than 70 distinct "datasets" annotated only in 8,000 documents). In addition, this effort was realized without a clear definition of a "dataset". For example, a research initiative name such as ADNI could be labeled as "dataset" even in contexts not related to the datasets produced by this initiative. Unfortunately, the Coleridge dataset makes it impossible to apply stateof-the-art NER techniques without a considerable additional labeling effort and we could use it in a very limited manner, as explained below.</p><p>One additional challenge is that, to our knowledge, there is no annotation on a complete full text versions of the annotated articles, well adapted to tackle the sparsity problem on a real distribution. The existing training data are always sets of positive sentences with at least one dataset annotation. In general, there is no guarantee that the rest of these articles do not include other datasets and checking such a large amount of content is highly time-consuming. To mitigate this issue, we divided the task in two steps, the first one to identify the data sentences in a complete article, and the second one to spot dataset mentions in selected data sentences.</p><p>The first step is implemented as data sentence classification model. We used 2,000 positive sentences containing at least one dataset mention and 20,000 negative data sentences without dataset mention from PubMed Central full-texts and from the Coleridge dataset. This combined dataset was used to fine-tune a LinkBERT-base binary classifier.</p><p>The second step is the dataset mention recognition in the identified data sentences. To train this model, we use the following resources:</p><p>• a re-annotated version of the dataset<ref type="foot" target="#foot_3">5</ref> developed by <ref type="bibr" target="#b18">(Heddes et al., 2021)</ref>, a set of 6,000 sentences in the IR/ML/NLP domain with 3,684 dataset mentions. We fully reviewed the dataset and re-annotated to follow our dataset annotation principles: it covers now new datasets (not just reused ones) with annotations at individual dataset level, avoid one annotation for a conjunction expression of datasets.</p><p>• a set of approx. 1000 sentences from PubMed Central full-texts with at least one dataset mention (named or unnamed datasets) and partial data acquisition device annotations.</p><p>These resources were used to train a LinkBERT base model <ref type="bibr" target="#b57">(Yasunaga et al., 2022)</ref> with an additional CRF activation layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Accuracy</head><p>As explained above, the first model is classifying if a sentence introduces a dataset or not.</p><p>A 10-fold cross-validation on our dataset gives the accuracy presented in Table <ref type="table" target="#tab_7">7</ref>.</p><p>Similarly, we evaluate the mention recognition using 10-fold cross-validation on the an-  <ref type="table" target="#tab_8">8</ref>. In the actual application of the models, the second model is applied only to sentences detected as "data sentence", so the error of the previous model detecting "data sentences" will be propagated to the second model. However, even combining the two error rates, the recognition of mentions of explicit datasets (dataset with names or without name but expressed as dataset or data) appears very reliable with the current amount of training data. This result supports the relevance of these extracted mentions for building indicators. At this stage, unnamed datasets are harder to recognize and will require additional training data and modeling efforts. We also present for reference the current recognition scores for data device mentions, but their manual annotations are still work-in-progress and very limited. We think that the automatic identification of data acquisition devices or data processing devices could help in the future to spot unnamed and implicit data in a more reliable way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Characterization of mention contexts</head><p>Whether or not a research dataset or software mentioned in an article was used, created and shared is important to monitor the compliance with Open Science policies. We hypoth-esize here that the wording used to introduce and describe a dataset or software mention can characterize its possible usage, creation and sharing. The sentences containing the mentions are used as classifier input, without additional features. As we observed that the wording used to describe the role of these mentions is very similar for dataset and software, we used the same classifiers for characterizing both research dataset and software mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Training data</head><p>The annotated data for training the classifiers are a combination of existing training data and additional manual annotation realized during the project:</p><p>• the used attribute of the Softcite dataset,</p><p>• the corresponding annotations in the SoMeSci dataset,</p><p>• a new additional set of 500 contexts manually annotated focusing more on datasets and the minority classes (created and shared).</p><p>Table <ref type="table" target="#tab_9">9</ref> presents the distribution of classes in this assembled training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Accuracy</head><p>The evaluation scores presented in Table <ref type="table" target="#tab_10">10</ref> are produced using 10-fold cross-validation based on three binary classifiers, one per class used/created/shared. The classifiers are fine-tuned LinkBERT base models <ref type="bibr" target="#b57">(Yasunaga et al., 2022)</ref>. Binary classifiers for each class perform significantly better than a single multiclass classifier (up to 4 F1-score points). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Recognition of availability statements</head><p>We have extended GROBID to identify data and code availability statements in research publications automatically. We define a data and/or code availability statements as a standalone section of a research publication (with a section title and one or several paragraphs) describing how the data and code involved in the research work can or cannot be accessed.</p><p>Availability statements usually appear in the front page of an article or at the end as a annex, but we also considered positions inside the main body, which is not rare with preprints. We did not put any constraints on the section title associated to "data availability statements".</p><p>95 articles out of the 520 training articles of the GROBID segmentation model have been further annotated with data availability section markups. This GROBID model is used to segment the main zone of a scientific article, such as header, body, bibliographical section, acknowledgement or funding. The data availability section is then structured and identified as such in the file TEI result file (TEI Consortium, 2010).</p><p>To cover the whole spectrum of possible difficulties, we evaluated the reliability of recognition on two set of high quality publisher publications and on one set of preprints:</p><p>• a set of 1000 random PLOS articles in <ref type="bibr" target="#b39">PDF and</ref><ref type="bibr">JATS XML (2003-2022)</ref>  On recent publications (2020 and after), we observed that the presence of data availability sections is correctly recognized in nearly all PLOS and eLife documents. On the other hand, preprint data availability statements can be challenging to identify because they can appear in non-usual positions in the article. They can also be introduced by a large variety of section titles depending on the described data. In peer-reviewed publisher versions, we generally observe a high regularity and very high precision similar to PLOS/eLife articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Architecture of the mention recognizers</head><p>The dataset and software mention extraction processes are very similar and share a common software architecture summarized in Figure <ref type="figure">1</ref>.</p><p>Figure <ref type="figure">1</ref>: Overview of the dataset and software mention recognition process.</p><p>We expect input full texts to be in PDF formats, but a large range of publisher XML formats are also supported. PDF are first parsed with the Open source PDF parser library pdfalto <ref type="bibr">("pdfalto", 2017-2023)</ref>. Complementary to the support of ALTO output, a modern format for OCR output, pdfalto implements additional features relevant to scientific documents, in particular better handling of multi-column documents, the recognition of superscript/subscript style and the robust recognition of line numbers for review manuscripts.</p><p>GROBID is then applied to structure the raw PDF stream into header, sections, paragraphs, footnotes, etc. Grobid applies in cascade a set of machine learning models to create hi-erarchical structures, combining text content and layout features (e.g. font and style information, relative positions, SVG objects, etc.). Bibliographical references are also extracted and parsed. If the input is encoded in a publisher XML format, we transform the XML into the same TEI format as produced by GROBID thanks to the Open Source Pub2TEI tool, which support the main XML publisher formats such as JATS, Elsevier, Wiley, etc. We can therefore centralize our document processing across PDF and XML sources.</p><p>Mining for specific entities is only relevant to certain textual structures of a scientific document, such as paragraphs, abstracts, figure captions, etc. In the Softcite corpus, we calculated that, on average, 28% of publication content should be filtered out. This includes metadata (author, affiliations), bibliographical sections, table and figure content, formulas, headnotes, page numbers, reference markers, or some editorial annexes (like conflicts of interest). Relying on GROBID not only improves the quality of textual content, but also makes it possible to apply the text mining process to relevant structures only, avoiding possible sources of false recognition.</p><p>To identify software names and related attributes, a first mention recognition is applied to relevant textual structures. For identified software names, a second model is then applied to refine the software mention types. For recognizing datasets, two models are applied.</p><p>A first classification model is applied to every sentences of the relevant textual structures to determine if the sentence is describing a dataset or not. A second model is applied on positive data sentences to identify dataset mention spans and attributes such as URL and bibliographical references. This division of the recognition task into two steps is introduced to mitigate the problem of lack of dataset annotations for complete documents, mentioned in the section 4.5.2.</p><p>Attachment of the bibliographical references in the context of the mentions are evaluated and if successful these bibliographical references are fully resolved against CrossRef DOI via the biblio-glutton service <ref type="bibr">("biblio-glutton", 2018-2023)</ref>. An entity disambiguation is realized in context using entity-fishing ("entity-fishing", 2016-2023) for every software mention candidates. If the candidates is significantly more likely scientific entities different from a software, the candidates are discarded to avoid likely false positives. Otherwise, if the software entity is a known disambiguated software present in Wikidata, the entity is linked via the resulting Q Wikidata identifier. Finally a document-level propagation is realized to identify possible overlooked dataset and software name matches in the same article in order to improve recall.</p><p>5 Application to the French Open Science Monitor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Monitoring indicators</head><p>To help steer French public policy, we have seen in section 1. </p><formula xml:id="formula_0">I naive = n share n publication</formula><p>(2)</p><formula xml:id="formula_1">I use = n use n text-mined<label>(3)</label></formula><formula xml:id="formula_2">I create = n create n use<label>(4)</label></formula><formula xml:id="formula_3">I share = n share n create<label>(5)</label></formula><p>A direct indicator would be, for example, the proportion of publications that share a dataset as given by I naive (2). This indicator is simple to understand, but has several shortcomings.</p><p>First, concerning the denominator, not all publications can be analysed by Softcite and DataStet, because the PDF could not systematically be downloaded. It should therefore be replaced by the number of publications that have been processed. Second, we want to have indicators whose upper bound has interpretations matching the objectives of the public policy. For example, if the described research work does not require the use of any datasets or software, the article is not relevant for the sharing ratio. Thus, the notion of a publication that shares a dataset should be clarified as a publication that uses, creates and shares a dataset.</p><p>We therefore propose to monitor the modified key indicator I share (5). Publications that do not create a dataset are however also meaningful to understand which proportion of the publications is relevant for data sharing. Therefore, we propose to complete the analysis with the two additional indicators I use (3) and I create (4).</p><p>These three indicators I use , I create and I share , correspond to a funnel analysis. They give a global and separated view of all relevant cases: the bottom of the funnel corresponds to the shareable data, which is actionable and which we want to increase. The other two are more descriptive, but will provide insights for understanding the practices regarding data, particularly by breaking them down by subject area.</p><p>The three indicators can be further broken down by metadata facets of the publications: year of publication, disciplines, publishers, etc.</p><p>We supplement these indicators by providing also the ratio of publications where an explicit All the modules are deployed on a managed Kubernetes cluster provided by the public cloud OVHcloud. Unfortunately, the cloud provider did not make available GPU at the time of the run. We used 5 servers with 16 CPU 240GB RAM and 5 servers with 32 CPU  Producing document-level indicators as defined in 5.1 is interesting in term of interpretability, but also for the sake of reliability. As the number of extracted mentions increases for one document, the number of observations for producing the indicators also increases. Figure <ref type="figure" target="#fig_11">3</ref> shows the distribution of documents by number of extracted mentions. We see that for datasets, 462,486 documents have three or more extracted mentions (85% of all documents having at least one dataset mention), and for software 185,710 documents (56% of all documents having at least one software mention). Considering for instance an error rate at mention level at 15-20%, when several mentions are extracted this error rate follows a polynomial reduction. Several mentions similarly classified (as used/created/shared)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Infrastructure and runtime</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dataset and software mention extraction</head><p>consolidates the document-level indicator at higher certainty levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">French Open Science Monitor indicators and dashboards</head><p>The first step of the funnel analysis described in section 5.1 focuses on the use of data in research works. As explained in section 4.7, we also tracked the proportion of publications that include a data and code availability statement section. As show in Figure <ref type="figure" target="#fig_14">6</ref>, the upward trend is very strong, as this indicators goes from only 1% in 2013 to 21% in 2021.  group with more than 200 members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Local versions of the Open Science Monitor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations and future work</head><p>Corpus completeness Several factors currently limit the completeness of our text mining processing framework:</p><p>• The creation of the country-wide corpus of publications is only covering articles with DOIs.</p><p>• In the current first version of the Monitor, around two thirds of the full texts have been successfully harvested.</p><p>• Only English content is currently supported by Softcite and DataStet.</p><p>We are aware that these factors, combined together, specifically impact certain scholar fields like Law, Social Science and Humanities. These fields have a lower adoption of DOIs, Open Access and publish more frequently in French.</p><p>We expect to increase both the coverage of publication by improving our affiliation matching and by further extending the collection to publications without DOI. The French monitor corpus already contains PhD thesis and all the documents indexed on the French open repository HAL, regardless of the presence of a DOI. These documents could potentially be analyzed to detect datasets and software mentions.</p><p>The effort required to the support of the French language by Softcite and DataStet will also be evaluated. Accordingly, a plan for producing additional training data in French will be planed for the next year.</p><p>Performance across domains Research dataset and software mention recognizers are currently limited by the relatively poor multidisciplinary coverage of the training data. <ref type="bibr" target="#b35">(Lopez et al., 2021)</ref> shows that the recognition performance for software mention falls by around 15 points F1-score on an entirely new scientific domain.</p><p>To mitigate this issue, we have started new round of manual annotations on multiple domains with systematic model retraining and evaluation. We already observed that, as the global corpus grows, the required number of annotated documents to cover a new domain decreases. We think that we can realistically expect a relatively uniform accuracy of the recognizer across all scholar fields in the mid term.</p><p>Large-scale research entity disambiguation Recognizing dataset and software entities could make it possible to foresee indicators centered that are not only centered on documents, but also on global production of research datasets and software and their reuse. By recognizing and reporting data and software reuse among a large corpus of publications over time, researchers could receive credit and acknowledgement for the impact of their datasets and software. Such credit can be strong incentive for researchers to open their data and software, as well as producing better datasets and software over time.</p><p>However dataset and software-level indicators presuppose that we are able to disam-biguate different mentions of the same dataset and software entity. This task is complicated because software names tend to be very ambiguous and research datasets are mostly unnamed. In addition, no reference database of research software with uniform metadata exists today. We have started to explore mention disambiguation at scale, including referencing when possible datasets with DataCite entries and software with Software</p><p>Heritage entries and we will assess the feasibility of dataset and software-level indicators in the future BSO.</p><p>Local Open Science Monitor Automatic affiliation-level identification is harder than country identification. Moreover, smaller institutions might not gather enough publications for the minority categories production and sharing on datasets and software for reliable trends over time. In order to partially mitigate these issues, the BSO includes the possibility to create local monitors based on a list of DOIs, which is relevant for institutions collecting the list of all their researcher publications for general reporting and evaluation purposes.</p><p>Covering dataset repositories While datasets associated to an existing publication are particularly important in the context of Open Science and scientific quality, other types of datasets might be developed without mentions in published research works. Some of them can simply be deposited on research dataset repositories. We think that an additional indicator following the global production of deposited datasets would complement the BSO and help to understand and drive research practices. However, poor affiliation metadata associated to datasets in repositories is an additional challenge. We are currently working on such an additional indicator, which is expected to be on air in the next BSO version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The French Open Science Monitor shows, first, that quality indicators can rely on open bibliographical data only. Being independent from proprietary databases is an advantage in term of public trust, sovereignty and sustainability. Beyond research publications, our platform also demonstrates that reliable indicators on datasets and software openness can be developed thanks to modern text mining techniques at a large scale. This approach does not depend on the adoption of PID for these research products, which is currently too limited.</p><p>Even if indicators are estimates and not exact absolute values, we think that the relative trends over several years are reliable. First as the indicators are produced under the same conditions over time, consistent global trends are captured. Second, several mentions identified in the same document lead to very reliable document-level measurements because the global error rate margin is significantly reduced. Rather than perfect absolute measurements, reliable trends are enough to start following the impact of Open Science policies and adapt effort accordingly.</p><p>We expect that these techniques will also help us to explore other research measurements in the next version of the Monitor. Dataset and software-level citations and impact in particular can pave the way to new incentives to recognize and improve the production of datasets and software by scientists.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>••</head><label></label><figDesc>Freshness: Policy indicators are developed to capture recent changes in publishing practices. The data acquisition underlying these indicators must reduce as much as possible delays between actual publication dates and measurements. • Adaptability to different geographical and organizational levels: To exploit indicators, we expect that further analyses are possible at lower levels of granularity than the sole national level. Deriving indicators at the level of geographical areas and at the level of individual organizations (universities, research institutes, research laboratories) are requirements for the proper study and adaptation of an Open Science policy. Adaptability to different research domains: Practices vary significantly from one research domain to another. The volume of scientific production is also specific to research areas, and one field could be entirely diluted and invisible within global indicators. Following the evolution of indicators by scientific and technical domains is a key requirement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Science indicators for research datasets and software To our knowledge, there are currently only two examples of deployed Open Science monitors related to research data and software, one part of the OpenAire infrastructure and the PLOS Open Science indicators. We discuss these two examples based on the quality criteria presented in the previous section. 1.4.1 OpenAire OpenAire is an infrastructure dedicated to open science and mainly funded by EU Horizon 2020 and Horizon Europe programs. Among other services, OpenAire developed the Open Science Observatory 2 to better understand the European open research landscape. The service includes dashboards related to publications, datasets and software.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>late 2022, and with an update in March 2023, PLOS published a series of open science indicators (Public Library of Science, 2023). These indicators are evaluated on the 71,109 PLOS research articles published during the 4 year period from January 2019 to December 2022. Three main indicators are provided: the rates of research data sharing per article, the rate of code sharing per article and rate of preprints associated to a published article.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Finally</head><label></label><figDesc>, although the text mining tools from DataSeer are open source and documented, their training data and Machine Learning models are currently not open. The trustfulness criteria for the researchers is impacted by the limitation for reproducing the annotations used to derived the indicators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3.</head><label></label><figDesc>Use an in-house open source affiliation matcher (L'Hôte &amp; Jeangirard, 2021) to detect affiliation countries from raw affiliation strings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>, has been developed to detect open data statements in full texts and applied at scale to 2.75 million articles on PubMed Central XML publications (Serghiou et al., 2021). It implements a rule-based approach for capturing open data statement patterns. ODDPub yields global information about openness of data and software used in a document. The rules have been developed for biomedical literature. However, we think ODDPub has underlying limitations for the production of data and software Open Science indicators and these limitation can be generalized to other similar pattern matching tools:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>system and was developed in parallel with the creation of the Softcite dataset. The system includes a variety of models trained with the Softcite dataset. The training uses sampling techniques to increase the number of negative examples for mitigating the problem of mention sparsity. The best model is a SciBERT model, with a CRF activation layer, finetuned with the positive examples of the Softcite dataset and additional negative examples selected with a technique called active sampling. The training set uses a 1:20 ratio (20 paragraphs without annotations for one paragraph with at least one annotation). With the Softcite dataset, it corresponds to around 2000 positive paragraphs and 40K negative paragraphs. The model has been evaluated on full paper content for 20% of the annotated articles. Additional entity disambiguation is used to filter out false positives and documentlevel mention propagation is used to increase recall. Input of the service can be PDF or various publisher XML formats.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>https://comtrade.un.org/), and cleaned by CEPII. (10.1371/journal.pone.0203915) All the data were recorded after each sub-culture (at 3, 6 and 9 weeks). (10.1038/s41598-018-37335-7) The dataset used in this study consist of natural products that have been tested for in-vitro antiplasmodial activities (NAA) compiled in-house from literature, PhD Theses and public chemical databases. (10.1371/journal.pone.0204644)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>3 that our indicators must meet several minimum quality conditions. In particular, indicators must have floors and ceilings that are easy to understand and interpret by the public, with ceiling values as objectives of a policy. For research publications, the French Open Science Monitor reports the percentage of open access publications of the previous years. Each year the new indicators are recalculated and do not depend on the indicator of the previous years. The openness indicators vary between 0% and 100%, and 100% is the French National Open Science Plan objective for 2030. For consistency and readability, we consider that the same requirements apply to indicators for research data and software. The mention detection uses the full-text of the publications as raw material. It is therefore straightforward to also propose indicators relating to the proportion of publications. As the methodology is similar for both datasets and software, the proposed indicators will be equivalent, replacing dataset by software. Research dataset and software notations and indicators n publication : number of publications n text-mined : number of text-mined publications n use : number of text-mined publications that mention the usage of at least one dataset n create : number of text-mined publications that mention the usage and the production of at least one of their datasets n share : number of text-mined publications that mention the usage, the production and the sharing of at least one of their dataset P text-mined = n text-mined n publication (1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2</head><label>2</label><figDesc>Figure 2 outlines the overall automated processing of the full texts part of the BSO platform. After the harvesting phase of PDF documents, several machine learning modules are applied in parallel to extract structured information from the full text: GROBID, Softcite and DataStet. The different extracted mention annotations and metadata are analyzed and aggregated by a Python module to produce the indicators at document level. This module feeds an ElasticSearch instance, used to export a public data dump and as back-end for the different BSO related dashboards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Global overview of the text-mining data flows</figDesc><graphic coords="44,72.00,72.00,468.00,326.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distribution of documents by number of extracted mentions in the French Open Science Monitor corpus covering 2013-2021, for (a) datasets and (b) software.</figDesc><graphic coords="45,72.00,446.11,234.00,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Figure 4: (a) Proportion of publications in France that mention the use of data. (b) Proportion of publications in France that mention having produced data. (c) Proportion of publications in France that mention the sharing of their data.</figDesc><graphic coords="46,72.00,397.45,152.09,76.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>One of the objectives of the French Open Science Monitor is to provide all research institutions with a local version of the indicators. Similarly as for measuring the openness of research publications on the existing dashboard, the new indicators can be bounded to datasets and software produced by the researchers affiliated with a particular organization. Not requiring any coding skills, more than 40 French research institutions have published their local monitor with the dataset and software indicators, as of May 2023. This capacity of simple customization has led to the creation of a French Open Science Monitor user</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Proportion of publications in France including a Data Availability Statement.</figDesc><graphic coords="48,72.00,72.00,467.99,236.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="39,95.40,155.76,421.19,315.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Share of the different sources in the overall French publication aggre-</figDesc><table><row><cell cols="8">gated corpus (total of 167,412 publications) for the year 2019, as reported by</cell></row><row><cell cols="2">(Chaignon &amp; Egret, 2022).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">Scopus WoS HAL ADS PubMed MAG BSO</cell></row><row><cell>Share of total (%)</cell><cell>67</cell><cell>58</cell><cell>38</cell><cell>9</cell><cell>29</cell><cell>61</cell><cell>92</cell></row><row><cell cols="8">(Chaignon &amp; Egret, 2022) further indicates that the approach used by the French Open</cell></row><row><cell cols="8">Science Monitor effectively identifies the vast majority of publications with a persistent</cell></row><row><cell cols="8">identifier (DOI) for Open Science monitoring. Table 1 reports the coverage estimated by</cell></row><row><cell>this study.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>presents the proportion of Open and Closed Access publica-</cell></row><row><cell>tion for the complete BSO collection (2013-2021) as of year-end 2022. We also indicate the</cell></row><row><cell>number for the first considered publication year (2013) and the last one (2021) to illustrate</cell></row><row><cell>the current Open Access dynamic.</cell></row><row><cell>Despite the presence of one or several direct full text access URL, around 15% of down-</cell></row><row><cell>loads fail. Main failure reasons include broken link, IP blocking after a threshold of down-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Number of publications in the French Open Science Monitor with Open Access (OA) and Closed Access (CA) shares with associated download rates of full texts (PDF), as of year-end 2022.</figDesc><table><row><cell>Publication periods</cell><cell>2013-2021</cell><cell>2013</cell><cell>2021</cell></row><row><cell>Overall # publications in BSO</cell><cell cols="2">1,426,140 143,095</cell><cell>160,217</cell></row><row><cell>-# succesful dowload</cell><cell>908,567</cell><cell>85,914</cell><cell>103,211</cell></row><row><cell>-% succesful dowload rate</cell><cell>63.7</cell><cell>60.0</cell><cell>64.4</cell></row><row><cell># OA publications</cell><cell>773,753</cell><cell>61,849</cell><cell>107,722</cell></row><row><cell>-# succesful downloads</cell><cell>660,501</cell><cell>52,549</cell><cell>85, 073</cell></row><row><cell>-% succesful download rate</cell><cell>85.4</cell><cell>85.0</cell><cell>79.0</cell></row><row><cell>Number of CA publications</cell><cell>652,387</cell><cell>81,246</cell><cell>52,495</cell></row><row><cell>-# succesful dowmloads</cell><cell>248,066</cell><cell>33,365</cell><cell>18,138</cell></row><row><cell>-% succesful dowmload rate</cell><cell>38.0</cell><cell>41.1</cell><cell>34.6</cell></row><row><cell cols="4">loads, or Cloudflare challenges to block machine-based download. The usage of robust</cell></row><row><cell cols="4">web harvesting techniques (rotating HTTP header, support of simple Cloudflare challenges,</cell></row><row><cell cols="4">random delays, bulk download of the full arXiv collection, etc.) only partially mitigates the</cell></row><row><cell>risk of failed downloads.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">For closed access publications, programmatic download of PDF documents depends on</cell></row><row><cell cols="4">the nature of the contractual agreement with individual publisher for Text and Data Min-</cell></row><row><cell cols="4">ing (TDM) application, leading to complex and time-consuming contractual negotiations.</cell></row><row><cell cols="4">Even when such a contractual agreement exists, implementing it can be costly. Most pub-</cell></row><row><cell cols="4">lishers have set procedures and technical constraints to control or even discourage this</cell></row><row><cell cols="4">type of projects. Moreover, such a contract may not be renewed, thus not guaranteeing the</cell></row><row><cell cols="4">sustainability of the approach over time for closed publications. Our full text harvesting</cell></row><row><cell cols="2">currently includes Elsevier and Wiley TDM web services.</cell><cell></cell><cell></cell></row><row><cell cols="4">Table 2 further summarizes the success rate of the harvesting of full text documents real-</cell></row><row><cell>ized in 2022.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Reported scores, reproduced scores and benchmark scores against an holdout set of full papers for three Deep Learning software mention recognizers. Fork for reproduced cross-evaluation and Softcite holdout evaluations available at https://github.com/kermitt2/software-mention-extraction-czi b Fork for reproduced SoMeSci cross-evaluation and Softcite holdout evaluations available at https://github.com/kermitt2/SoMeNLP Table3illustrates the issue of training and predicting with very different mention distributions. We indicate reported scores based on cross-evaluation on the annotated corpus (all over-representing mentions) and our reproduced scores in the first two columns. In the third column, we report realistic evaluation scores on the full content of a set of annotated articles (the softcite holdout set, 20% of the Softcite articles).</figDesc><table><row><cell>Software mention</cell><cell>F1-score on</cell><cell>F-1 score on</cell><cell>F1-score on</cell><cell>Note</cell></row><row><cell>recognizer</cell><cell>annotated</cell><cell>annotated</cell><cell>holdout set</cell><cell></cell></row><row><cell></cell><cell>corpus as</cell><cell>corpus, re-</cell><cell>(full article</cell><cell></cell></row><row><cell></cell><cell>reported in</cell><cell>produced</cell><cell>content)</cell><cell></cell></row><row><cell></cell><cell>original pub-</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>lication</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CZI recognizer</cell><cell>92.0</cell><cell>85.5 a</cell><cell>56.3 a</cell><cell>(software name</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>and version only)</cell></row><row><cell>SoMeSci recog-</cell><cell>88.3</cell><cell>84.0 b</cell><cell>62.4 b</cell><cell>("Application"</cell></row><row><cell>nizer</cell><cell></cell><cell></cell><cell></cell><cell>name only)</cell></row><row><cell>Softcite rec-</cell><cell>-</cell><cell>82.3</cell><cell>79.1</cell><cell>(software name,</cell></row><row><cell>ognizer (using</cell><cell></cell><cell></cell><cell></cell><cell>version, pub-</cell></row><row><cell>around 40K neg-</cell><cell></cell><cell></cell><cell></cell><cell>lisher, url)</cell></row><row><cell>ative sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>examples)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>a</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Annotation overview for the Softcite corpus version 1.0 and 2.0.</figDesc><table><row><cell>Softcite dataset version</cell><cell>v1.0 (2020)</cell><cell>v2.0 (2023)</cell></row><row><cell>number of documents</cell><cell>4,971</cell><cell>4,971</cell></row><row><cell>software name (total)</cell><cell>4,093</cell><cell>5,134</cell></row><row><cell>-environment</cell><cell>-</cell><cell>1,089</cell></row><row><cell>-component</cell><cell>-</cell><cell>88</cell></row><row><cell>-implicit</cell><cell>-</cell><cell>106</cell></row><row><cell>version</cell><cell>1,258</cell><cell>1,478</cell></row><row><cell>publisher</cell><cell>1,111</cell><cell>1,311</cell></row><row><cell>URL</cell><cell>172</cell><cell>231</cell></row><row><cell>programming language</cell><cell>-</cell><cell>71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Evaluation scores of the fine-tuned SciBERT model for software mention recognition. They report F1-scores of 48.6, 63.4 and 66.2 for dataset mention recognition on the EneRex, SciREX and Papers With Code datasets respectively.</figDesc><table><row><cell></cell><cell>preci-</cell><cell>recall</cell><cell>F1-score</cell><cell>support</cell></row><row><cell></cell><cell>sion</cell><cell></cell><cell></cell><cell></cell></row><row><cell>publisher</cell><cell>75.51</cell><cell>88.80</cell><cell>81.62</cell><cell>250</cell></row><row><cell>software</cell><cell>74.01</cell><cell>88.98</cell><cell>80.81</cell><cell>989</cell></row><row><cell>url</cell><cell>53.97</cell><cell>82.93</cell><cell>65.38</cell><cell>41</cell></row><row><cell>version</cell><cell>83.99</cell><cell>90.81</cell><cell>87.27</cell><cell>283</cell></row><row><cell>all (micro avg.)</cell><cell>75.22</cell><cell>89.12</cell><cell>81.58</cell><cell>1563</cell></row><row><cell cols="5">a fine-tuned SciBERT model with an additional CRF activation layer. This model is applied</cell></row><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>F1-score</cell><cell>support</cell></row><row><cell>component</cell><cell>71.43</cell><cell>71.43</cell><cell>71.43</cell><cell>7</cell></row><row><cell>environment</cell><cell>83.33</cell><cell>83.33</cell><cell>83.33</cell><cell>102</cell></row><row><cell>implicit</cell><cell>72.73</cell><cell>57.14</cell><cell>64.00</cell><cell>14</cell></row><row><cell>language</cell><cell>100.00</cell><cell>61.54</cell><cell>76.19</cell><cell>13</cell></row><row><cell>all (micro avg.)</cell><cell>82.81</cell><cell>77.94</cell><cell>80.30</cell><cell>136</cell></row><row><cell cols="2">4.5 Research datasets</cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.5.1 Existing work</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Similarly to software mentions, data citations are inconsistent, vague, incomplete and indi-</cell></row><row><cell cols="5">rect (Mooney, 2011). Beyond rule-based method such as ODDPub (Riedel et al., 2020), the</cell></row><row><cell cols="5">limitation of which we have discussed in section 4.2, several works have explored machine</cell></row><row><cell cols="5">learning techniques for the automatic recognition of datasets in technical and scientific</cell></row><row><cell>documents.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p><p><p><p><p>only to sequences where at least one software mention has been identified previously. The following scores are thus produced using 10-fold cross-validation on a training set based on all the paragraph with at least one annotated software.</p>Table</p>6</p>: Evaluation scores of fine-tuned SciBERT model for predicting software mention sub-types.</p>The Coleridge Initiative held a Kaggle competition on dataset recognition in 2021 in the field of public health and social policies ("Coleridge Initiative -Show US the Data", 2021). However, we found very hard to rely on the dataset or methods developed for this competition, as further discussed in section 4.5.2.</p><ref type="bibr" target="#b24">(Jain et al., 2020)</ref> </p>developed SciREX, a dataset of 438 annotated arXiv documents on the Machine Learning domain, with identification of named datasets, among other entities. The reported global F1-score for mention recognition with a BERT-based model is 71.2. Finally, EneRex</p><ref type="bibr" target="#b58">(Yousuf et al., 2022)</ref> </p>is a system for the extraction of various, including technical, facets from scientific publications, again on the Machine Learning field, including used datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Evaluation scores for first-stage data sentence recognition.</figDesc><table><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>F1-score</cell><cell>support (10%)</cell></row><row><cell>data sentence</cell><cell>93.70</cell><cell>96.21</cell><cell>94.94</cell><cell>200</cell></row><row><cell>not data sentence</cell><cell>97.56</cell><cell>95.92</cell><cell>96.73</cell><cell>2000</cell></row><row><cell>notated dataset, see Table</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Evaluation scores for dataset mention recognition in predicted data sentences.</figDesc><table><row><cell></cell><cell cols="2">precision recall</cell><cell cols="2">F1-score support (10%)</cell></row><row><cell>named dataset</cell><cell>89.04</cell><cell>89.46</cell><cell>89.24</cell><cell>466</cell></row><row><cell cols="2">unnamed mentionned dataset 71.85</cell><cell>67.15</cell><cell>69.38</cell><cell>927</cell></row><row><cell>data device</cell><cell>51.91</cell><cell>37.94</cell><cell>42.61</cell><cell>97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="2">: Distribution in manually an-</cell></row><row><cell cols="2">notated training data of the classes for</cell></row><row><cell cols="2">dataset and software mention charac-</cell></row><row><cell>terization.</cell><cell></cell></row><row><cell>total contexts</cell><cell>3,643</cell></row><row><cell>used</cell><cell>2,774</cell></row><row><cell>not used</cell><cell>869</cell></row><row><cell>created</cell><cell>338</cell></row><row><cell>not created</cell><cell>3,305</cell></row><row><cell>shared</cell><cell>266</cell></row><row><cell>not shared</cell><cell>3,377</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Evaluation scores for dataset and software mention characterization.</figDesc><table><row><cell></cell><cell>precision</cell><cell>recall</cell><cell>F1-score</cell><cell>support</cell></row><row><cell>used</cell><cell>96.83</cell><cell>94.18</cell><cell>95.49</cell><cell>292</cell></row><row><cell>not used</cell><cell>84.40</cell><cell>91.09</cell><cell>87.62</cell><cell>101</cell></row><row><cell>created</cell><cell>81.08</cell><cell>83.33</cell><cell>82.19</cell><cell>31</cell></row><row><cell>not created</cell><cell>98.31</cell><cell>98.04</cell><cell>98.18</cell><cell>362</cell></row><row><cell>shared</cell><cell>81.82</cell><cell>90.00</cell><cell>85.71</cell><cell>26</cell></row><row><cell>not shared</cell><cell>99.35</cell><cell>98.71</cell><cell>99.03</cell><cell>385</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Table 11 presents an evaluation of the accuracy of the data and code availability statement recognition in a end-to-end scenario with PDF as input. Evaluation scores for the automatic identification of Data Availability Statements. Datasets are available at https://zenodo.org/record/7708580.</figDesc><table><row><cell>containing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Table 12 presents statistics on the number of documents processed by the text mining Statistics on the proportion of text-mined publications in the French Open Science Monitor corpus covering 2013-2021.</figDesc><table><row><cell cols="4">modules and the total of extracted mentions in the BSO published in early 2023 (period</cell></row><row><cell>2013-2021).</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Publications 2013-2021</cell></row><row><cell></cell><cell># documents</cell><cell cols="2">share (%) # mentions</cell></row><row><cell>Full corpus</cell><cell>1,426,140</cell><cell>100.0</cell><cell></cell></row><row><cell>Full texts downloaded</cell><cell>908,567</cell><cell>63.7</cell><cell></cell></row><row><cell>Processed with GROBID</cell><cell>743,700</cell><cell>52.1</cell><cell></cell></row><row><cell>Processed with Softcite</cell><cell>742,289</cell><cell>52.0</cell><cell>3,567,547</cell></row><row><cell>Processed with DataStet</cell><cell>621,306</cell><cell cols="2">43.6 5,607,080</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://osobservatory.openaire.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>It means for example that one additional example of open code will increase or decrease the reported F1-score by around ten percents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>From one of the author of<ref type="bibr" target="#b56">(Westergaard et al., 2017)</ref>, an effort to apply text mining to 15 million scientific articles, "We probably spent more computational resources teasing the text out of PDFs and beating it into shape than we spent on the actual text mining."<ref type="bibr" target="#b37">(McKenzie, 2017)</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/xjaeh/ner_dataset_recognition</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Isabelle Blanc, Marin Dacos, Emmanuel Weisenburger (French Ministry of Higher Education and Research) and Jean-Francois Lutz (University of Lorraine) for stimulating discussions and support. We also thank James Howison (UT Austin) and Karthik Ram (UC Berkeley) for their valuable advises regarding research software.</p><p>We acknowledge the Jetstream cloud environment part of XSEDE and the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing computing resources that have contributed to the creation of relevant machine learning models in 2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding information</head><p>The extension of the French Open Monitor (BSO) presented in this document was supported by France Relance/NextGenerationEU funding. Previous work on the Softcite dataset and the Softcite Software mention recognizer by Patrice Lopez was supported by the Alfred P. Sloan Foundation, Grant/Award Number: 2016-7209 (2018-2020) and the Gordon and Betty Moore Foundation, Grant/Award Number 8622 (2021). DataStet is a continuation of dataseer-ml, developed by Patrice Lopez in the context of the DataSeer project (2019-2020), supported by the Alfred P. Sloan Foundation.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data Availability Statement section has been recognized in the full text. Although Data Availability Statements offer no guarantee of actual sharing for various reasons <ref type="bibr" target="#b14">(Gabelica et al., 2022)</ref>, it allows the tracking of the evolution of adoption of this practice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>at Zenodo, https://doi.org/10.5281/zenodo.8033739. The annotated corpus of mention context characterizations is available at Zenodo, https://doi.org/10.5281/zenodo.8033868.</p><p>Datasets relative to Data Availability Statement recognition are part of the Grobid annotated data (available at https://github.com/kermitt2/grobid) and evaluation datasets are available at Zenodo, https://zenodo.org/record/7708580.</p><p>The source code used for the French Open Science Monitor is available on GitHub under the organization https://github.com/Barometre-de-la-Science-Ouverte, under MIT or Apache 2 licenses. The web dashboards of the monitor are available at https://github.com/dataesr/bso-ui.</p><p>The source code, documentation and releases of the following tools are available under Apache 2 licenses (except when indicated) as indicated below:</p><p>• Softcite software mention recognizer: https://github.com/softcite/software-mentions • DataStet: https://github.com/kermitt2/datastet • Grobid : https://github.com/kermitt2/grobid • Pub2TEI: https://github.com/kermitt2/Pub2TEI (MIT)</p><p>• entity-fishing: https://github.com/kermitt2/entity-fishing • biblio-glutton: https://github.com/kermitt2/biblio-glutton • pdfalto: https://github.com/kermitt2/pdfalto (GPL-2.0) Author contributions AB was involved in analysis and interpretation of data, drafting and revising the article. LB was involved in management and coordination, conception and design, acquisition of data, analysis and interpretation of data, drafting and revising the article. ALH was involved in conception and design, software development, acquisition of data, analysis and interpretation of data, drafting and revising the article. EJ was involved in conception and design, software development, acquisition of data, analysis and interpretation of data, drafting and revising the article. PL was involved in conception and design, acquisition of data, software development, analysis and interpretation of data, drafting and revising the article. LR was involved in conception and design, analysis and interpretation of data, drafting and revising the article.</p><p>Competing interests Patrice Lopez is a contractor self-employed in his software service company science-miner. The other authors declare no competing interests.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Scibert: A pretrained language model for scientific text. Biblio-glutton</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<ptr target="https://github.com/kermitt2/biblio-glutton" />
		<imprint>
			<date type="published" when="2018">2019. 2018-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Extending the open monitoring of open science: A new framework for the French Open Science Monitor (BSO</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bracco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>L'hôte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jeangirard</surname></persName>
		</author>
		<author>
			<persName><surname>Torny</surname></persName>
		</author>
		<author>
			<persName><surname>Didier</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-03651518" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bretel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Medves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monteil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<ptr target="https://inria.hal.science/inria-00537302" />
		<title level="m">Back to meaning -information structuring in the PEER project. TEI Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying scientific publications countrywide and measuring their open access: The case of the French Open Science Barometer (BSO)</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chaignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Egret</surname></persName>
		</author>
		<idno type="DOI">10.1162/qss_a_00179</idno>
		<ptr target="https://doi.org/10.1162/qss_a_00179" />
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="36" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation of rule-based annotators for named-entity recognition tasks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chiticariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1002" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Coleridge initiative -show us the data</title>
		<ptr target="https://www.kaggle.com/competitions/coleridgeinitiative-show-us-the-data" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bringing citations and usage metrics together to make data count</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cousijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Feeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lowenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Presani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Simons</surname></persName>
		</author>
		<idno type="DOI">10.5334/dsj-2019-009</idno>
		<ptr target="https://doi.org/http://doi.org/10.5334/dsj-2019-009" />
	</analytic>
	<monogr>
		<title level="j">Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<ptr target="https://github.com/dataseer" />
		<title level="m">Dataseer project</title>
		<imprint>
			<date type="published" when="2019">2019-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Softcite dataset: A dataset of software mentions in biomedical and economic research publications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howison</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.24454</idno>
		<ptr target="https://doi.org/10.1002/asi.24454" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="870" to="884" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding progress in software citation: A study of software citation in the CORD-19 corpus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howison</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.1022</idno>
		<ptr target="https://doi.org/10.7717/peerj-cs.1022" />
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="page" from="8" to="e1022" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ambiguity and variability of database and software names in bioinformatics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kovacevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nenadic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical semantics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Open research data, data portals and data publication-an introduction to the data curation landscape</title>
		<author>
			<persName><forename type="first">K</forename><surname>Elger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Biskaborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lantuit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Polarforschung</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The rise and rise of unpaywall</title>
		<author>
			<persName><forename type="first">H</forename><surname>Else</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">560</biblScope>
			<biblScope unit="issue">7718</biblScope>
			<biblScope unit="page" from="290" to="291" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://github.com/kermitt2/entity-fishing" />
		<title level="m">Entity-fishing</title>
		<imprint>
			<date type="published" when="2016">2016-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Many researchers were not compliant with their published data sharing statement: A mixed-methods study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gabelica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bojƒçiƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Puljak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jclinepi.2022.05.019</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.jclinepi.2022.05.019" />
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Epidemiology</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Okg-soft: An open knowledge graph with machine readable scientific software metadata</title>
		<author>
			<persName><forename type="first">D</forename><surname>Garijo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<idno type="DOI">10.1109/eScience.2019.00046</idno>
		<ptr target="https://doi.org/10.1109/eScience" />
	</analytic>
	<monogr>
		<title level="m">15th International Conference on eScience (eScience)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Grobid</surname></persName>
		</author>
		<ptr target="https://github.com/kermitt2/grobid" />
		<imprint>
			<date type="published" when="2008">2008-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Do usage counts of scientific data make sense? an investigation of the dryad repository</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1108/LHT-12-2016-0158</idno>
		<ptr target="https://doi.org/10.1108/LHT-12-2016-0158" />
	</analytic>
	<monogr>
		<title level="j">Library Hi Tech</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="332" to="342" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The automatic detection of dataset names in scientific articles</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heddes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meerdink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pieters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marx</surname></persName>
		</author>
		<idno type="DOI">10.3390/data6080084</idno>
		<ptr target="https://doi.org/10.3390/data6080084" />
	</analytic>
	<monogr>
		<title level="j">Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Identification of tasks, datasets, evaluation metrics, and numeric scores for scientific leaderboards construction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jochim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bonin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguly</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1513</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1513" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5203" to="5213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fact sheet: Biden-harris administration announces new actions to advance open and equitable research</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>House</surname></persName>
		</author>
		<ptr target="https://www.whitehouse.gov/ostp/news-updates/2023/01/11/fact-sheet-biden-harris-administration-announces-new-actions-to-advance-open-and-equitable-research/" />
		<imprint>
			<date type="published" when="2023-05-17">2023. May 17th, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Software in the Scientific Literature: Problems with Seeing, Finding, and Using Software Mentioned in the Biology Literature</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bullard</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.23538</idno>
		<ptr target="https://doi.org/10.1002/asi.23538" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2137" to="2155" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Howison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cohoon</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.7995565</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7995565" />
		<title level="m">Softcite dataset version 2</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Istrate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Taraborelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Torkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veytsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2209.00693" />
		<title level="m">A large dataset of software mentions in the biomedical literature</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scirex: A challenge dataset for document-level information extraction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Monitoring Open Access at a national level: French case study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jeangirard</surname></persName>
		</author>
		<idno type="DOI">10.4000/proceedings.elpub.2019.20</idno>
		<ptr target="https://doi.org/10.4000/proceedings.elpub" />
	</analytic>
	<monogr>
		<title level="m">ELPUB 2019 23rd edition of the International Conference on Electronic Publishing, Academic publishing and digital bibliodiversity</title>
		<imprint>
			<date type="published" when="1920">2019. 2019.20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unique, persistent, resolvable: Identifiers as the foundation of fair</title>
		<author>
			<persName><forename type="first">N</forename><surname>Juty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Wimalaratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soiland-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Goble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chue Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muench</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bouquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edmunds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Feeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grenier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pastrana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yeston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Recognizing the value of software: A software citation guide [version 2; peer review: 2 approved</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<idno type="DOI">10.12688/f1000research.26932.2</idno>
		<ptr target="https://doi.org/10.12688/f1000research.26932.2" />
		<title level="m">F1000Research</title>
		<imprint>
			<date type="published" when="1257">1257</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Literature Review on Methods for the Extraction of Usage Statements of Software and Data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2019.2943847</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2019.2943847" />
	</analytic>
	<monogr>
		<title level="j">Computing in Science Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A natural language processing pipeline for detecting informal data references in academic literature</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lafia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hemphill</surname></persName>
		</author>
		<idno type="DOI">10.1002/pra2.614</idno>
		<ptr target="https://doi.org/https://doi.org/10.1002/pra2.614" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Information Science and Technology</title>
		<meeting>the Association for Information Science and Technology</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Larregue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent-Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lebaron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Larivière</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2585783</idno>
		<ptr target="https://doi.org/10.5281/zenodo.2585783" />
		<title level="m">Covid-19: where is the data?</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Share the code, not just the data: A case study of the reproducibility of articles published in the journal of memory and language under the open data policy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Laurinavichyute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vasishth</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jml.2022.104332</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.jml.2022.104332" />
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">104332</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequencing, combining and sampling classifiers to help find needles in haystacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deleris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th European Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>ECAI</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Using elasticsearch for entity recognition in affiliation disambiguation</title>
		<author>
			<persName><forename type="first">A</forename><surname>L'hôte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jeangirard</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.01958" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mining software entities in scientific literature: Document-level NER for an extremely imbalance and large-scale task</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howison</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3481936</idno>
		<ptr target="https://doi.org/10.1145/3459637.3481936" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3986" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploring the concept of pid literacy: User perceptions and understanding of persistent identifiers in support of open scholarly infrastructure</title>
		<author>
			<persName><forename type="first">G</forename><surname>Macgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Lancho-Barrantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07367</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Want to analyze millions of scientific papers all at once? here&apos;s the best way to do it</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mckenzie</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aan7139</idno>
		<ptr target="https://doi.org/10.1126/science.aan7139" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">2nd National Plan for Open Science</title>
		<author>
			<persName><surname>Mesr</surname></persName>
		</author>
		<ptr target="https://cache.media.enseignementsup-recherche.gouv.fr/file/science_ouverte/20/9/MEN_brochure_PNSO_web_1415209" />
	</analytic>
	<monogr>
		<title level="m">National Plan for Open Science</title>
		<imprint>
			<date type="published" when="2018">2018. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Citing data sources in the social sciences: Do authors do it?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pdf Mooney</surname></persName>
		</author>
		<idno type="DOI">10.1087/20110204</idno>
		<ptr target="https://doi.org/https://doi.org/10.1087/20110204" />
	</analytic>
	<monogr>
		<title level="j">Learned Publishing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Memorandum for the heads of executive departments and agenciesensuring free, immediate, and equitable access to federally funded research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nelson</surname></persName>
		</author>
		<ptr target="https://www.whitehouse.gov/wp-content/uploads/2022/08/08-2022-OSTP-Public-Access-Memo.pdf" />
		<imprint>
			<date type="published" when="2022-05-17">2022. May 17th, 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Informal data citation for data sharing and reuse is more common than formal data citation in biomedical fields</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolfram</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.24049</idno>
		<ptr target="https://doi.org/https://doi.org/10.1002/asi.24049" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1346" to="1354" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pdfalto</surname></persName>
		</author>
		<ptr target="https://github.com/kermitt2/pdfalto" />
		<imprint>
			<date type="published" when="2017">2017-2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Philippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hammitzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Janosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Werkhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hettrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leinweber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Druskat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Henwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Lohani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sinha</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2585783</idno>
		<ptr target="https://doi.org/10.5281/zenodo.2585783" />
		<title level="m">softwaresaved/international-survey: Public release for 2018 results</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<idno type="DOI">10.6084/m9.figshare.21687686.v2</idno>
		<ptr target="https://doi.org/10.6084/m9.figshare.21687686.v2" />
		<title level="m">PLOS Open Science Indicators</title>
		<imprint>
			<publisher>Public Library of Science</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Oddpub -a text-mining algorithm to detect data sharing in biomedical publications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bobrov</surname></persName>
		</author>
		<idno type="DOI">10.5334/dsj-2020-042</idno>
		<ptr target="https://doi.org/http://doi.org/10.5334/dsj-2020-042" />
	</analytic>
	<monogr>
		<title level="j">Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Somesci-a 5 star open data gold standard knowledge graph of software mentions in scientific articles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bensmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dietze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krüger</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482017</idno>
		<ptr target="https://doi.org/10.1145/3459637.3482017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4574" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The role of software in science: A knowledge analysis of software mentions in PubMed Central</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bensmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dietze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krüger</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.835</idno>
		<ptr target="https://doi.org/10.7717/peerj-cs.835" />
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="page" from="8" to="e835" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Challenges and opportunities</title>
		<idno type="DOI">10.1126/science.331.6018.692</idno>
		<ptr target="https://doi.org/10.1126/science.331.6018.692" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6018</biblScope>
			<biblScope unit="page" from="692" to="693" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Science</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Assessment of transparency indicators across the biomedical literature: How open is open?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Serghiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Contopoulos-Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boyack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.3001107</idno>
		<ptr target="https://doi.org/https://doi.org/10.1371/journal.pbio.3001107" />
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3001107</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Publication practices during the covid-19 pandemic: Expedited publishing or simply an early bird effect?</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Sevryugina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Dicks</surname></persName>
		</author>
		<idno type="DOI">10.1002/leap.1483</idno>
		<ptr target="https://doi.org/https://doi.org/10.1002/leap.1483" />
	</analytic>
	<monogr>
		<title level="j">Learned Publishing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="563" to="573" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Theory and practice of data citation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Silvello</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.23917</idno>
		<ptr target="https://doi.org/https://doi.org/10.1002/asi.23917" />
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<ptr target="https://www.tei-c.org/P5" />
		<title level="m">Guidelines for electronic text encoding and interchange</title>
		<editor>
			<persName><surname>Tei Consortium</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Changes in data sharing and data reuse practices and perceptions among scientists worldwide</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tenopir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Allard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pjesivac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dorsett</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0134826</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0134826" />
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Comparing Rule-based, Featurebased and Deep Neural Methods for De-identification of Dutch Medical Records. Proceedings of the 1st ACM WSDM Health Search and Data Mining Workshop</title>
		<author>
			<persName><forename type="first">J</forename><surname>Trienes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trieschnigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>HSDM2020</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Overly optimistic prediction results on imbalanced data: A case study of flaws and benefits when applying over-sampling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vandewiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kovács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sterckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Janssens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ongenae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Backere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Turck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roelens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Decruyenaere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Hoecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2020.101987</idno>
		<ptr target="https://doi.org/https://doi.org/10.1016/j.artmed.2020.101987" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page">101987</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Text mining of 15 million full-text scientific articles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Westergaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Staerfeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tønsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brunak</surname></persName>
		</author>
		<idno type="DOI">10.1101/162099</idno>
		<ptr target="https://doi.org/10.1101/162099" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Lessons from deep learning applied to scholarly information extraction: What works, what doesn&apos;t, and future directions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Yousuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Kaushal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Dunham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Self</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<idno>ArXiv, abs/2207.04029</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A context-based framework for modeling the role and function of on-line resource citations in scientific literature</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5206" to="5215" />
		</imprint>
		<respStmt>
			<orgName>EMNLP-IJCNLP</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<idno type="DOI">10.5281/zenodo.7995565</idno>
		<ptr target="https://doi.org/10.5281/zenodo.7995565" />
		<title level="m">Data and software availability The data resulting from this work are publicly available on the French Ministry of Higher Education and Research open data portal: dataset</title>
		<imprint/>
	</monogr>
	<note>Softcite corpus version 2.0 is available at Zenodo</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
