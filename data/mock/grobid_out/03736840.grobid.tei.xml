<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BERTrade: Using Contextual Embeddings to Parse Old French</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Loïc</forename><surname>Grobol</surname></persName>
							<email>lgrobol@parisnanterre.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Paris Nanterre</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country>Nanterre France (</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LIFO</orgName>
								<orgName type="institution" key="instit2">Université d&apos;Orléans</orgName>
								<orgName type="institution" key="instit3">INSA Centre�-�Val-de-Loire</orgName>
								<address>
									<settlement>Orléans</settlement>
									<country>France (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathilde</forename><surname>Regnault</surname></persName>
							<email>mathilde.regnault@ling.uni-stuttgart.de</email>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Lattice</settlement>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">Université Sorbonne Nouvelle</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France (</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Institut für Linguistik/Romanistik (ILR)</orgName>
								<orgName type="institution">Universität Stuttgart</orgName>
								<address>
									<country>Deutschland (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Ortiz Suarez</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France (</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">Sorbonne Université</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
							<email>laurent.romary@inria.fr</email>
							<affiliation key="aff5">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France (</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoit</forename><surname>Crabbé</surname></persName>
							<email>benoit.crabbe@u-paris.fr</email>
							<affiliation key="aff7">
								<orgName type="laboratory">) LLF</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris Cité</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BERTrade: Using Contextual Embeddings to Parse Old French</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BA52F53A1E54D6897993E5446CAB820</idno>
					<idno type="DOI">10.5281/zenodo.6461220</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-08-24T15:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Old French</term>
					<term>Contextual word embeddings</term>
					<term>Dependency Parsing</term>
					<term>Part of Speech Tagging</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The successes of contextual word embeddings learned by training large-scale language models, while remarkable, have mostly occurred for languages where significant amounts of raw texts are available and where annotated data in downstream tasks have a relatively regular spelling. Conversely, it is not yet completely clear if these models are also well suited for lesser-resourced and more irregular languages. We study the case of Old French, which is in the interesting position of having relatively limited amount of available raw text, but enough annotated resources to assess the relevance of contextual word embedding models for downstream NLP tasks. In particular, we use POS-tagging and dependency parsing to evaluate the quality of such models in a large array of configurations, including models trained from scratch from small amounts of raw text and models pre-trained on other languages but fine-tuned on Medieval French data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There is a growing interest in digital humanities for automatic processing and annotation of historical texts. In this work, we study how to take advantage of current NLP models of the BERT family to advance the state of the art in processing historical languages, taking Old French (9th-13th century French) as a use case. Old French is one of the historical languages for which we have the largest amount of syntactically annotated data, and we expect that our results on these language states may be generalised and used as a source of inspiration for researchers currently developing annotated resources for other historical languages. Using contextual word embeddings as input representations has brought clear gains in performances for most of the NLP tasks for which they have been used. However, this has mostly been attested in languages where sufficient (raw) linguistic data is available. For less-resourced languages, the most common approach has been to leverage multilingual models such as mBERT <ref type="bibr" target="#b10">(Devlin et al. 2019)</ref> Historical languages are typical cases where available linguistic data is limited, with no chance of acquiring new texts. They are also not normalized by spelling and institutional conventions and tend to be more heterogeneous than contemporary lesser-resourced languages. Old French is a particularly interesting language for this kind of study, since relatively to its limited amount of available raw text, its volume of annotated linguistic data is quite high, due to the existence of the SRCMF dependency treebank <ref type="bibr" target="#b40">(Prévost and Stein 2013)</ref> and its latest incarnation in the Universal Dependency project <ref type="bibr" target="#b34">(Nivre et al. 2020)</ref>, which boasts around 17.7 K sentences<ref type="foot" target="#foot_0">1</ref> for around 171 K words. Another interesting property of Old French is its proximity to a well-resourced language, namely contemporary French, for which monolingual contextual embeddings models exist and have been shown to be relevant for dependency parsing <ref type="bibr" target="#b21">(Le et al. 2020;</ref><ref type="bibr" target="#b29">Martin et al. 2020)</ref>. Last, but certainly not least, the design of an accurate syntactic parser for Old French would be a very valuable tool for computer-assisted linguistic studies. Indeed, studying the historical variation of syntax in a language that lacks both native speakers and centralized standard variants can be very challenging, due to the prohibitive cost of manual annotation. Automatic syntactic annotations, either as a "silver-standard" truth or as a bootstrapping step towards manual annotation, can drastically reduce that cost. In this work, exploiting this currently unique situation of Old French among lesser-resourced and historical languages, we use dependency parsing and POS-tagging of Old French as probes of the relevance of contextual embeddings in a context of high heterogeneity and relative scarcity of data. More precisely, we consider several neural language models, some of which trained or fine-tuned on a new corpus of raw Old and Middle French texts, and use their internal representations of words as inputs to train taggers and parsers on the SRCMF treebank. The resulting tagging and parsing scores then serve as an evaluation of the quality and usefulness of these representations. We claim the following contributions:</p><p>• We provide empirical evidence that contextual embeddings are relevant for historical language processing, even when no data is available beyond the treebank used to train a parser.</p><p>• We provide a comparative study of several strategies for obtaining such contextual embeddings. Specifically, we compare cases where raw data is available in the target language and cases where existing contextual embeddings are available for the contemporary counterpart of a historical language.</p><p>• We release two publicly available resources for Old French: BERTrade 23 , a set of contextual word embedding models ; and a state-of-the-art POS-tagging and dependency parsing model<ref type="foot" target="#foot_3">4</ref> .</p><p>The paper is organized as follows. Section 2 provides an overview of related work that aims at taking advantage of the BERT family of language models in scenarios where the amount of data is limited. In Section 3 we provide a description of the dataset we gathered to conduct our experiments, and finally we report experiments in Section 4 involving reusing BERT from other languages and training BERT models on Old French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Since the introduction of contextualized word representations <ref type="bibr" target="#b39">(Peters et al. 2018;</ref><ref type="bibr" target="#b1">Akbik et al. 2018;</ref><ref type="bibr" target="#b10">Devlin et al. 2019</ref>) and the many improvements proposed for them in the consumption of computational resources <ref type="bibr" target="#b8">(Clark et al. 2020)</ref>, in the amount of data required to fine-tune them <ref type="bibr" target="#b41">(Raffel et al. 2020)</ref>, and more recently in the length of the contextual window <ref type="bibr" target="#b61">(Xiong et al. 2021)</ref>; there have also been important advancements from a digital humanities point of view on unsupervised domain adaptation (Ramponi and Plank 2020). In this case, one specializes a language model to a particular domain with unlabeled data in order to improve performance in downstream tasks. This can be achieved by pre-training the models from scratch with specialized data <ref type="bibr" target="#b2">(Beltagy et al. 2019)</ref> or by continuing the training of a general model with a new corpus <ref type="bibr" target="#b22">(Lee et al. 2019;</ref><ref type="bibr" target="#b38">Peng et al. 2019)</ref>. This last method has already been successfully implemented in the context of historical languages, in particular <ref type="bibr" target="#b19">Han and Eisenstein (2019)</ref> showed that one can successfully adapt the original BERT <ref type="bibr" target="#b10">(Devlin et al. 2019)</ref> to Early Modern English by continuing the pre-training on historical raw texts. In a multilingual context, transformer-based models such as mBERT have been adapted to low-resource languages and evaluated in dependency parsing and POS-tagging, showing promising results <ref type="bibr" target="#b7">(Chau et al. 2020;</ref><ref type="bibr" target="#b33">Muller et al. 2020;</ref><ref type="bibr" target="#b18">Gururangan et al. 2020;</ref><ref type="bibr" target="#b58">Z. Wang et al. 2020</ref>). However, this multilingual approach has also been criticized for favoring monolingual pre-training even when data is scarce <ref type="bibr" target="#b56">(Virtanen et al. 2019;</ref><ref type="bibr" target="#b36">Ortiz Suárez et al. 2020)</ref>. Indeed, even when only small pre-training corpora are available, BERT-like models have also been successfully pre-trained, resulting in well-performing models <ref type="bibr" target="#b31">(Micheli et al. 2020)</ref>. Furthermore, compact BERT-like models have also been studied <ref type="bibr" target="#b53">(Turc et al. 2019)</ref> and might prove useful in data constrained conditions, such as monolingual pre-training of contextualized word representation for low-resource languages.</p><p>Regarding corpora for historical languages, very few of them have manually annotated syntactical resources for their medieval states. English has three such treebanks (University of Oxford 2001; <ref type="bibr" target="#b20">Kroch et al. 2000;</ref><ref type="bibr" target="#b52">Traugott and Pintzuk 2008)</ref> for Old and Middle English. The TOROT treebank for Old Church Slavonic, Old East Slavonic and Middle Russian is another large resource (Berdicevskis and Eckhoff 2020). There is a treebank for Medieval Latin as well, the Index Thomisticus Treebank <ref type="bibr" target="#b37">(Passarotti 2019</ref>). To our knowledge, the last large treebank containing medieval texts is IcePaHC for Icelandic <ref type="bibr" target="#b44">(Rögnvaldsson et al. 2012)</ref>. Some other corpora were annotated automatically in order to reduce the cost of annotation. For example, <ref type="bibr" target="#b43">Rocio et al. (2003)</ref> adapted a parsing pipeline for contemporary Portuguese and Lee and Kong (2016) used a previously annotated treebank <ref type="bibr" target="#b23">(Lee and Kong 2012)</ref> to parse a larger medieval Chinese corpus. Concerning contemporary regional Romance languages, <ref type="bibr" target="#b32">Miletic et al. (2020)</ref> also used a smaller treebank to generate new annotations, and concluded that using similar languages to train a model does not improve parsing. Although there are many resources for Latin, and some for Ancient Greek, we do not include them here, because they do not face the same challenges as medieval states of language, in particular the high level of spelling variability. Lastly, concerning dependency parsing and POS-tagging of Old French in particular, the works of <ref type="bibr" target="#b16">Guibon et al. (2014)</ref> and <ref type="bibr" target="#b47">Stein (2014)</ref> and <ref type="bibr" target="#b47">Stein (2016)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data</head><p>This section describes the raw corpus of Medieval French we gathered in order to train unsupervised language models for Old French. To our knowledge, it is one of the largest such dataset gathered for Medieval French, although it remains quite small (55 MiB in total) relatively to the corpora usually used for pre-training contextual embeddings models. Older texts are close to Late Latin, and verse is prevalent until the end of the 13th century. Old French has a relatively free word order. Until the mid-11th century, the prevalent order is Subject-Object-Verb (SOV), which is then gradually supplanted by SVO, which is the default order in contemporary French. Unlike most languages with free word order, the functions of verbal arguments are not always given away by morphological clues, the already simplistic case system of Old French disappears progressively through the covered period.</p><p>There are also many cases of syntactic ambiguity. For example, in the following quote from Lancelot,<ref type="foot" target="#foot_4">5</ref> (verse 5436), both "la dame" and "Lancelot" could be the subject or the object of "Vit" and only the context enables the reader to understand that "la dame" is the subject. Word order is also relatively free within constituents. For example, a noun modifier can be on the left or on the right of its governor, and it is not necessarily preceded by a preposition. In contemporary French, it can only appear on the right, and it is found without a preposition only in some cases like named entities. Because of the general free word order and the absence of punctuation in our treebank, this adds up to the ambiguity of the analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dolant</head><p>In each of the following examples from the SRCMF corpus, the noun following roi ("king") has a different analysis: head of roi, modifier, argument of the same verb or a different one, with no explicit marking:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fus tu donc pus a la roi cort</head><p>Were you then no more at the king court Furthermore, overt subjects are not mandatory, and are often dropped in texts written in verse until the 12th century, after which the presence of subjects increases through time. These phenomena are particularly prevalent in verse, where metric and rhyming constraints often lead to more contrived syntactic forms than in prose.</p><p>Another source of ambiguity is the variety of spellings, due to the lack of spelling standard. For example, the word moult (transl. a lot (of), very), emblematic of this period, is initially an adjective, and it is progressively grammaticalized, becoming an adverb. Several forms appear at the same time, some with a declension, some  <ref type="bibr" target="#b45">(Rothwell and Trotter 2005)</ref> 17.2 3.25 NCA <ref type="bibr" target="#b48">(Stein et al. 2006)</ref> 9.7 2.05 Chartes Douai <ref type="bibr" target="#b14">(Gleßgen 2003)</ref> 3.1 0.56 OpenMedFr <ref type="bibr" target="#b60">(Wrisley 2018)</ref> 1.7 0.33 Geste <ref type="bibr" target="#b6">(Camps et al. 2016)</ref> 1.5 0.32 MCVF <ref type="bibr" target="#b30">(Martineau 2008)</ref> 1  Medieval French has many factors of variation: language evolution, dialects, domains, forms of text (verse or prose) and lack of standard. Our dataset gives us a representation of Medieval French that is as accurate and diversified as possible, given the limited amount of material that survived to these days. The detailed instructions to replicate this dataset are described in the Appendix. No particular processing is done on the original documents.</p><p>In order to get a sound evaluation of the contextual embeddings trained with this dataset, we filter out the documents that are also present in the SRCMF treebank used for evaluation purposes in section 4 <ref type="foot" target="#foot_5">6</ref> . The resulting corpus is quite heterogeneous: legal texts and verse literature are in the majority, whereas other domains, such as historical and didactic texts, are under-represented, as can be seen in fig. <ref type="figure" target="#fig_3">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate a set of alternative word representations on Old French, using their usefulness for POS-tagging and dependency parsing as a downstream evaluation. To that end, we train and evaluate a parser/tagger using the annotated treebank of Old French (SRCMF, <ref type="bibr" target="#b40">Prévost and Stein (2013)</ref>) as provided by the 2.7 version of the UD dataset <ref type="bibr" target="#b63">(Zeman et al. 2020</ref>) as a reference treebank.</p><p>Our parser/tagger probe uses Dozat and Manning (2018)'s neural graph parser made as reimplemented by <ref type="bibr" target="#b21">Le et al. (2020)</ref> and <ref type="bibr" target="#b15">Grobol and Crabbé (2021)</ref>, using the same hyperparameters. Word representations are obtained by concatenating subword embeddings, averaged over transformer layers together with character embeddings and non contextualized word embeddings. This representation is similar to those used by <ref type="bibr" target="#b50">Straka et al. (2019)</ref> and <ref type="bibr" target="#b25">Ling et al. (2015)</ref>. In all of our experiments, the contextual embeddings are fine-tuned while training the parser. Unlike the recent CoNLL challenges settings, we assume gold tokenization, since the syntactic annotations we target provide a reference word-based segmentation. Using a predicted one could only add noise to our experiments. Furthermore, for most European languages using a Latin script-including Old and Middle French-, word segmentation is acceptably approximated by simple typographic tokenization. The remaining of this section presents our experimental results, sorted by nature of required data. We report UPOS POS-tagging scores as well as unlabeled and labeled attachment scores for dependency parsing (respectively UAS and LAS), as given by the CoNLL-2018 scorer, computed on the development set of SRCMF to avoid overfitting the architecture and transfer learning procedure to the test set. Results on the test set are provided only for the dev-best models to allow us to compare our results to the state of the art.</p><p>Due to the number of costly experiments, 7 the results are reported on single runs. The results should therefore be interpreted only with respects to the broad trends: small score differences between competing settings should be taken with care.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines</head><p>Embeddings We first compare a baseline where contextual embeddings are not used at all (Vanilla) with two settings using models with no preexisting knowledge of Old French: Random-base, a randomly initialized model using the same architecture and model size as RoBERTa-base <ref type="bibr" target="#b26">(Liu et al. 2019</ref>) and finBERT <ref type="bibr" target="#b56">(Virtanen et al. 2019)</ref>, a contextual embedding model from Finnish, a Uralic language that is unrelated to Old French. These baselines are meant to check that the gain in performances observed when using models with some (possibly indirect) knowledge of Old French are linked to this knowledge and not simply due to an increase in the number of trainable parameters (for the random baseline) or to a weight distribution induced by training on a language modeling task that would be universally good for all languages (for the finBERT baseline, which can thus be seen as a different kind of weight initialization).</p><p>Table <ref type="table" target="#tab_3">2</ref> shows the results obtained in these configurations, which show that using a model with random weights, even fine-tuned for these tasks, does not bring any improvement, and is in fact even worse than using no contextual embeddings at all. In contrast, using a model that has been pretrained for language modeling-even for an unrelated language-brings some modest improvements. This suggests that pretraining gives a structure to this kind of model that makes it suitable for fine-tuning on the downstream task, but the impact of this gain is clearly-and predictably-very limited compared to what can be expected for representations that have been trained on relevant linguistic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">With related contextual embeddings</head><p>When a low-resource language is close to a well-resourced one, it is possible to leverage models designed for the latter. For Old French, contemporary French is an obvious candidate and two contextual embeddings models are available:</p><p>7 See the Appendix for elements on the carbon footprint of our experiments. FlauBERT <ref type="bibr" target="#b21">(Le et al. 2020)</ref> and CamemBERT <ref type="bibr" target="#b29">(Martin et al. 2020)</ref>. Furthermore, mBERT <ref type="bibr" target="#b10">(Devlin et al. 2019</ref>), a model trained on a multilingual corpus which does not include Old French (possibly apart from some fragments in its contemporary French training data), has been shown to be suitable for many languages, and in particular for Indo-European and Romance languages <ref type="bibr" target="#b50">(Straka et al. 2019;</ref><ref type="bibr" target="#b33">Muller et al. 2020)</ref>. We report in table 3 the results obtained when using these language models directly, without additional fine-tuning involving Old French data. As expected, these results show significant improvements over the baselines, confirming that using contextual embeddings for a related language works better than both randomly initialized embeddings and embeddings pretrained for an unrelated language-even after fine-tuning. More surprisingly, the best results here are obtained with mBERT. This could mean that mBERT benefits from having been pretrained for a wider range of languages, including in particular other Romance languages that share with Old French some features, lost in contemporary French: for instance null subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">With raw linguistic data</head><p>We now try to take advantage of the raw Medieval French data described in section 3. To that end, we explore two strategies: training a model from scratch and refining existing models by "post-training" them-running a few more training epochs on the Medieval French raw data. In the "from scratch" strategy, we first train a BBPE subword tokenizer (C. <ref type="bibr" target="#b57">Wang et al. 2020</ref>) on our raw corpus, then train a RoBERTa <ref type="bibr" target="#b26">(Liu et al. 2019</ref>) masked language model. Taking inspiration from <ref type="bibr" target="#b31">Micheli et al. (2020)</ref>, who worked in a setting close to ours: a small and noisy pretraining corpus used to create a model from scratch, we used a RoBERTa architecture. As reported in table 4, we tested several parametrizations of the architecture also inspired by <ref type="bibr" target="#b53">Turc et al. (2019)</ref>. Out of these alternatives, the "BERTrade-petit" configuration was the most successful and this is the one we keep for the following experiments.</p><p>For the "post-training" strategy, we continue the training of the pre-trained models used in sections 4.1 and 4.2, for 12 epochs on our raw corpus. We used the same RoBERTa masked language modeling task, using the same parameters as Z. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Putting it all together</head><p>Finally, in table 6, we compare the performances of our models on the test set of SRCMF with those obtained by <ref type="bibr" target="#b50">Straka et al. (2019)</ref>, with similar methods. The difference between the models is that we fine-tune the word embeddings, while <ref type="bibr" target="#b50">Straka et al. (2019)</ref> keep them frozen. Our mBERT baseline, which is the closest to their configuration, shows that even without any additional data, task-specific fine-tuning already brings significant improvements, while our models refined using our raw corpus of Medieval French bring further improvements, leading to state-of-the-art results that are consistent with their results on the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have shown that building a monolingual contextual word embeddings model for Medieval French is possible even with limited and heterogeneous linguistic data and that it can bring significant performance gains in parsing and POS-tagging. To that end, the best strategy seems to be post-training a contextual word embedding model for contemporary French on raw Medieval French documents. We have not directly addressed the internal heterogeneity issue in both our pretraining and fine-tuning data, relying instead on the versatility of the representation models we considered to bypass it, but it seems a promising perspective for future work-for instance by using finer-grained post-training, concentrating on specific linguistic sub-periods or genres. For historical languages in general, this suggests that language-specific fine-tuning is more efficient when applied to a model pre-trained for their contemporary counterpart than when applied to a multilingual model. While this study is not currently easy to replicate for other languages due to the lack of annotated data for a suitable downstream task, it suggests that the considerable amount of work required to gather even a small amount of raw texts in the target language is a sound investment, given the significant improvements it can bring to contextual word representations. Beyond historical languages, these findings could also help for processing minority dialectal variants and contact languages of well-resourced languages, and we leave for future work the exploration of these generalizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Oïl languages</figDesc><graphic coords="4,70.87,70.86,220.11,244.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>'</head><label></label><figDesc>The lady saw that Lancelot was mournful and meditative.'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>you not at the king's court anymore?" (Beroul Tristan) la fille au riche roi pescheor the daughter of the rich king fisher flat "the daughter of the rich Fisher King" (Queste del Saint Graal) De Guenelun atent li reis nuveles From Ganelon waits the king news nsubj obj "The king waits for news from Ganelon." (Chanson de Roland) Biax sire fet li rois escu vos envoiera Diex Dear Sir says the king shield you send-FUT God nsubj obj "Dear Sir, says the king, God will send you a shield." (Queste del Saint Graal)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of form and domain, gathered from documents metadata and manual annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Data sources for the raw corpus used for model pretraining, with sizes in bytes and number of words. Due to the nature of the documents, mixing prose, verse, titles, annotations… estimating a number of sentence would be error-prone and can be abstracted over, given that the models trained here do not depend on strict sentence boundaries.</figDesc><table><row><cell>.4</cell><cell>0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on SRCMF dev -no additional data.</figDesc><table><row><cell></cell><cell>UPOS UAS LAS</cell></row><row><cell>Vanilla</cell><cell>93.51 87.60 81.54</cell></row><row><cell cols="2">Random-base 93.17 86.97 80.71</cell></row><row><cell>finBERT</cell><cell>94.44 88.44 82.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Base model UPOS UAS LAS</head><label></label><figDesc></figDesc><table><row><cell>FlauBERT</cell><cell>95.70 90.43 85.45</cell></row><row><cell cols="2">CamemBERT 95.86 91.15 86.31</cell></row><row><cell>mBERT</cell><cell>96.06 91.52 86.83</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on SRCMF dev -monolingual models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc><ref type="bibr" target="#b57">Wang et al. (2020)</ref> (but without vocabulary modifications), resulting in the BERTrade-X models, where X is the name of the base model. The results of these experiments are reported in Table5. Comparing these to our results of section 4.2 shows that training a model from scratch, even on such limited amounts of data, yields a better model than a Results on SRCMF dev -Performances of different model sizes when training from scratch</figDesc><table><row><cell>Name</cell><cell></cell><cell cols="4">Layers Embeddings Heads UPOS UAS LAS</cell></row><row><cell cols="2">BERTrade-tiny</cell><cell>2</cell><cell>128</cell><cell>2</cell><cell>94.03 88.66 82.79</cell></row><row><cell cols="2">BERTrade-small</cell><cell>4</cell><cell>512</cell><cell>8</cell><cell>96.53 86.30 87.49</cell></row><row><cell cols="2">BERTrade-petit</cell><cell>12</cell><cell>256</cell><cell>4</cell><cell>97.14 91.90 89.18</cell></row><row><cell cols="2">BERTrade-medium</cell><cell>8</cell><cell>512</cell><cell>8</cell><cell>96.62 91.92 87.60</cell></row><row><cell cols="2">BERTrade-base</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>96.74 92.37 88.42</cell></row><row><cell>Base model</cell><cell cols="2">UPOS UAS LAS</cell><cell></cell><cell></cell></row><row><cell>BERTrade-petit</cell><cell cols="2">97.14 92.95 89.18</cell><cell></cell><cell></cell></row><row><cell>BERTrade-finBERT</cell><cell cols="2">96.28 92.12 87.92</cell><cell></cell><cell></cell></row><row><cell>BERTrade-mBERT</cell><cell cols="2">96.95 93.33 89.60</cell><cell></cell><cell></cell></row><row><cell cols="3">BERTrade-CamemBERT 97.16 93.75 90.06</cell><cell></cell><cell></cell></row><row><cell>BERTrade-FlauBERT</cell><cell cols="2">96.94 93.75 90.07</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results on SRCMF dev -using raw data.</figDesc><table><row><cell cols="2">simple task-specific fine-tuning of mBERT. However,</cell></row><row><cell cols="2">post-training mBERT yields even better results, and the</cell></row><row><cell cols="2">best ones are obtained by post-training the models for</cell></row><row><cell>contemporary French.</cell><cell></cell></row><row><cell>Model</cell><cell>UPOS UAS LAS</cell></row><row><cell>Straka et al. (2019)</cell><cell>96.26 91.83 86.75</cell></row><row><cell>mBERT</cell><cell>96.19 92.03 87.52</cell></row><row><cell>BERTrade-petit</cell><cell>96.60 92.20 87.95</cell></row><row><cell>BERTrade-mBERT</cell><cell>97.11 93.86 90.37</cell></row><row><cell>BERTrade-FlauBERT</cell><cell>97.15 93.96 90.57</cell></row><row><cell cols="2">BERTrade-CamemBERT 97.29 94.36 90.90</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results on SRCMF test</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Putting it in the second place of all French language treebanks in number of sentences.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://doi.org/10.5281/zenodo.6461220</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Bertrade de Laon, also known as Berthe au Grand Pied was the mother of Charlemagne.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/hopsparser/hopsparser/ blob/main/docs/models.md#srcmf-ud</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>In the edition from Pierre Kunstmann, from the online Base de français médiéval: http://catalog.bfm-corpus. org/CharretteKu.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>As noted by<ref type="bibr" target="#b18">Gururangan et al. (2020)</ref>, pre-training on task specific data provides an additional boost, that would muddle our results, since our objective here is not so much task optimization as embeddings benchmarking.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was funded by the ANR projet ANR-16-CE38-0010 "Profiterole" (PRocessing Old French Instrumented TExts for the Representation Of Language Evolution) and used the HPC resources of IDRIS under the allocation 2020-AD011011637 made by GENCI.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Collecting the Data</head><p>The following data can be downloaded directly from their website:</p><p>• Chartes de l'Aube: https://sites.google.com/site/achimstein/ research/resources</p><p>Extract raw text from XML files: &lt;body&gt;, then &lt;s&gt;, then &lt;word&gt;.</p><p>• Geste: https://github.com/Jean-Baptiste-Camps/ Geste Raw text is available under /txt/norm/.</p><p>• OpenMedFr: https://github.com/OpenMedFr/texts Remove the header of each file (until *** START), its last line (*** END), paragraph breaks (#|) and folios or pages numbers. Special permissions are required to access and use these sources:</p><p>• AND: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Details on the Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Models Trained From Scratch</head><p>These are trained for 32 epochs in a masked language modeling task using the same parameters as RoBERTa <ref type="bibr" target="#b26">(Liu et al. 2019</ref>) but a smaller batch size of 256 samples 8 , which amounts to a magnitude of 1×10 5 steps. We also use a smaller vocabulary size (8192) than other works, in line with the observations of <ref type="bibr" target="#b12">Ding et al. (2019)</ref> that learning large vocabularies on small corpora defeats the purpose of sub-word tokenization. Using a larger vocabulary size of 5×10 4 (like FlauBERT) also did not seem to bring any improvements in our preliminary experiments and made pre-training more expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Post-training</head><p>The pretrained models we used in the post-training settings are those available in the 4.2.0 version of Huggingface Transformers <ref type="bibr" target="#b59">(Wolf et al. 2020</ref>) and the exact handles are:</p><p>The post-trained models are those with MLM heads, which we did not reset before post-training, so the post-training phase can be seen as a language transfer task for masked language modeling out of which we extract a contextual word embeddings model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Carbon Footprint</head><p>In light of recent concerns about the power consumption and carbon footprint of deep learning models (Schwartz   8 Preliminary experiments with larger batch sizes showed no significant improvement to compensate for the heavier computational load. Pre-train: We use a cluster of 4 machines each one having 8 GPU Nvidia Tesla V100 SXM2 32 GiB, 384 GiB of RAM, and two Intel Xeon Gold 6226 processors. One Nvidia Tesla V100 card is rated at around 300 W, 9 while the Xeon Gold 6226 processor is rated at 125 W, 10 . For the DRAM we can use the work of <ref type="bibr" target="#b9">Desrochers et al. (2016)</ref> to estimate the total power draw of 384 GiB of RAM at around 39 W. The total power draw of this setting adds up to around 10 756 W. We train 11 different models in this configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Power (W) # Models Duration (h) Consumption (kWh) CO 2 e (kg)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-train:</head><p>We use a single machine having 4 GPU Nvidia Tesla V100 SXM2 32 GiB, 192 GiB of RAM and two Intel Xeon Gold 6248 processors. The Xeon Gold 6248 processor is rated at 150 W, 11 , and the DRAM total power draw can be estimated at around 20 W. The total power draw of this setting adds up to around 1520 W. We train 4 different models in this configuration.</p><p>Having this information, we can now use the formula proposed by <ref type="bibr" target="#b51">Strubell et al. (2019)</ref> in order to compute the total power required for each setting:</p><p>p t = 1.58t(cp c +p r +gp g ) 1000 Where c and g are the number of CPUs and GPUs respectively, p c is the average power draw (in W) from all CPU sockets, p r the average power draw from all DRAM sockets, and p g the average power draw of a single GPU. We estimate the total power consumption by adding GPU, CPU and DRAM consumption, and then multiplying by the Power Usage Effectiveness (PUE), which accounts for the additional energy required to support the compute infrastructure. We use a PUE coefficient of 1.58, the 2018 global average for data centers <ref type="bibr" target="#b51">(Strubell et al. 2019)</ref>. In table 7 we report the training times in hours, as well as the total power draw (in Watts) of the system used to train the models. We use this information to compute the 9 Nvidia Tesla V100 specification 10 Intel Xeon Gold 6226 specification 11 Intel Xeon Gold 6248 specification total power consumption of each setting, also reported in table <ref type="table">7</ref>. We can further estimate the CO 2 emissions in kilograms of each single model by multiplying the total power consumption by the average CO 2 emissions per kWh in our region which were around 32 g kW -1 h in January 2021, 12 when the models were trained. Thus the total CO 2 emissions in kg for one single model can be computed as:</p><p>CO 2 e = 0.032p t All emissions are also reported in table 7.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Bibliographical References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextual String Embeddings for Sequence Labeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciB-ERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11">Nov. 2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">FAccT &apos;21. Virtual Event, Canada</title>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Diachronic Treebank of Russian Spanning More Than a Thousand Years. English</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berdicevskis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eckhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="5251" to="5256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Camps</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Geste: un corpus de chansons de geste</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing with Multilingual BERT, a Small Corpus, and a Small Treebank</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="1324" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Validation of DRAM RAPL Power Measurements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Desrochers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Paradis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Weaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Memory Systems</title>
		<meeting>the Second International Symposium on Memory Systems<address><addrLine>Alexandria, VA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="455" to="470" />
		</imprint>
	</monogr>
	<note type="report_type">MEMSYS &apos;16</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Association for Computational Linguistics</title>
		<author>
			<persName><forename type="first">Minnesota</forename><surname>Minneapolis</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Call for Prudent Choice of Subword Merge Operations in Neural Machine Translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Translation Summit</title>
		<meeting>Machine Translation Summit<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>European Association for Machine Translation</publisher>
			<date type="published" when="2019-08">Aug. 2019</date>
			<biblScope unit="volume">XVII</biblScope>
			<biblScope unit="page" from="204" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simpler but More Accurate Semantic Dependency Parsing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="484" to="490" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">L&apos;élaboration philologique et l&apos;étude lexicologique des Plus anciens documents linguistiques de la France à l&apos;aide de l&apos;informatique</title>
		<author>
			<persName><forename type="first">M.-D</forename><surname>Gleßgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mémoires et documents de l&apos;École des chartes</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="371" to="386" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Analyse en dépendances du français avec des plongements contextualisés</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grobol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crabbé</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<pubPlace>France</pubPlace>
		</imprint>
	</monogr>
	<note>In 28e Conférence sur le Traitement Automatique des Langues Naturelles. Lille (virtuel</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parsing Poorly Standardized Language Dependency on Old French</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guibon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Workshop on Treebanks and Linguistic Theories</title>
		<editor>
			<persName><forename type="first">V</forename><surname>Henrich</surname></persName>
		</editor>
		<meeting>the Thirteenth International Workshop on Treebanks and Linguistic Theories<address><addrLine>Tübingen, Deutschland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Base de français médiéval : une base de référence de sources médiévales ouverte et libre au service de la communauté scientifique</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guillot-Barbance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavrentiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diachroniques</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1809581</biblScope>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">eprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Don&apos;t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gururangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised Domain Adaptation of Contextualized Embeddings for Sequence Labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">Nov. 2019</date>
			<biblScope unit="page" from="4238" to="4248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The Penn-Helsinki Parsed Corpus of Middle English (PPCME2). CD-ROM</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kroch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Santorini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>U. o. P. Department of Linguistics</publisher>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FlauBERT: Unsupervised Language Model Pre-training for French. English</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="2479" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/32527770/btz682.pdf" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">eprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A dependency treebank of classical Chinese poems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="191" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A dependency treebank of Chinese Buddhist texts. Digital Scholarship in the Humanities</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="140" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">Sept. 2015</date>
			<biblScope unit="page" from="1520" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Marchello-Nizia</surname></persName>
		</author>
		<title level="m">Histoire de la langue française aux XIVe et XVe siècles. Bordas</title>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Grande Grammaire historique du français</title>
		<author>
			<persName><forename type="first">C</forename><surname>Marchello-Nizia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>de Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CamemBERT: a Tasty French Language Model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7203" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Un corpus pour l&apos;analyse de la variation et du changement linguistique</title>
		<author>
			<persName><forename type="first">F</forename><surname>Martineau</surname></persName>
		</author>
		<ptr target="http://journals.openedition.org/corpus/1508" />
	</analytic>
	<monogr>
		<title level="j">Corpus</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2008-07">July 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the importance of pre-training data volume for compact language models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoffschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="7853" to="7858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Building a Universal Dependencies Treebank for Occitan. English</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miletic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="2932" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12858</idno>
		<idno>arXiv: 2010.12858</idno>
		<title level="m">When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models</title>
		<imprint>
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Universal Dependencies v2: An Evergrowing Multilingual Treebank Collection. English</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page" from="4034" to="4043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">European Language Resources Association</title>
		<author>
			<persName><forename type="first">France</forename><surname>Marseille</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Suárez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="1703" to="1714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The Project of the Index Thomisticus Treebank</title>
		<author>
			<persName><forename type="first">M</forename><surname>Passarotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Classical Philology: Ancient Greek and Latin in the Digital Revolution</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Berti</surname></persName>
		</editor>
		<imprint>
			<publisher>De Gruyter Saur</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="299" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">Aug. 2019</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Syntactic Reference Corpus of Medieval French (SRCMF)</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prévost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ENS de Lyon</publisher>
		</imprint>
	</monogr>
	<note>UPDATE VERSION NUMBER</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural Unsupervised Domain Adaptation in NLP-A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramponi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12">Dec. 2020</date>
			<biblScope unit="page" from="6838" to="6855" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automated creation of a medieval portuguese partial treebank</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rocio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Treebanks: Building and Using Parsed Corpora</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The Icelandic Parsed Historical Corpus (IcePaHC)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rögnvaldsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1977" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">W</forename><surname>Rothwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Trotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anglo-Normand Dictionary</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020-11">Nov. 2020</date>
			<pubPlace>Green AI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Old French Dependency Parsing: Results of Two Parsers Analysed from a Linguistic Point of View</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Reykjavík, Ísland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05">May 2014. 2016</date>
			<biblScope unit="page" from="707" to="713" />
		</imprint>
	</monogr>
	<note>Proceedings of the Ninth International Conference on Language Resources and Evaluation</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kunstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-D</forename><surname>Gleßgen</surname></persName>
		</author>
		<title level="m">Nouveau Corpus d</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">UDPipe 2.0 Prototype at CoNLL 2018 UD Shared Task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">Oct. 2018</date>
			<biblScope unit="page" from="197" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Evaluating Contextualized Embeddings on 54 Languages in POS Tagging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Straková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hajič</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07448</idno>
		<idno>arXiv: 1908.07448</idno>
	</analytic>
	<monogr>
		<title level="m">Lemmatization and Dependency Parsing</title>
		<imprint>
			<date type="published" when="2019-08">Aug. 2019</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Energy and Policy Considerations for Deep Learning in NLP</title>
		<author>
			<persName><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Coding the York-Toronto-Helsinki Parsed Corpus of Old English Prose to investigate the syntax-pragmatics interface. Studies in the History of the English Language IV. Empirical and Analytical Advances in the Study of English Language Change</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Traugott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pintzuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Mouton de Gruyter</publisher>
			<biblScope unit="page" from="61" to="80" />
			<pubPlace>Berlin/New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Turc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962[cs]:arXiv:1908.08962</idno>
		<title level="m">Well-Read Students Learn Better: On the Importance of Pre-training Compact Models</title>
		<imprint>
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">The York-Helsinki parsed corpus of Old English poetry (YCOEP)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Oxford Text Archive</publisher>
		</imprint>
		<respStmt>
			<orgName>University of Oxford</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Champagne 1270-1300</title>
		<author>
			<persName><forename type="first">P</forename><surname>Van Reenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wattel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Mulken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chartes en langue française conservées aux Archives de l&apos;</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<pubPlace>Aube</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Multilingual is not enough: BERT for Finnish</title>
		<author>
			<persName><forename type="first">A</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="arXiv">CoRR,abs/1912.07076:arXiv:1912.07076</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<title level="m">Neural Machine Translation with Byte-Level Subwords. en. Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9154" to="9160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Extending Multilingual BERT to Low-Resource Languages</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020-10">Oct. 2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Wrisley</surname></persName>
		</author>
		<title level="m">The Medieval French Initiative</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>
		<meeting>the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">Nov. 2020. Oct. 2018</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>Universal Dependencies 2.7</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
